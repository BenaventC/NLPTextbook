{
  "hash": "6502014c47d4ce1f8848c57a583cc44b",
  "result": {
    "engine": "knitr",
    "markdown": "# Préambule {.unnumbered}\n\n\n\"Au commencement était le verbe\", et désormais le verbe est partout, non pas l'esprit de Dieu, mais la parole humaine qui s'éteignait avec le vent, la rumeur mais qui désormais, accumule ses traces imprimées de manière systématique, non plus des petits mots passés de main en main, mais l'enregistrement des transactions informatiques. Le verbe est désormais une copie du monde, moins l'affirmation d'une vision, que la stratification de nos manifestations sociales.\n\nC'est bien sur le fruit d'un développement technique engagés depuis 10 000 ans et dont l'imprimerie est une nouvelle étape. La révolution actuelle réside dans la colonisation de l'espace social, chaque bribes de parole est enregistrée , transcrite, archivée. Le livre n'est plus qu'un îlot dans un océan de texte.\n\nIl y a un défi empirique pour les sciences sociales, la société produit massivement la documentation de son développement. L'exploitation de ces sources est un enjeu majeur pour la psychologique, sociologique, économique, le droit, les sciences de gestion en ouvrant un nouvel accès au données.\n\nA mesure que ces données prolifèrent, des méthodes pour les analyser se sont développées à grande vitesse, moins pour des motivations d'études que pour répondre à des besoins opérationnels. Aujourd'hui, dans le sillage de l'invention des embeddings, nous sommes à l'ère des grands modèles de langages qui ouvrent de nouveaux horizons dans la capacité de produire du texte (par exemple des descriptions d'images), de transformer le texte (résumé ou traduction), et surtout de l'annoter (classification, NER...)\n\n## Le but de l'ouvrage\n\nCe e-book est le syllabus d'un cours que nous dispensons sous différents formats et avec différents degrés d'approfondissement. Il fait l'inventaire des méthodes les plus courantes incluant le développement des embeddings et aboutit à l'usage des LLM pour différentes formes d'annotations. Il a une vocation pratique, étudier des cas, et décrire des codes.\n\n## Les contributeurs\n\nOn ne peut (encore) en  produire la liste exhaustives, mais plusieurs générations d'étudiants ont contribué à ce travail en explorant un certain nombre de jeux de données proposés ici.\n\n## Le plan du cours\n\nIl s'organise selon les grandes étapes du processus d'analyse et des types de problèmes posés par le traitement et l'analyse du texte. Ce processus va de l'acquisition des données à leurs représentations en passant par des phases de transformation, mais aussi un ordre de complexité des modèles et des ressources.\n\nIl suit ainsi une sorte de développement historique qui se construit par l'accumulation de différentes philosophies d'analyses et de méthodes et qui peut se présenter en trois grandes périodes:\n\n-   compter les mots et jouer avec leur co-occurences\n\n-   annoter des mots en s'appuyant sur leurs régularités syntaxiques.\n\n-   encoder les mots dans un espace vectoriel, cette perspective a été ouverte par word2vec , étendue avec les transformers, systématisée par les grands modèles de langages tels que gpt4 ou Bloom.\n\nVoici les raisons qui organisent le cours en 20 chapitres (c'est à ajuster au cours de la rédaction)\n\n-  Préambule (ce que vous êtes en train de lire !)\n\n-  Chapitre 1 : une introduction générale à des éléments linguistique et technique et sociaux de l'analyse du langage naturel et de ses fulgurantes évolutions au cours de la dernière décennie.\n\n-  Chapitre 2 : Constituer des corpus. Pour les sciences sociales, le texte est un document qu'on étudie par collection. Il faut aussi penser à la notion de corpus.\n\n-  Chapitre 4 : corpus - techniques avancées D'un point de vue matériel le corpus est aussi\n\n-  Chapitre 5 : Explorer et naviguer dans le corpus\n\n-  Chapitre 6 : Analyse quantitative du corpus\n\n-  Chapitre 7 : Tokenisation et dtm\n\n-  Chapitre 8 : Analyse des co-occurences\n\n-  Chapitre 9 : au début il y avait l'analyse factorielle\n\n-  Chapitre 10 : SVD et LSA :\n\n-  Chapitre 11 : Topic model\n\n-  Chapitre 12 : word2vec to doc2vec\n\n-  Chapitre 13 : Transformers\n\n-  Chapitre 14 : Supervised modeling\n\n-  Chapitre 15 : LLM\n\n-  Chapitre 16 : NER\n\n-  Chapitre 17 : abstracting\n\n-  Chapitre 18 : generative models , un monde à inventer.\n\n\n## Data sets\n\nOn s'appuiera sur des data test \"réel\" et publics\n\n-   **[Trump Tweeter Archive](https://www.thetrumparchive.com/)** : c'est un site qui rassemble tous les tweets émis par Donald Trump depuis 2010, et propose un outil de navigation.\n\n-   **Tripadvisor en Polynésie** constitués par Gwevy et Chabrier de l'Université de Polynésie.\n\n-   **Airbnb Paris 2023** à partir de [Inside Airbnb](https://insideairbnb.com/)\n\n-   **PMP40ans** : ce set de données comprend tous les résumés d'articles publiés par la revue Politique et Management Public sur une période de 40 années de sa naissance en 1983 jusqu'en 2023. C'est un observatoire intéressant d'une discipline naissance et de l'évolution des normes professionnelles de la recherche en gestion.\n\n-   **RapLyrics** : Une collection de texte de rap français des années 80 à 2020 constituées par un groupe d'étudiants. \n\n\n## Packages\n\nCe cours utilise les ressources de r, de [rstudio et de Quarto](https://posit.co/), de l'analyse des données à la publication de ce book. On utilise [Zotero](https://www.zotero.org/) pour gérer la bibliographie.Il n'y a jamais une solution unique, nous avons fait des choix de méthodes qui se reflètent dans celui des packages dont voici la liste commentée. Pour l'usage des modèle LLM on basculera progressivement en python qui propose immensément plus de ressources. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#environnement de travail\nlibrary(tidyverse) # the perfect tool box. Comprend dplyr pour la gestion des données, ggplot pour les graphiques, et d'autres packages utiles comme lubreidate ou stringr.\nlibrary(flextable) # pour produire des tableaux élégants\n\n#ses compléments  \nlibrary(ggwordcloud) ##complete ggplot pour les nuages de mots\nlibrary(ggrepel) ## pour éviter la superposition de label\n\n#glosaire \nlibrary(glossary)\nglossary_path(\"glossary.yml\")\n# un exemple pour ajouter des termes au cours de la rédaction\n#glossary_add(term = \"loi de Dirichlet\",\n#             def = \"la loi de Dirichlet, souvent notée Dir(α), est une famille de lois de probabilité continues pour des variables #aléatoires multinomiales.(Ex. probabilités d'un mot parmi les trois milles d'usage courant\")\n\n#ressource nlp  \nlibrary(quanteda) \nlibrary(udpipe)\n\n#analyse de données\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n#python\nlibrary(reticulate)\n```\n:::\n\n\n\n\n\n## Ressources complémentaires\n\n-  [r en français](https://larmarange.github.io/analyse-R/)\n-  On encourage vivement le lecteur à jouer avec les modèles de [Hugging Face](https://huggingface.co/models)\n-  Le [Stanford Natural Language Processing Group](https://nlp.stanford.edu/) est sans doute la principale institution scientifique dans ce domaine.\n-  En France : Acss-PSL, Ollion, science po ...\n\nPour aller plus loin, il est possible de suivre le [NLPWorkshop de Acss](https://acss-dig.psl.eu/fr/seminaires/nlp) ou le cours \" introduction au NLP pour les sciences sociales \" dispensé chaque année en novembre au sein de la PSL week. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}