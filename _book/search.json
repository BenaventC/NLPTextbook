[
  {
    "objectID": "q07_tokenisation.html",
    "href": "q07_tokenisation.html",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "8.1 Quelques exercices de tokenization",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#quelques-exercices-de-tokenization",
    "href": "q07_tokenisation.html#quelques-exercices-de-tokenization",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "8.1.1 Les lettres\nCommençons par un exemple simple, à l’aide d’une courte citation de Max Weber. On choisit les lettres pour unité de découpe, et l’on utilise le package ‘tokenizer’. Automatiquement, ‘tokenizer’ met le texte en minuscule et élimine la ponctuation\n\n\nShow the code\n#Les données\nMaxWeber &lt;- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.\")\n\n#On tokenise, plus on transforme en dataframe le résultat.\ntoc_maxweber&lt;-tokenize_characters(MaxWeber)%&gt;%\n        as.data.frame()%&gt;%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo&lt;-toc_maxweber %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=n())%&gt;%\n        filter(n&gt;0)\n\n#On représente par un diagramme en barre cette distribution des occuences d'apparition, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=10, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+\n        labs(title = \"Fréquence des tokens, unité = lettres\", \n             x=\"tokens\", \n             y=\"nombre d'occurences\", \n             caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n\n\n\n\n\n\n\n\n\n\n\n8.1.2 les mots\n\n\nShow the code\n#Les données\n\nMaxWeber &lt;- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains. La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de l’intérêt général.\")\n\n#On tokenise, plus on transforme en dataframe le résultat. le strp_punc écarte la ponctuation du processus.\ntoc_maxweber&lt;-tokenize_words(MaxWeber,strip_punct=TRUE)%&gt;%\n        as.data.frame()%&gt;%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=sum(n))\n\n#On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=4, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"Fréquence des tokens, unité = mots\", x=\"tokens\", y=\"nombre d'occurences\", caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n\n\n\n\n\n\n\n\n\nOn peut également constater que certains mots sont proches, par exemple les deux derniers sur le graphiques précédents qui sont des déclinaisons du verbe appliquer. Il peut alors être pertinent de regrouper ces différentes formes verbales (comme un mot au singulier et au pluriel, au féminin et au masculin, ou conjugué sous différentes formes), pour faciliter l’analyse. C’est ce qu’on fait avec les opérations de stemming ou de lemmatisation, présentées au chapitre 8.\n\n\n8.1.3 Les phrases\nOn reproduit les mêmes opérations, mais cette fois sur les phrases de l’exemple précédent.\n\n\nShow the code\ntokenize_sentences(MaxWeber)%&gt;%\n  as.data.frame()%&gt;%\n  rename(tokens=1)%&gt;%\n  flextable(cwidth = 6)\n\n\ntokensBureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés.Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent.Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de l’intérêt général.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#n-grammes",
    "href": "q07_tokenisation.html#n-grammes",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "8.2 N-grammes",
    "text": "8.2 N-grammes\nLes n-grammes sont des séquences de n tokens, généralement consécutifs. Sur la d’un base d’un corpus important on peut calculer la probabilité d’apparition d’un n-gramme. C’est l’exercice auquel une équipe de google s’est attelé avec le Books Ngram Viewer. et dont nous encourageons l’étude détaillée.\nL’article original doit aussi être lu. Sur la base de 5 millions de livres numérisés en 2011 et représentant 4% du nombre total de livres à jamais publiés à ce moment, être lu aussi, il montre que, pour l’anglais, chaque année 8500 mots entrent dans le lexique, sans que les dictionnaires, pour des raisons évidentes de concision, n’en font l’inventaire. Le webster comprend 300 000 mots, alors que pour l’anglais l’estimation du nombre de termes différents, est passé de l’ordre de 500 000 en 1900, à plus d’1.5 million en 2000. Il donne aussi une dimension au lexique : celle de sa densité. Dans un corpus, à un moment donné quelle est la probabilité d’observer un token donné (au plus simple un mot). Sur un corpus de 500 milliards de termes (pour toutes les langues traitées), un mot qui apparaît une fois sur 1000 ( 10^-3), apparaît 5 millions de fois, s’il apparaît une fois sur 1 million, il apparaît 5000 fois. L’échelle de mesure est ainsi la fréquence : 10-3, 10-5, 10-6 ….\nLa notion de ngram au-delà de son apparence de tautologie possède aussi un intérêt théorique majeure. Les ngram sont des chaines de markov. Intérêt théorique des n gram : ce sont des chaines de markov.\nProcessus de markov\nPour comprendre l’importance de ce concept on peut considérer au moins 3 applications remarquables\nApplication à l’auto-complétion . Son efficacité dépend naturellement de la taille des données récoltées. On comprend que pour google il est aisé d’être précis dans l’estimation de ces probabilités!\nMais mieux encore, ces propriété markovienne ( probabilistique) permettent aussi la correction d’erreur avec l’algorithme de Viterbi https://fr.wikipedia.org/wiki/Algorithme_de_Viterbi\n\nLe principe de ‘textcat’ est fondée sur ces n-grammes de lettre. Chaque langue se caractérise par une distribution particulière des n-grammes. Pour décider de l’appartenance d’un texte à une langue, si on dispose des profils de distribution, on compare la distribution des n-grammes du texte à ces références. On peut ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche.\n\n\n8.2.1 Mise en oeuvre\nSi la notion est simple, sa mise en oeuvre l’est presque autant. Nous l’illustrons avec une séries d’exemples et le package “tokenizer” qui a ici un avantage pédagogique. On étudiera plus loin les ressources de “quanteda”.\nOn commence de suite par un exemple sur les lettres. c’est de pure forme. On sélectionne les ngrammes (k&gt;1 et k&gt;4) dont la fréquence est supérieure à trois.\n\n\nShow the code\n#tokenization des lettres\ntoc_maxweber&lt;-tokenize_character_shingles(MaxWeber,n=3, n_min=2) %&gt;%\n        as.data.frame()%&gt;%\n  rename(tokens=1)\n\nflextable(head(toc_maxweber, n=20))\n\n\ntokensbuburururerereaeaeauauaucucucrcrcrararatatatititie\n\n\nShow the code\nfoo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=sum(n))%&gt;%\n  filter(n&gt;3)\n\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n  annotate(\"text\", x=5,y=11, label=paste(\"nombre total de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"digrammes et trigrammes des lettres\", x=\"n-gramme\", y=\"nombre d'occurences\")\n\n\n\n\n\n\n\n\n\nOn peut faire la même chose sur les mots, et en plus en éliminant les stopwords. l’intérêt de la procédure est de se concentrer sur les idées, des paires de mots consistants du point de vue sémantique. Un ordre ce dégage ! la bureaucratie est un moyen, puis une rationalité qui dépend d’un exercice.\nOn note qu’il n’y a pas besoin de beaucoup de mots pour produire du sens par l’analyse statistique de leurs distributions.\n\n\nShow the code\ntoc_maxweber&lt;-tokenize_ngrams(MaxWeber,\n                              n=3, \n                              n_min=2, \n                              stopwords = stopwords('fr')) %&gt;%\n  as.data.frame()%&gt;%\n  rename(tokens=1)\n\nqflextable(head(toc_maxweber, n=19))\n\n\ntokensbureaucratie moyenbureaucratie moyen plusmoyen plusmoyen plus rationnelplus rationnelplus rationnel l’onrationnel l’onrationnel l’on connaissel’on connaissel’on connaisse exercerconnaisse exercerconnaisse exercer contrôleexercer contrôleexercer contrôle impératifcontrôle impératifcontrôle impératif êtresimpératif êtresimpératif êtres humainsêtres humains\n\n\nOn peut également s’intéresser aux n-grammes non directement consécutifs mais séparés par k tokens. C’est sans doute un moyen de saisir des corrélations de mots à plus grande distance. La possibilité formelle est là , nous avouons ne pas savoir à quel usage elle correspond. Nous serions sur ce point ravi d’avoir des réponses.\n\n\nShow the code\ntoc_maxweber&lt;-tokenize_skip_ngrams(MaxWeber,\n                                   n=3, \n                                   n_min=2, \n                                   k=2, \n                                   stopwords = stopwords('fr')) %&gt;%\n        as.data.frame()%&gt;%rename(tokens=1)\nqflextable(head(toc_maxweber, n=19))\n\n\ntokensbureaucratie moyenbureaucratie plusbureaucratie rationnelbureaucratie moyen plusbureaucratie moyen rationnelbureaucratie moyen l’onbureaucratie plus rationnelbureaucratie plus l’onbureaucratie plus connaissebureaucratie rationnel l’onbureaucratie rationnel connaissebureaucratie rationnel exercermoyen plusmoyen rationnelmoyen l’onmoyen plus rationnelmoyen plus l’onmoyen plus connaissemoyen rationnel l’on\n\n\nDans cet exemple, aucun n-gramme n’est répété, mais c’est rarement le cas avec des corpus plus importants. Dans ce cas, une forte répétition de n-grammes est un indice d’une unité sémantique composée de plusieurs tokens que l’on peut alors regrouper en un seul et même token. C’est ce que l’on verra dans la section suivante, avec la méthodes des collocation,",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#choisir-des-n-grammes-pertinents",
    "href": "q07_tokenisation.html#choisir-des-n-grammes-pertinents",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "8.3 Choisir des n-grammes pertinents",
    "text": "8.3 Choisir des n-grammes pertinents\nDans ce e-book l’unité principale d’analyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur d’un mot, une valeur sémantique, par exemple, l’expression “Assemblée Nationale”. Ces deux mots réunis constituent un syntagme, une unité de sens. La question qui se pose est alors de savoir comment les identifier dans le flot des n-grammes ?\nLa technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités d’apparition laisse espérer, c’est qu’ils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non.\nLe package quanteda propose une bonne solution à ce problème avec la fonction collocation.\n\n8.3.1 Créer les tokens avec ‘quanteda’\nÀ partir du corpus des commentaires de TripAdvisor concernant les hôtels de Polynésie Française,\n\n\nShow the code\n#les données\nAvisTripadvisor&lt;-read_rds(\"./data/AvisTripadvisor.rds\")\nAvisTripadvisor$Taille_hotel&lt;-as.character(AvisTripadvisor$Taille_hotel)\nAvisTripadvisor$Taille_hotel[is.na(AvisTripadvisor$Taille_hotel)]&lt;-\"Autre\"\n\n\n#création du corpus\ncorpus&lt;-corpus(AvisTripadvisor,docid_field = \"ID\",text_field = \"Commetaire\", docvars =AvisTripadvisor)\nft&lt;- head(corpus,3) %&gt;% as.data.frame()%&gt;%\n  rename(tokens=1)%&gt;%\n  flextable(cwidth = 6)\nft\n\n\ncorpustokensTout est magnifique au Vahine island. Séjour de rêve avec une équipe très chaleureuse . Le cadre est splendide. Notre meilleur hébergement en Polynésie.Merci à toute l' équipe pour votre gentillesse et vos nombreuses attentions.Tout était parfait, notre meilleure expérience et plus belle découverte en Polynésie.Un grand merci à Amélie et toute son équipe pour un service de très haut niveau.De la chambre, en passant par la restauration et les activités, tout était parfait. Encore merci à tous, en espérant revenir très vite.Un séjour magnifique, 3 jours époustouflants, accueillis chaleureusement par toute l’équipe du Vahine Island. Tout est au rendez vous, la cuisine délicieuse, le lieu au delà de notre imagination, un calme parfait. À faire, re faire, sans jamais s’en lasser.\n\n\nOn crée un objet de format token, avec la fonction tokens de quanteda, et on choisit d’enlèver la ponctuation, les symboles et les nombres avec les arguments correspondants.\nNotre tokenizer ici a quelques problèmes . Si le “.” n’est pas suivi d’un espace, il ne peut distinguer les mots, il ne saisit pas non plus l’ellipse de “l’équipe”, qui devrait distinguer le determinant “la” de “équipe. Souvent il faudra au préalable remanier les chaines de caractère pour une meilleure qualité de tokenisation.\n\n\nShow the code\n#transformation en objet token\ntok&lt;-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)\n\nhead(tok,3) \n\n\nTokens consisting of 3 documents and 21 docvars.\n1 :\n [1] \"Tout\"       \"est\"        \"magnifique\" \"au\"         \"Vahine\"    \n [6] \"island\"     \"Séjour\"     \"de\"         \"rêve\"       \"avec\"      \n[11] \"une\"        \"équipe\"    \n[ ... and 22 more ]\n\n2 :\n [1] \"Tout\"         \"était\"        \"parfait\"      \"notre\"        \"meilleure\"   \n [6] \"expérience\"   \"et\"           \"plus\"         \"belle\"        \"découverte\"  \n[11] \"en\"           \"Polynésie.Un\"\n[ ... and 37 more ]\n\n3 :\n [1] \"Un\"              \"séjour\"          \"magnifique\"      \"jours\"          \n [5] \"époustouflants\"  \"accueillis\"      \"chaleureusement\" \"par\"            \n [9] \"toute\"           \"l’équipe\"        \"du\"              \"Vahine\"         \n[ ... and 27 more ]\n\n\nAu passage, on peut aussi enlever les stopwords. C’est a dire les mots d’usages courants mais sans signification intrinsèque : les conjonctions, les déterminants, etc, qui se présentent sous la formes de dictionnaires.\nPour que les n-grammes très fréquents restent des syntagmes signifiants, on laisse apparentes les positions des stopwords, avec l’option padding= TRUE.\n\n\nShow the code\n#enlever les stopwords\ntok&lt;-tokens_remove(tok,stopwords('fr'),padding=FALSE)\nhead(tok, 3)\n\n\nTokens consisting of 3 documents and 21 docvars.\n1 :\n [1] \"Tout\"        \"magnifique\"  \"Vahine\"      \"island\"      \"Séjour\"     \n [6] \"rêve\"        \"équipe\"      \"très\"        \"chaleureuse\" \"cadre\"      \n[11] \"splendide\"   \"meilleur\"   \n[ ... and 7 more ]\n\n2 :\n [1] \"Tout\"         \"parfait\"      \"meilleure\"    \"expérience\"   \"plus\"        \n [6] \"belle\"        \"découverte\"   \"Polynésie.Un\" \"grand\"        \"merci\"       \n[11] \"Amélie\"       \"toute\"       \n[ ... and 18 more ]\n\n3 :\n [1] \"séjour\"          \"magnifique\"      \"jours\"           \"époustouflants\" \n [5] \"accueillis\"      \"chaleureusement\" \"toute\"           \"l’équipe\"       \n [9] \"Vahine\"          \"Island\"          \"Tout\"            \"rendez\"         \n[ ... and 13 more ]\n\n\n\n\n8.3.2 Application à la détection des entités nommées\nOn cherche ici à identifier les noms propres présents dans le corpus.\n\n\nShow the code\nlibrary(quanteda.textstats)\n#on sélectionne les mots commençant par une majuscule\ntoks_cap &lt;- tokens_select(tok, \n                               pattern = \"^[A-Z]\",\n                               valuetype = \"regex\",\n                               case_insensitive = FALSE, \n                               padding = TRUE)\n\n#on cherche les collocations\ntstat_col_cap &lt;- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE)\n\nflextable(head(as.data.frame(tstat_col_cap)))\n\n\ncollocationcountcount_nestedlengthlambdazBora Bora155026.60401653.27337Sylvie Yves21028.18378325.93161Muriel Franck19027.14159224.46065Corinne Frédéric17028.60970123.23737Pearl Beach13028.03793521.75092Yves Sylvie13027.28575421.32976\n\n\n\n\n8.3.3 Composer des tokens à partir d’expressions multi-mots : collocation\nDans ce corpus, les noms propres correspondent aux noms des îles et des hôtels, et aux prénoms composés. La valeur du lambda montre la force de l’association entre les mots, on retiendra d’une manière générale un lambda au moins supérieur à 3 pour remplacer les tokens d’origine par leurs n-grammes.\n\n\nShow the code\ntoks_comp &lt;- tokens_compound(tok, pattern = tstat_col_cap[tstat_col_cap$z &gt; 3,], \n                             case_insensitive = FALSE)\n\nhead(toks_comp)\n\n\nTokens consisting of 6 documents and 21 docvars.\n1 :\n [1] \"Tout\"        \"magnifique\"  \"Vahine\"      \"island\"      \"Séjour\"     \n [6] \"rêve\"        \"équipe\"      \"très\"        \"chaleureuse\" \"cadre\"      \n[11] \"splendide\"   \"meilleur\"   \n[ ... and 7 more ]\n\n2 :\n [1] \"Tout\"         \"parfait\"      \"meilleure\"    \"expérience\"   \"plus\"        \n [6] \"belle\"        \"découverte\"   \"Polynésie.Un\" \"grand\"        \"merci\"       \n[11] \"Amélie\"       \"toute\"       \n[ ... and 18 more ]\n\n3 :\n [1] \"séjour\"          \"magnifique\"      \"jours\"           \"époustouflants\" \n [5] \"accueillis\"      \"chaleureusement\" \"toute\"           \"l’équipe\"       \n [9] \"Vahine_Island\"   \"Tout\"            \"rendez\"          \"cuisine\"        \n[ ... and 12 more ]\n\n4 :\n [1] \"Vraiment\"    \"beau\"        \"cadre\"       \"idyllique\"   \"personnel\"  \n [6] \"très\"        \"attentionné\" \"adoré\"       \"séjour\"      \"attention\"  \n[11] \"spéciale\"    \"voyage\"     \n[ ... and 15 more ]\n\n5 :\n [1] \"Vahiné_Island\" \"entre\"         \"monde\"         \"parallèle\"    \n [5] \"où\"            \"sait\"          \"plus\"          \"très\"         \n [9] \"bien\"          \"tient\"         \"réel\"          \"tient\"        \n[ ... and 60 more ]\n\n6 :\n [1] \"adoré\"         \"séjour\"        \"Vahiné_Island\" \"enfants\"      \n [5] \"bungalow\"      \"pilotis\"       \"bungalow\"      \"plage\"        \n [9] \"c'était\"       \"paradis\"       \"motu\"          \"idyllique\"    \n[ ... and 40 more ]\n\n\n\n\n8.3.4 Identifier les autres concepts\nDans ce corpus, on peut aussi s’attendre à voir apparaître d’autres expressions multi-mots qui représentent des concepts, telles que “petit déjeuner”.\n\n\nShow the code\ncol&lt;-textstat_collocations(toks_comp, min_count = 10)\n\nflextable(head(as.data.frame(col)))\n\n\ncollocationcountcount_nestedlengthlambdazpetit déjeuner769026.95571479.38781très bien714023.02262164.09272très bon422023.50611653.16466salle bain274028.72414252.20121très agréable374023.63673750.52490grand merci167025.11229550.38965\n\n\nAu vue de la diversité des collocations, on choisit un lambda supérieur à 7 pour retenir les concepts les plus pertinents.\n\n\nShow the code\ntoks_comp &lt;- tokens_compound(tok, pattern = col[col$z &gt; 7,])\n\nhead(toks_comp)\n\n\nTokens consisting of 6 documents and 21 docvars.\n1 :\n [1] \"Tout\"            \"magnifique\"      \"Vahine\"          \"island\"         \n [5] \"Séjour_rêve\"     \"équipe\"          \"très\"            \"chaleureuse\"    \n [9] \"cadre_splendide\" \"meilleur\"        \"hébergement\"     \"Polynésie.Merci\"\n[ ... and 4 more ]\n\n2 :\n [1] \"Tout_parfait\" \"meilleure\"    \"expérience\"   \"plus_belle\"   \"découverte\"  \n [6] \"Polynésie.Un\" \"grand_merci\"  \"Amélie\"       \"toute_équipe\" \"service\"     \n[11] \"très\"         \"haut\"        \n[ ... and 10 more ]\n\n3 :\n [1] \"séjour\"             \"magnifique\"         \"jours\"             \n [4] \"époustouflants\"     \"accueillis\"         \"chaleureusement\"   \n [7] \"toute_l’équipe\"     \"Vahine\"             \"Island\"            \n[10] \"Tout\"               \"rendez\"             \"cuisine_délicieuse\"\n[ ... and 11 more ]\n\n4 :\n [1] \"Vraiment\"                   \"beau\"                      \n [3] \"cadre_idyllique\"            \"personnel_très_attentionné\"\n [5] \"adoré_séjour\"               \"attention\"                 \n [7] \"spéciale\"                   \"voyage_noce\"               \n [9] \"soirée\"                     \"d’anniversaire\"            \n[11] \"paysage\"                    \"magnifique\"                \n[ ... and 9 more ]\n\n5 :\n [1] \"Vahiné\"    \"Island\"    \"entre\"     \"monde\"     \"parallèle\" \"où\"       \n [7] \"sait\"      \"plus\"      \"très_bien\" \"tient\"     \"réel\"      \"tient\"    \n[ ... and 45 more ]\n\n6 :\n [1] \"adoré_séjour\"     \"Vahiné\"           \"Island\"           \"enfants\"         \n [5] \"bungalow_pilotis\" \"bungalow_plage\"   \"c'était\"          \"paradis\"         \n [9] \"motu\"             \"idyllique\"        \"situé\"            \"lagon\"           \n[ ... and 33 more ]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#des-tokens-au-dtm",
    "href": "q07_tokenisation.html#des-tokens-au-dtm",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "8.4 Des tokens au dtm",
    "text": "8.4 Des tokens au dtm\nD’un point de vue pratique le nombre de colonne est de l’ordre de plusieurs milliers. Dans le cas de texte courts, quelques dizaines, ou même centaine de mots, moins de quelques pour-cent des lignes auront une valeur. C’est un tableau creux (sparse matrix) qui peut nécessiter des représentations particulières et plus économes.\n\n\nShow the code\n#on transforme en document-features matrix pour des représentations graphiques \n\ndfm&lt;-dfm(toks_comp,remove_padding=TRUE) \n\ndfm &lt;- dfm_remove(dfm, stopwords(\"fr\"))\n\ndfm_top &lt;- topfeatures(dfm, n = 80, decreasing = TRUE,  scheme =\"count\")\n\ndfm_top\n\n\n             a           très           plus           tout           bien \n          2345           1355           1194           1141            930 \n       pension        chambre          hôtel       bungalow             si \n           896            883            804            775            748 \n         c'est     restaurant            peu          plage           fait \n           740            716            702            670            663 \n           car          repas         séjour      personnel          faire \n           623            606            600            577            551 \n     bungalows        service          merci       vraiment     magnifique \n           527            480            479            475            472 \n         comme        piscine          lagon          aussi        cuisine \n           469            469            467            463            453 \n      chambres          c’est           donc        accueil petit_déjeuner \n           450            448            436            432            427 \n       l'hôtel          petit          super           tous            vue \n           426            419            395            392            386 \n       l’hôtel           soir           prix        surtout          juste \n           386            377            371            359            351 \n            où       toujours            bon            top        qualité \n           348            348            346            343            315 \n         quand        endroit       agréable           deux      polynésie \n           315            313            313            306            305 \n       famille          cadre      l'accueil       terrasse           nuit \n           303            302            299            294            288 \n    recommande          d'une           rien          calme           motu \n           284            284            281            279            276 \n     également        superbe             ça          n'est    gentillesse \n           275            267            266            265            257 \n          lieu       poissons       beaucoup          après      très_bien \n           257            254            254            253            248 \n          trop           être   restauration        journée          temps \n           247            247            246            246            244 \n\n\nShow the code\n#un nuage de mots rapide library(quanteda.textplots) \n\ntextplot_wordcloud(dfm, max_word=80, color = rev(RColorBrewer::brewer.pal(6, \"RdBu\")))\n\n\n\n\n\nMots les plus fréquents du corpus\n\n\n\n\nShow the code\ndfm2&lt;-dfm %&gt;% dfm_subset(!is.na(Taille_hotel))%&gt;%\n  dfm_group(groups = Taille_hotel)\n\n\ntextplot_wordcloud(dfm2, comparison = TRUE, max_words = 200,\n                   color = c(\"blue\", \"red\"))\n\n\n\n\n\nMots les plus fréquents du corpus",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#pondérer-la-fréquences-des-termes",
    "href": "q07_tokenisation.html#pondérer-la-fréquences-des-termes",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "8.5 Pondérer la fréquences des termes",
    "text": "8.5 Pondérer la fréquences des termes\nDans un dtm, chaque cellule du tableau indique la fréquence d’un mot dans un document. Cette mesure cependant peut ne pas être pertinentes. Les mots les plus fréquents peuvent se distribuer également entre les documents, et leur fréquence indique au mieux un lieu commun.\nL’idée clé va être de pondérer la fréquence d’un terme dans un document par la présence de ce terme à travers les documents. S’il se retrouve partout on en minore le poids, s’ils est présent dans une petit groupe de document on va lui donner de l’importance.\n\n8.5.1 le concept de tfidf\nCette idée est formalisée par le critère du TfIdf. La fréquence d’un terme dans un document (Tf) par être pondéré par l’inverse de la fréquence des documents dans lesquels il est présent (Idf). S’il est présent partout (c’est le cas des verbes auxiliaires) sont poids sera minimal, s’il est concentré dans un petit nombre de documents on lui donnera un poids plus important.\nFormalisation\nla fréquence brute des termes est le rapport du nombre de termes i dans le document d sur le nombre de termes du document i\n\\(tf_{id}= n_{id}\\)\nCette fréquence peut être modulée par le nombre de mots dans le texte, cette fréquence relative permet d’avoir une idée de la densité d’un terme dans un document.\n\\(tf_{id}= n_{id}/\\sum_{i}(n_{id}\\)\nla pondération\n\\(idf_{id}= log10(D_{}/D_{i})\\)\nCe concept souligne une chose capitale : un terme, ou token, dans un corpus, est caractérisé par une double distribution : d’une part sa part dans l’ensemble des termes de références, et d’autre part, sa présence à travers les documents.\n\n\n8.5.2 D’autres variantes\nCelles de l’idf https://programminghistorian.org/fr/lecons/analyse-de-documents-avec-tfidf\nle bm25 est une généralisation à une requête Q de plusieurs termes et vise à qualifier la pertinence\non développe avec une étude empirique.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#conclusion",
    "href": "q07_tokenisation.html#conclusion",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "8.6 Conclusion",
    "text": "8.6 Conclusion\nDans ce chapitre, nous avons vu comment découper un corpus en unités, les tokens. Nous avons abordé le sujet des n-grammes, et vu comment composer des tokens à partir de concepts multi-mots, identifiés par des n-grammes adjacents.\nOn conclue sur l’idée que la distribution des termes d’un texte doit se comprendre dans deux dimensions : la fréquence des termes parmi tous les termes, leur fréquence à travers les documents.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#footnotes",
    "href": "q07_tokenisation.html#footnotes",
    "title": "8  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "En français digramme est la meilleure traduction de bigram, une bigramme en français est un mot dont les lettres peuvent former deux autres mot.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q04_explorer.html#kwic",
    "href": "q04_explorer.html#kwic",
    "title": "5  Explorer le corpus",
    "section": "5.1 Kwic",
    "text": "5.1 Kwic\nLe premier réflexe dans la lecture d’un corpus est de rechercher dans quels contextes sont utilisé des mots cibles. C’est l’objet d’une vieille technique les : Key Word In Context.\nExemple :\n\n\nShow the code\ndf &lt;- read_csv(\"data/PMPLast3.csv\", locale = locale(encoding = \"WINDOWS-1252\"))%&gt;%\n   rename(Text=11, Year=3)\n \ncorp&lt;-corpus(df$Text)\n \nfoo&lt;- kwic(corp, \"évaluation\",\n  window = 6) %&gt;%\n  as.data.frame()\n\nset_flextable_defaults(\n  font.size = 10, theme_fun = theme_vanilla,\n  padding = 6,\n  background.color = \"#EFEFEF\")\n\nft&lt;-flextable(foo)\nft &lt;- set_caption(ft, caption = \"Keywords in context\") \nft\n\n\ndocnamefromtoprekeywordpostpatterntext312121des politiques municipales ? L 'évaluationde la qualification de ces consultationsévaluationtext631313- plus particulièrement les recherches enévaluationet les analyses avantage - coûtévaluationtext63110110, lorsqu'ils sont en présence d'uneévaluationqui conclut à l'échec ou àévaluationtext63169169l'autre , les leçons issues d'uneévaluationd'un programme . Cet article aévaluationtext796262décision facilement suspecté . L 'évaluationest donc nécessaire à la survieévaluationtext113137137point d'outils qui rendent possible uneévaluationdes politiques publiques .évaluationtext123261261pas sans un contrôle , uneévaluationdes politiques déjà mises en œuvreévaluationtext2255151à un recensement et à uneévaluationdes moyens à la disposition duévaluationtext257129129passer d'une notation traditionnelle à uneévaluationmoderne révèlent les contradictions entre cesévaluationtext2661919soit d'une fétichisation ( pour qu'uneévaluationsoit bonne et que ses conclusionsévaluationtext2769595présente une synthèse et une premièreévaluationde travaux effectués dans ce domaineévaluationtext2846565engendré des déceptions , accompagnées d'uneévaluationsans doute trop sommaire et rapideévaluationtext3138686actuelle , il propose ensuite uneévaluationcritique de la nouvelle organisation enévaluationtext3315656les Universités ont toutes réclamé leurévaluationqui leur permet de mieux seévaluationtext3417676sont associées , ainsi qu'une brèveévaluationde l'intérêt qu'elles présentent pour différentsévaluationtext356115115de coqueluche , nous effectuons uneévaluationcomparée des coûts de la maladieévaluationtext3673434l'évolution de ces projets affecte leurévaluationet par conséquent le choix deévaluationtext3939797encore peu utilisés , et l'auto-évaluationformalisée restait embryonnaire . Il fautévaluationtext439148148fourni le matériel empirique pour uneévaluationcritique des approches théoriques et desévaluationtext448173173on peut tenter de mener uneévaluationa priori de la mise enévaluationtext45355Quand on leur parleévaluation, beaucoup de décideurs pensent aussitôtévaluationtext454169169cet article est donc celle d'uneévaluationde processus , dont le principalévaluationtext4753636dresser le cahier des charges d'uneévaluationqui sera effectuée dans un secondévaluationtext47899un jugement de valeur , uneévaluationest un processus composé d'étapes dontévaluationtext48144La relation entreévaluationet budget peut être examinée àévaluationtext4855959les enjeux des politiques soumises àévaluation. Or , les acteurs ,évaluationtext492245245sur une autonomie accrue et uneévaluationdes résultats . Mais dans leévaluationtext4958888un modèle micro-économique reposant sur uneévaluationex-ante des performances des entreprises ,évaluationtext496166166mais qu'il est couplé à uneévaluationde la performance par objectifs procheévaluationtext50155Le texte propose uneévaluationde la performance financière des communesévaluationtext521347347enjeux de gestion . Une premièreévaluationde la démarche , courant 1997évaluationtext528233233, l'équilibre souhaité entre mesure etévaluation. La dernière partie s'attachera àévaluationtext528269269dichotomie pour systématiquement allier mesure etévaluation.évaluationtext5507171confrontés à la relation ambivalente entreévaluationet décision publique . Leurs modesévaluationtext607242242l’Évaluation , l'auteur plaide pour uneévaluationpragmatique et propose un changement deévaluationtext6117171bien souvent , la fiabilité d'uneévaluationne repose pas uniquement sur laévaluationtext6243131mal défriché . Pourtant , cetteévaluationest possible en suivant une procédureévaluationtext653146146regard de ses effets , cetteévaluationdite pluraliste s'avère être singulièrement incomplèteévaluationtext673129129communication étudie le lien complexe entreévaluationet décision . Diverses analyses montrentévaluationtext6955656savoir faire entrer , dans uneévaluationdite rationalisante des projets d'investissements ,évaluationtext706106106négociation des prochains contrats , uneévaluationpluriannuelle de la réalisation des premiersévaluationtext724235235conduites dans 1021 services et uneévaluationqualitative du dispositif apportent des éclairagesévaluationtext785151151. L’espace limité réservé à cetteévaluationoblige à forcer le trait .évaluationtext7875656qu'il est souvent difficile de concilierévaluationdes politiques publiques , d'une partévaluationtext7879696à se substituer à une réelleévaluationdes résultats et des effets finauxévaluationtext7946565temps , les modalités de cetteévaluation. La mise en évidence d’uneévaluationtext794123123de soins , complété par uneévaluationde la qualité , qui empruntentévaluationtext844110110mettons ensuite en évidence que cetteévaluationparticipe à la constitution progressive etévaluationtext8555555a évolué l’intérêt porté à cetteévaluationsur la période 1980-2007 dans cesévaluationtext855112112mettant en avant la logique d’uneévaluationactuelle en vue de la modernisationévaluationtext8776969publique , cette démarche propose uneévaluationintégrée à son contexte politique prenantévaluationtext8783535lancement de l’expérimentation et de sonévaluation, sa mise en œuvre etévaluationtext9022121communication financière sur Internet . Uneévaluationdes pratiques actuelles des collectivités émettricesévaluationtext916149149une double figure de l’innovation enévaluationdans les collectivités territoriales : l’innovationévaluationtext9386060des lois d’une part , entreévaluationex ante et ex post deévaluationtext9412626la portée dans le cadre d’uneévaluationde la politique publique éponyme .évaluationtext992101101( validation dans la pratique etévaluation) , grâce à la créationévaluationtext9937979sphères administrative et politique , sonévaluationsystématique et une réduction de laévaluation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Explorer le corpus</span>"
    ]
  },
  {
    "objectID": "q04_explorer.html#explorer-le-corpus",
    "href": "q04_explorer.html#explorer-le-corpus",
    "title": "5  Explorer le corpus",
    "section": "5.2 Explorer le corpus",
    "text": "5.2 Explorer le corpus\nAvant de procéder aux analyses du corpus, il est souvent utile de le représenter. On va utiliser le package Corpora explore à cette fin. Il permet de préparer un corpus et de le visualiser de manière interactive avec la génération d’une app shiny. Malheureusement nous ne savons pas rendre compte de la dynamique de l’outil. On peut naviguer aisément dans l’ensemble de texte.\nOn va utiliser une collection de donnée préparée avec Manel Benzarafa de l’Université paris Nanterre, et qui comprend l’intégralité des résumés, auteurs etc.. relatifs aux articles publiés dans PMP. Une base bibliographique intégrale. 1025 articles la compose.\n\n\nShow the code\nlibrary(readr) \n#install.packages(\"corporaexplorer\") \nlibrary(corporaexplorer)   \nlibrary(readr) \n\nfoo&lt;-df %&gt;% select(Key, Author, Title, Year, Text) %&gt;%\n  rename(year=Year)%&gt;%filter(Text!=\"Null\" & !is.na(year))    \n\ncorpus &lt;- prepare_data(foo, date_based_corpus =FALSE, grouping_variable = \"year\")\n\nexplore(corpus)\n\n\nShiny applications not supported in static R Markdown documents\n\n\nDans la photo d’écran suivante, on teste les termes ” politique” et “management”. Chaque tuile ( uile) représente un des 1025 abstracts qui composent le corpus. Les couleurs correspondent à la fréquence des deux termes.\n\n\n\nExploration des abstracts de PMP\n\n\n\n5.2.1 reprendre le topic de revtools dans topic model\nhttps://revtools.net/screening.html#screening-with-topic-models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Explorer le corpus</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#stringr",
    "href": "q05_manipuler_caractere.html#stringr",
    "title": "6  Manipuler les chaines de caractères",
    "section": "6.1 Stringr",
    "text": "6.1 Stringr\nC’est l’outil général que nous favorisons , il est intégré dans la suite tidyverse et propose de nombreuses fonctions.\nhttps://stringr.tidyverse.org/index.html\n\n6.1.1 Compter :\nDans le chunck suivant on utilise le set de donnée PMP40ans. Le but est de compter le nombre de fois où un terme apparaît dans chacun des textes avec la fonction str_count. On en donne l’évolution au cours du temps\n\n\nShow the code\nfoo &lt;- df %&gt;% \n  dplyr::select(Year, Text)%&gt;% \n  filter(!is.na(Text))%&gt;% #on filtre les textes manquants\n  mutate(n=str_count(Text, \"évaluation\")) %&gt;% #\n  group_by(Year)%&gt;% #on agrège sur l'année \n  summarise(n=sum(n))\n\nggplot(foo, aes(Year,n))+geom_line() +\n  geom_smooth()+\n  ylim(0,40)\n\n\n\n\n\n\n\n\n\n\n\n6.1.2 Extraire\nDans certaines situations on peut souhaiter extraire une chaîne de caractère pour l’utiliser à un autre usage.L’exemple ordinaire est celui d’extraire l’année d’une date. ou un auteur\nstr_extract\n\n\n6.1.3 Remplacer\nstr_replace",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#regex",
    "href": "q05_manipuler_caractere.html#regex",
    "title": "6  Manipuler les chaines de caractères",
    "section": "6.2 Regex",
    "text": "6.2 Regex\nDans la section précédente on a appris à rechercher une chaîne de caractères, ou plusieurs dans une chaîne de caractères, à les compter, les extraire, les enlever, les remplacer. Dans nos exemples, on a employé des pattern simple. Dans l’exemple “évaluation”, on a tenu compte uniquement de cette forme, elle aurait pu être au pluriel, porté une majuscule, exacte ou inexacte. L’enjeu est de pouvoir saisir toute les variantes en une seule expression.\nDans notre exemple la solution est la suivante : [E|e|é]valuation.*, en voulant saisir plus de variation on peut employer [E|e|é]valu.* qui permet se saisir les former verbales.\nUne expression régulière, ou regex, ou expression rationnelle ou expression normale ou motif est une chaîne de caractères qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Son invention est attribuée au logicien Stephen Cole Kleene)\nsa pratique se fonde sur une forte théorie.\n\n6.2.1 les éléments principaux\nhttps://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432\nLes regex reposent sur plusieurs concepts :\n\ndes jeux de caractères qui sont défini par des crochets []. Dans l’exemple suivant on remplace toutes les voyelles, par rien, puis toute les majuscules par X. Il y a différentes variantes par exemple [A-Z] pour les majuscules ou [0-9] pour les chiffres.\n\n\n\nShow the code\n# Mathias Malzieu\nx&lt;-c(\"Si Cendrillon avait eu une horloge dans le coeur, elle aurait bloqué les aiguilles à minuit moins une et se serait éclaté au bal toute sa vie.\")\n\nstr_replace_all(x,\"[aeiou]\", \"\")\n\n\n[1] \"S Cndrlln vt  n hrlg dns l cr, ll rt blqé ls glls à mnt mns n t s srt éclté  bl tt s v.\"\n\n\nShow the code\nstr_replace_all(x,\"[A-Z]\", \"X\")\n\n\n[1] \"Xi Xendrillon avait eu une horloge dans le coeur, elle aurait bloqué les aiguilles à minuit moins une et se serait éclaté au bal toute sa vie.\"\n\n\n\nmeta caractères Les méta caractères représentent un type de caractère. Ils commencent généralement par une barre oblique inverse (backslash). La barre oblique inverse étant un caractère spécial dans R, elle doit être échappée chaque fois qu’elle est utilisée avec une autre barre oblique inverse. En d’autres termes, R exige deux barres obliques inverses lors de l’utilisation de métacaractères. Chaque méta-caractère correspondra à un seul caractère.\n\n\\s : Ce méta caractère représente les espaces. Il correspondra à chaque espace, tabulation et nouvelle ligne.\n\nLes quantificateurs permette de contrôler le nombre de caractère que l’on attend. Par exemple , le quantificateur + indique que l’on souhaite que l’élément recherché apparaisse une ou plus fois. Si l’action est d’identifier a, a+, renvoie a, aa, aaa etc . Ceci ne semble pas très utile, sauf si le caractère peut être n’importe quelle lettre de l’alphabet, ce qui est représenté par le . Une variante du + est *, c’est la même idée mais cela inclue l’option de 0 caractère.\nGroupes de capture\n\n\n\n6.2.2 Des formules usuelles\nLa formulation de regex est un art, dans le quotidien on se contentera de reprendre des formules communes\nen voici quelles-unes\n\ndate “[0-9]{2}-[0-9]{2}-[0-9]{2}”\nmention\nadresse web\nnuméro de téléphone",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#conclusion",
    "href": "q05_manipuler_caractere.html#conclusion",
    "title": "6  Manipuler les chaines de caractères",
    "section": "6.3 Conclusion",
    "text": "6.3 Conclusion\nSur le plan pratique nous avons fortement avancé, nous pouvons jouer avec les chaînes de caractère, et en saisir les variations. nous avons les moyens de pré-traiter le texte\n\npour réduire les morphologies\npour extraire des entités nommées\n…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html",
    "href": "q06_Analyse_quantitative.html",
    "title": "7  Analyse Quantitative du corpus",
    "section": "",
    "text": "7.1 Comptons les mots\nIl y 56571 tweets et 9 variables et 1.117018^{6} mots cumulés.\nOn peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr, un package précieux que nous allons étudier de plus dans le chapitres suivant : str_count.\nOn note immédiatement la bi-modalité de la distribution qui correspond au changement de règle par Twitter en matière de non de caractères utilisés, étendu de 180 à 280.\nShow the code\nggplot(df, aes(x=nb_mots))+\n  geom_histogram(fill=col_1, binwidth = 10)+\n  labs(title=paste0(\"Nombre total de mots du corpus : \",sum_mots), \n       x=\"Nombre de mots par post\", \n       y=\"Fréquence\")\nLa bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de l’album de Joy Division : un graphique en crêtes (ridges plot) avec ggridges.\nLe résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à l’occasion d’autre contenu en 50 mots environ.\nShow the code\ndf$Year&lt;-format(df$date, format = \"%Y\") #on extrait l'année de la date\nfoo&lt;- df %&gt;% \n  filter(Year!=\"2021\") #parce qu'il n'y a quelques quelques jours en janvier, le compte a été interrompu vers le 5 janavier\n\nggplot(foo,aes(x = nb_mots, y = Year, group = Year)) +\n  geom_density_ridges(scale = 4, color=\"grey90\",fill=col_1, alpha=.7)+\n  theme_ridges() +\n  scale_x_continuous(limits = c(1, 60)) +\n  labs(x=\"Nombre de mots par post\",\n       y=NULL)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#la-production-dans-le-temps",
    "href": "q06_Analyse_quantitative.html#la-production-dans-le-temps",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.2 La production dans le temps",
    "text": "7.2 La production dans le temps\nExaminons le nombre de tweets produit au cours du temps.\nOn se rappellera qu’après une carrière immobilière menée dans les casinos, le golf et les hôtels, l’appétit médiatique de Trump s’est réalisé dans “the apprentice”, de 2004 à 2015. C’est un pro de la TV, il a une formation de Popstar. Il sera élu en Décembre 2016 pour prendre le pouvoir en Janvier.\n\n\nShow the code\ndf$date2&lt;-format(df$date, \"%Y-%m-%d\")\ndf$date3&lt;-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet&lt;-as.character(df$isRetweet)\n\nfoo&lt;-df %&gt;% filter(Year!=2021)%&gt;%\n  group_by(date3, isRetweet)%&gt;%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n\n\n\n\n\n\n\n\n\nShow the code\n#raf : labeliser avec les dates clés",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#popularité-des-tweets",
    "href": "q06_Analyse_quantitative.html#popularité-des-tweets",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.3 Popularité des tweets",
    "text": "7.3 Popularité des tweets\nDans ce set de donnée si l’auteur est unique, la réception est multiple, rappelons que près de 90 millions de personnes suivaient Trump. On possède deux indicateurs : le nombre de retweet et de RT.\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year!=2021)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(sum_rt=sum(retweets),\n            sum_fav=sum(favorites),\n            mean_rt=mean(retweets),\n            mean_fav=mean(favorites)\n            ) %&gt;%\n  pivot_longer(-Year,names_to = \"variable\", values_to = \"values\")\n\nggplot(foo, aes(x=Year, y=values,group=variable))+\n  geom_line(aes(color=variable), size=1,alpha=.8)+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_4)+ \n  facet_wrap(vars(variable), ncol=2, scale=\"free\")+\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nOn s’aperçoit d’un changement de régime, quand l’audience est limitée RT et fac sont fortement corrélés, l’entrée en politique de Trump se caractérise par un changement de régime. La décorrélation peut s’expliquer par l’usage d’agent d’influences, qui retweetent plus qu’ils n’approuvent. Ils contribuent à la décorrélation.\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year!=2021)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(cor(favorites, retweets)) %&gt;%\n  rename(correlation =2)\n            \nggplot(foo, aes(x=Year, y=correlation, group=1))+\n  geom_line(size=1,alpha=.8, color=col_1b)+\n  geom_smooth(color=col_1b, size=1.5, fill=\"grey90\")+\n  labs(y=\"Corrélation entre fav et rt\", x=NULL)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year==2020)\n\n\nm &lt;- ggplot(foo, aes(x = favorites, y = retweets)) +\n geom_point() +\nscale_x_log10()+\nscale_y_log10()\n\n# contour lines\nm + geom_density_2d()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf$date2&lt;-format(df$date, \"%Y-%m-%d\")\ndf$date3&lt;-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet&lt;-as.character(df$isRetweet)\n\nfoo&lt;-df %&gt;% filter(Year!=2021)%&gt;%\n  group_by(date3, isRetweet)%&gt;%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n\n\n\n\n\n\n\n\n\nShow the code\n#raf : labeliser avec les dates clés",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#lisibilité-et-complexité-lexicale",
    "href": "q06_Analyse_quantitative.html#lisibilité-et-complexité-lexicale",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.4 Lisibilité et complexité lexicale",
    "text": "7.4 Lisibilité et complexité lexicale\nPour aller un peu plus loin - nous savons désormais que Trump aime une forme courte en 21 mots, et que son expérience de twitter est longue, on peut s’intéresser à des paramètres clés relatifs aux conditions de la réception: les textes sont-ils aisés à lire ? sont-ils sophistiqués ?\nIntroduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. Ce sont des classiques, les critères initiaux de l’analyse quantitative du texte. Ils sont toujours utiles.\n\n7.4.1 Les indices de lisibilité\nLa lisibilité est une notion ancienne tout autant que sa mesure (par exemple Coleman and Liau (1975)). Elle répond à la question du degré de maîtrise requis pour lire un texte en s’appuyant sur les caractéristiques objectives du texte plutôt que sur sa perception. Il s’agissait donc d’évaluer la complexité d’un texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes, et la complexité des phrases mesurée par le nombre de mots.\nA partir de ces deux paramètres, une multitudes d’indicateurs ont été proposés. Dans l’exemple qui va suivre, on se contente d’un grand classique, le plus ancien, l’indice de Flesch (Flesch 1948) et de ses constituants : le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase.\nOn les retrouve, avec une autre grande variété, dans le package compagnon de quanteda , quanteda.textstats , qui en fournit des dizaines de variantes.\n\n\nShow the code\n#on sélectionne les tweets originaux\n\nfoo&lt;-df %&gt;% filter(isRetweet==FALSE) # on ne prend pas en compte les RT\n\n#la fonction de calcul de lisibilité\nreadability&lt;-textstat_readability(foo$text, \n                                  measure = c(\"Flesch\",\"meanSentenceLength\", \"meanWordSyllables\"),\n                                  min_sentence_length = 3,\n                                  max_sentence_length = 1000) \n# on joint les données\nfoo&lt;-cbind(foo,readability[,2:4])\n\n#on agrège par année\nfoo1&lt;-foo %&gt;% filter(Year!=2021) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(all_tweet=n(),\n            Flesch=mean(Flesch, na.rm=TRUE), \n            SentenceLength= mean(meanSentenceLength, na.rm=TRUE),\n            WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %&gt;%\n  gather(variable, value, -Year)\n\n#visualisation\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la lisibilité des tweets de Trump\", x=NULL, y=NULL)\n\n\n\n\n\n\n\n\n\nPour aider le lecteur à donner un sens, voici l’abaque proposée par Flesch lui-même.\n\n\n\nFlesch\n\n\nOn peut aussi prendre pour références les éléments suivants:\n“All Plain English examples in this book score at least 60. Here are the scores of some reading materials I’ve tested. These are average scores of random samples.” ( source ?)\n\nComics 92\nConsumer ads in magazines 82\nReader’s Digest 65\nTime 52\nWall Street Journal 43\nHarvard Business Review 43\nHarvard Law Review 32\nAuto insurance policy 10\n\nTrump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Le Reader’s Digest est beaucoup plus simple, il se situe au dessus de la Harvard Business Review !\n\n\n7.4.2 Les indices de complexité lexicale\nLa complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, l’indicateur marque plus cette variété que les variations de complexité lexicale.(Tweedie and Baayen 1998)\nDans notre univers trumpesque, ce n’est pas trop sensible, d’autant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweet, une autre approche pourrait être de concaténer l’ensemble des tweets d’une période (un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvre l’ensemble des sujets d’intérêt de Trump, que les tweets fractionnent nécessairement. Ce qui en en question dans la mise en pratique n’est pas seulement la question du choix de l’indice mais aussi la définition de l’unité de calcul. La diversité lexicale concerne sans doute plus le discours que la phrase.\nOn choisit de ne travailler sur deux des multiples indicateurs disponibles :\n\nle CTTR de Caroll qui rapporte le nombre de mots distincts ( V) sur le nombre de mots exprimés. Avec ce critère la diversité maximale est obtenue quand le nombre de mots différents est égal au nombre de mots exprimés.\n\n\\[\nCTTR = \\frac{V}{\\sqrt{2N}}\n\\]\n\nle Mass supposé être moins sensible à la longueur des textes. (voir Torruella et Capsada 2013 ou )\n\n\\[\nM = \\frac{log(n) - log(t)}{log² (n)}\n\\]\nOn notera que le problème de la longueur variables des textes dans un corpus produit un biais : les textes seront mécaniquement plus divers que les textes court, ce qui conduit à des approches segmentées, où la mesure de diversité est une moyenne des moyenne pour chacun des segments de texte . On emploie ici le MATTR, dont le MA signifie moyenne mobile (moving average), et le TTR le token/type ratio.\n(attention un pb de log dans le calcul)\n\n\nShow the code\n#on retient les tweets de plus de 5 mots\nfoo&lt;-df %&gt;%\n  filter(nb_mots&gt;10 & isRetweet==\"FALSE\" & Year!=2021)\n\n#la fonction de calcul de diversité\nt1=Sys.time()\nlexdiv&lt;-tokens(foo$text)%&gt;%\n  textstat_lexdiv(foo$text, measure = c(\"CTTR\", \"Maas\"),  log.base = 10,\n                  remove_numbers = TRUE,  \n                  remove_punct = TRUE,  \n                  remove_symbols = TRUE,\n                  remove_hyphens = TRUE) \nt2=Sys.time()\nt&lt;- t2-t1\nt\n\n\nTime difference of 4.507283 secs\n\n\nShow the code\n#On combine les données et on aggrège sur l'année\nfoo&lt;-cbind(foo,lexdiv[,2:3])\nfoo1&lt;-foo %&gt;% \n  group_by(Year) %&gt;%\n  summarise(CTTR=mean(CTTR, na.rm=TRUE), \n            Maas=mean(Maas, na.rm=TRUE)) %&gt;%\n  gather(variable, value, -Year)\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la diversité lexicale des tweets de Trump\", x=NULL, y=NULL)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#la-mesure-de-la-concentration-des-termes",
    "href": "q06_Analyse_quantitative.html#la-mesure-de-la-concentration-des-termes",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.5 La mesure de la concentration des termes",
    "text": "7.5 La mesure de la concentration des termes\nLe langage d’un point de vue quantitatif a été caractérisé depuis bien longtemps et il est caractérisé par la loi de Zipf distribution 1949 . Estout est un des premiers à avoir proposé la loi, mais un sténographe fameux en aurait mis en évidence empiriquement avant 1916.\n\n7.5.1 Un débat théorique\nCelle-ci s’exprime de la manière suivante : si on classe les termes par ordre de fréquence r, le produit de leur rang par la fréquence est égale à une constante. Autrement dit dans une métrique log, la relation entre rang et fréquence est linéaire et sa pente est négative.\n\\[f(r).{r}={K} \\] Mandelbrot la généralise\nles études empiriques\nOn peut encore débattre de sa signification. Une économie cognitive? Une conséquence de la théorie de l’information ? Un signature pour identifier un discours ## Les lois de distribution du langage\nLes lois puissances qui déterminent cet univers et marque dans le registre de l’analyse quantitative une évolution sérieuse. Alors que le modèle dominant de la statistique est la distribution gaussienne, le traitement du langage doit se familiariser avec des distributions puissance.\nLa régularité statistique nécessite une interprétation, il n’y en a pas qu’une\n\nla loi du moindre de effort qui va aboutir à un débat entre Mandelbrot et Simon.\nEconomie de l’information, un mot puis deux, puis trois, puis des mots spécifique à un certain registre d’action. Le sens est une affaire d’échelle, du concret au général. Le plus concret est celui des mots scientifiques telles qu’on les retrouve en biologie pour décrire une espèce, en droit pour décrire une propriété, ou en médecine pour désigner une pathologie ou un élément d’anatomie. ( espace de connaissance - épistémologique)\nla distribution des locuteurs et de leurs élocution. Dans le langage vernaculaire celui de nos sociabilités, peu de mots sont employés, à mesure qu’on les enregistre ils prennent une place plus importance d’un point de de vue statistique dans les corpus. Les mots rares sont employés par peu de personne. (espace des population - démographique)\n\n\n\n7.5.2 Application\nExaminons plus concrètement\nOn reprend la procédure de comptage des mots par groupe, mais sans filtrer sur la fréquence de ces mots. On en obtient n_w mots distincts. En examinant avec plus de détail il y a 5 élément d’information : a ) le trait (feature), c’est à dire ici le mot étudié, b) le nombre de fois où il apparaît dans le corpus, c)le rank qui lui correspond, d)le nombre de documents dans lesquels il apparaît.\nLe tableau nous donne les 10 premiers. Il y a 28000 éléments, dont une simple inspection montrera qu’il sont des liens ou d’autres mentions. Il est nécessaire de retraiter le corpus pour éliminer ces éléments. Nous étudierons comment faire dans les chapitres suivants. A ce stade on se contente d’enlever les apax1 dont on ne peut véritablement calculer un rang ( ils ont tous le dernier). Il nous reste 10854 termes.\n\n\nShow the code\nfoo&lt;-df %&gt;% \n  filter(isRetweet==FALSE) %&gt;%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks&lt;- tokens(foo$text) %&gt;% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\nfreq_g &lt;- textstat_frequency(toks)%&gt;%\n  as.data.frame() %&gt;%\n  filter(feature!=\"amp\") %&gt;%\n  filter(frequency&gt;1)\n\nflextable(head(freq_g, 15))\n\n\nMots les plus fréquentsfeaturefrequencyrankdocfreqgroupgreat4,07713,755allthank2,04132,029allpeople2,04041,895alljust1,63351,572allnow1,54461,505allpresident1,46671,322alltrump1,42481,307allbig1,36091,263allnews1,347101,254allcountry1,274111,233allget1,180121,083allfake1,177131,100allnew1,167141,089alldemocrats1,163151,119allmany1,108161,061all\n\n\nOn peut aussi représenter cela graphiquement. La loi de zipf s’observe jusqu’au 500 premiers mots, ensuite elle suit une autre pente. C’est assez caractéristique, la loi de zipf est incomplète, d’un point de vue empirique il semble qu’il y ait deux lois qui se superposes, la loi des mots communs, et celle des mots singuliers. C’est du moins hypothèse que nous proposons.\n\n\nShow the code\nggplot(freq_g, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"gam\", color=col_1)\n\n\n\n\n\n\n\n\n\nShow the code\nfoo&lt;-freq_g %&gt;%filter(rank &lt;500)\nggplot(foo, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"lm\", color=col_1)\n\n\n\n\n\n\n\n\n\nPour être plus qualitatif Une première manière de représenter la diversité du corpus est de représenter les mots les plus fréquents, selon cette fréquente et le ratio fréquence par document. Plus ce dernier est élevé plus il fait du mot un mot spécifique.\n\n\nShow the code\nfoo&lt;- freq_g %&gt;%\n  filter(frequency&gt;150 &frequency&lt;1500 )\n\nggplot(foo, aes(x=docfreq, y= frequency/docfreq))+\n  geom_text_repel(aes(label=feature), size=2, max.overlaps = 30)+\n  scale_x_log10()+\n  scale_y_log10()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#comptons-les-mots-1",
    "href": "q06_Analyse_quantitative.html#comptons-les-mots-1",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.6 Comptons les mots",
    "text": "7.6 Comptons les mots\nIl est temps de compter les mots, chacun d’entre eux, de se faire une idée une idée de leurs fréquences, de leur distribution.\nSouvent on éliminera ceux qui apparaissent de manière occasionnelle, mais aussi ceux qui apparaissent systématiquement dans tous les textes. Une fois ces deux filtrages effectués, le lexique est généralement de l’ordre de 500 à 10000 mots.\nDeux outils sont disponibles: les nuages de mots et les lollyplots. Les premiers donnent une idée immédiates, les seconds se prêtent mieux à une analyse systématique\n\n7.6.1 Les nuages de mots\nIls sont devenus extrêmement populaires même si l’effet esthétique est plus important que leur utilité analytique.\nggwordcloud\nPour l’application on prépare les données avec quanteda : on tokenise et on construit le dfm (pour le détail voir chapitre tokenization), ce qui nous permets notamment d’éliminer la ponctuation et les mots courants (articles, déterminant etc) qui apportent peu de signification.\n\n\nShow the code\nfoo&lt;-df %&gt;% filter(isRetweet==FALSE) %&gt;%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks&lt;- tokens(foo$text) %&gt;% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\n\ndocvars(toks,\"Year\")&lt;-foo$Year\n\n#on se concentre du les termes utilisés 300 fois.  \n\nfoo&lt;-toks %&gt;% \n    dfm_trim(min_termfreq = 250, verbose = FALSE)\n\nfreq &lt;- textstat_frequency(foo)\n\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal() +  scale_size_area(max_size = 10) + \n  scale_color_gradient(low = col_1, high = col_1b)\n\n\n\n\n\n\n\n\n\nShow the code\nggsave(\"image/g0.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")\n\n\nEt pour faire des comparaisons, entre l’année 2016 qui le conduit à être élu, 2018 une année de midterm et 2020 année de sa défaite, on utilise la même procédure mais on distingue un comptage de fréquence de mot par période. Le ggplot est identique aux précédents mais comprend en plus une géométrie “facet_wrap” qui éclate le nuages de mot selon les 3 périodes étudiées.\n\n\nShow the code\nfoo&lt;-toks %&gt;% \n  dfm_group(groups = Year) %&gt;%\n    dfm_trim(min_termfreq = 1, verbose = FALSE)\n\nurl_regex &lt;- \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n  \n#pour compter la fréquence des mots par année\nfreq &lt;- textstat_frequency(foo, group =Year) %&gt;%\n  mutate(feature=str_remove_all(feature, url_regex))|&gt;\n  filter(!is.na(feature) & frequency&gt;200 )%&gt;%\n  filter(feature!=\"amp\")\n           \n\nset.seed(42)\nlibrary(ggwordcloud)\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal()+\n  facet_wrap(vars(group)) +  \n  scale_size_area(max_size = 8) \n\n\n\n\n\n\n\n\n\nShow the code\nggsave(\"./image/g1.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#conclusion",
    "href": "q06_Analyse_quantitative.html#conclusion",
    "title": "7  Analyse Quantitative du corpus",
    "section": "7.7 Conclusion",
    "text": "7.7 Conclusion\nNous aurons appris à\n\nCompter le nombre de documents et leurs longueurs\nMesurer la complexité du langage\nMesurer la diversité de son vocabulaire.\nA évaluer la concentration des sources\nA se donner une première idée de la lexicographie\n\nCes mesures n’ont ne sens que si elles peuvent être l’objet de comparaison :\n\nDe manière interne la comparaison se fait dans dans le temps et à travers des segments. On s’intéresse moins au niveau, qu’aux différences entre les niveaux.\nDe manière externe elle requiert un étalonnage. Comparer par rapport au français courant, à un niveau de langue soutenu, ou relâché. L’étalonnage revient à caractériser des types de corpus : presses, écriture savante, réseaux sociaux, publications officielles etc. On ne peut que souhaiter que des comparaisons systématiques soient engagées et compilées pour donner des points de repère précis quand on étudie un corpus particulier.\n\nElles participent à un premier niveau d’analyse du texte, en surface, visant à apprécier la dynamique de sa production, à établir les échelles d’analyse, à repérer les éléments structurels.\nLe texte est une matière qui a un poids (le nombre de mot), une variété (le nombre d’expressions), une complexité (les règles qui l’organisent). nous venons de nous doter des premiers outils d’analyse, il est temps de passer à la suite.\n\n\n\n\nColeman, Meri, and T. L. Liau. 1975. “A Computer Readability Formula Designed for Machine Scoring.” Journal of Applied Psychology 60 (2): 283–84. https://doi.org/10.1037/h0076540.\n\n\nFlesch, Rudolph. 1948. “A New Readability Yardstick.” Journal of Applied Psychology 32 (3): 221–33. https://doi.org/10.1037/h0057532.\n\n\nTweedie, Fiona J., and R. Harald Baayen. 1998. “How Variable May a Constant Be? Measures of Lexical Richness in Perspective.” Computers and the Humanities 32: 323–52.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#footnotes",
    "href": "q06_Analyse_quantitative.html#footnotes",
    "title": "7  Analyse Quantitative du corpus",
    "section": "",
    "text": "les apax sont les termes qui n’apparaissent qu’une seule fois dans un corpus.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "",
    "text": "1 Préambule\n“Au commencement était le verbe”, et désormais le verbe est partout, non pas l’esprit de Dieu, mais la parole humaine qui s’éteignait avec le vent, la rumeur mais qui désormais, accumule ses traces imprimées de manière systématique, non plus des petits mots passés de main en main, mais l’enregistrement des transactions informatiques.\nLe verbe est désormais une copie du monde, moins l’affirmation d’une vision, que la stratification de nos manifestations sociales.\nC’est bien sur le fruit d’un développement technique engagés depuis 10 000 ans et dont l’imprimerie est une nouvelle étape. La révolution actuelle réside dans la colonisation de l’espace social, chaque bribes de parole est enregistrée , transcrite, archivée. Le livre n’est plus qu’un îlot dans un océan de texte.\nIl y a un défi empirique pour les sciences sociales, la société produit massivement la documentation de son développement. L’exploitation de ces sources est un enjeu majeur pour la psychologique, sociologique, économique, le droit, les sciences de gestion en ouvrant un nouvel accès au données.\nA mesure que ces données prolifèrent, des méthodes pour les analyser se sont développées à grande vitesse, moins pour des motivations d’études que pour répondre à des besoins opérationnels. Aujourd’hui, dans le sillage de l’invention des embeddings, nous sommes à l’ère des grands modèles de langages qui ouvrent de nouveaux horizons dans la capacité de produire du texte (par exemple des descriptions d’images), de transformer le texte (résumé ou traduction), et surtout de l’annoter ( classification, NER…)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "index.html#le-but-de-louvrage",
    "href": "index.html#le-but-de-louvrage",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "1.1 Le but de l’ouvrage",
    "text": "1.1 Le but de l’ouvrage\nCe e-book est le syllabus d’un cours que nous dispensons sous différents formats et avec différents degrés d’approfondissement. Il fait l’inventaire des méthodes les plus courantes incluant le développement des embeddings et aboutit à l’usage des LLM pour différentes formes d’annotations.\nIl a une vocation pratique , des cas, des codes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "index.html#les-contributeurs",
    "href": "index.html#les-contributeurs",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "1.2 Les contributeurs",
    "text": "1.2 Les contributeurs\nOn ne peut (encore) en produire la liste exhaustives, mais plusieurs générations d’étudiants ont contribué à ce travail en explorant un certain nombre de jeux de données proposés ici.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "index.html#le-plan-du-cours",
    "href": "index.html#le-plan-du-cours",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "1.3 Le plan du cours",
    "text": "1.3 Le plan du cours\nIl s’organise selon les grandes étapes du processus d’analyse et des types de problèmes posés par le traitement et l’analyse du texte. Ce processus va de l’acquisition des données à leurs représentations en passant par des phases de transformation, mais aussi un ordre de complexité des modèles et des ressources.\nIl suit ainsi une sorte de développement historique qui se construit par l’accumulation de différentes philosophies d’analyses et de méthodes et qui peut se présenter en trois grandes périodes:\n\ncompter les mots et jouer avec leur co-occurences\nannoter des mots en s’appuyant sur leurs régularités syntaxiques.\nencoder les mots dans un espace vectoriel, cette perspective a été ouverte par word2vec , étendue avec les transformers, systématisée par les grands modèles de langages tels que gpt4 ou Bloom.\n\nVoici les raisons qui organisent le cours en 20 chapitres (c’est à ajuster au cours de la rédaction)\n\nPréambule (ce que vous êtes en train de lire !)\nChapitre 1 : une introduction générale à des éléments linguistique et technique et sociaux de l’analyse du langage naturel et de ses fulgurantes évolutions au cours de la dernière décennie.\nChapitre 2 : Constituer des corpus. Pour les sciences sociales, le texte est un document qu’on étudie par collection. Il faut aussi penser à la notion de corpus.\nChapitre 4 : corpus - techniques avancées D’un point de vue matériel le corpus est aussi\nChapitre 5 : Explorer et naviguer dans le corpus\nChapitre 6 : Analyse quantitative du corpus\nChapitre 7 : Tokenisation et dtm\nChapitre 8 : Analyse des co-occurences\nChapitre 9 : au début il y avait l’analyse factorielle\nChapitre 10 : SVD et LSA :\nChapitre 11 : Topic model\nChapitre 12 : Supervised modeling\nChapitre 13 : word2vec to doc2vec\nChapitre 14 : Transformers\nChapitre 15 : tasks , Zsl, abstracting\nchapitre 16 : generative models , un monde à inventer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "index.html#data-sets",
    "href": "index.html#data-sets",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "1.4 Data sets",
    "text": "1.4 Data sets\nOn s’appuiera sur des data test “réel” et publics\n\nTrump Tweeter Archive : c’est un site qui rassemble tous les tweets émis par Donald Trump depuis 2010, et propose un outil de navigation.\nTripadvisor en Polynésie\nAirbnb Paris 2019\nPMP40ans : ce set de données comprend tous les résumés d’articles publiés par la revue Politique et Management Public sur une période de 40 années de sa naissance en 1983 jusqu’en 2023. C’est un observatoire intéressant d’une discipline naissance et de l’évolution des normes professionnelles de la recherche en gestion.\nyyy\nzzz",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Introduction aux méthodes de traitement du langage naturel",
    "section": "1.5 Packages",
    "text": "1.5 Packages\nCe cours avec les ressources de r, de rstudio et de Quarto. Les apports externes le sont sous forme d’images. Zotero pour la bibliographie.\nIl n’y a jamais une solution unique, nous avons fait des choix de méthodes qui se reflètent dans celui des packages dont voici la liste commentée.\n\n\nShow the code\n#environnement de travail\nlibrary(tidyverse) # the perfect tool box.\nlibrary(glossary)\nglossary_path(\"glossary.yml\")\n\n# un exemple pour ajouter des termes au cours de la rédaction\n#glossary_add(term = \"loi de Dirichlet\",\n#             def = \"la loi de Dirichlet, souvent notée Dir(α), est une famille de lois de probabilité continues pour des variables #aléatoires multinomiales.Ex. probabilités d'un mot parmi les trois milles d'usage courant\")\n\n#ses compléments  \nlibrary(ggwordcloud)\n\nlibrary(flextable) # pour produire des tableaux élégants\n\n#ressource nlp  \nlibrary(quanteda) \nlibrary(udpipe)\n\n#analyse de données\nlibrary(FactoMineR)\nlibrary(factoextra)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Préambule</span>"
    ]
  },
  {
    "objectID": "q02_corpus.html",
    "href": "q02_corpus.html",
    "title": "3  La notion de corpus",
    "section": "",
    "text": "3.1 Les éléments du corpus\nCe qui fait un corpus est composé de trois éléments : une collection, des textes qui se manifestent comme une suite de caractère répondant à des règles plus ou moins connue, et à des informations associées, les méta-donnée. Examinons chacun de ces éléments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La notion de corpus</span>"
    ]
  },
  {
    "objectID": "q02_corpus.html#les-éléments-du-corpus",
    "href": "q02_corpus.html#les-éléments-du-corpus",
    "title": "3  La notion de corpus",
    "section": "",
    "text": "3.1.1 Le document\nL’ensemble des documents constitue la collection , elle peut être systématique et exhaustives, par exemple s’il s’agit de toute la correspondance d’un écrivain. Elle peut aussi se constituer comme un échantillon et répondre au règles de la théorie de l’échantillonnage.\n\n3.1.1.1 Le document comme chaine de caractère\nUne unité de texte se définit comme une chaîne de caractères qui constitue un document. Celui ci peut être de forme quelconque : un livre, un article, une note, une transcription, il reste une séquence plus ou moins longue de caractères qui répondent à des règles de composition dont on ignore la nature a priori. Dans le cas de texte courts, cette chaîne de caractères est simple.\nDans celui d’un article, elle peut être plus complexe et comporter des combinaison de caractères qui signalent un effet de composition. par exemple la chaîne \\n définit un saut de page, et si le document est codé en xml, certaines séquences identifient des balises. Par exemple la chaîne &lt;h1&gt; indique que les caractères qui vont suivre définissent le contenu d’un titre de niveau 1 et que cette définition s’achève avec la balise &lt;|&gt;. L’exemple adresse une convention qui est celle des langage xml qui séparent le contenu du contenant, c’est à dire des actions qui seront opérées sur le contenu.\nL’analyse de ces structure est un préalable indispensable pour les lire correctement. Et il sera souvent néçéssaire de nettoyer le texte avant son exploitation.\n\n\n3.1.1.2 la structuration\nDifférents types de corpus Le degré de structuration : séquentielle, plan spatiale ( ex = le curriculum viate, la fiche de brevet,…)\n\n\n3.1.1.3 Metadonnées\nUn document est souvent associé à des méta-données qui permettent de le situer ce qui permet des comparaisons dans le temps l’espace ou les locuteurs. Un bon corpus est celui qui associe aux documents, l’ensemble le plus pertinent et le plus large de méta-données.\n\nUn ou des auteurs. A chaque document peut être attaché un ou plusieurs auteurs. Et c’est auteurs eux-même peuvent être caractérisés.\nChaque document peut être associé à une date de production ou de publication.\nChaque document peut être associé à une origine géographique, générique comme le pays d’origine ou géolocalisé dans le cas d’un message émis dans un réseau social.\nIl peut être documenté par des éléments de contexte : les unités précédentes, et subséquentes - c’est le cas des chats et des forums qui peuvent être identifié par les fils dans lesquels ils s’inscrivent et la position qu’ils y occupent.\nune ou des autorités : par exemple le média où il a été publié ce qui pose la question de la source et celle de sa crédibilité\n\nLa liste n’est pas limitative et peut comprendre des éléments de diplomatique, cette discipline, ancienne, qui vise à établir une compréhension critique des documents écrits, pour en particulier établir leur authenticité .voir\n\n\n\n3.1.2 Une collection\nIl y a une très forte diversité de corpus. Celle-ci est cependant peut se décrire au travers d’un certains nombre de critères.\n\n3.1.2.1 L’échelle et l’étendue\nIl peut y avoir de très petits corpus, par exemples la transcription de quelques dizaines d’entretiens. d’autres qui rassemblent des millions de textes courts. L’échelles des corpus va de quelques unités à plusieurs millions. Les milliards sont rares. Concrètement il y a des corpus de quelques dizaines ou centaines d’éléments, d’autres qui se comptent en dizaines de milliers, d’autres en centaines et millions d’unités. Méthodes et capacité de calculs ne sont pas forcément les mêmes.\nCourts à l’image des tweets, ou de résumés d’articles, ou longs s’il s’agit d’articles de recherche complet, ou très long, le cas des livres. les textes court sont ce que les méthodes avancée traitent bien . Les textes longs pose la question de co-références. On peut cependant les décomposer, le niveau de phrase, celui du paragraphe, le chapitre, le livre.\nA ce stade, 4 types de corpus\n\n\n\ntextes\nPeu\nNombreux\n\n\n\n\ncourt\nCas\nSocial data\n\n\nLong\nComparative\nDeep data\n\n\n\n\n\n3.1.2.2 les auteurs\nLes corpus peuvent aussi se distinguer par la diversité de leurs auteurs\n\nla monographie quand le corpus compile tous les documents d’une même source\nla polygraphie quand les auteurs sont presque aussi nombreux que le nombre de textes. Quand les auteurs sont multiples, il peuvent constituer le texte de deux manières\npar addition : c’est l’exemple du message publicitaire qui superpose une scène, des incrustations, de la musique.\npar interaction : c’est le mode tu texte de théâtre ou la structure du texte distribuent entre les personnage des fragments de paroles qui interagissent.\n\nencore une autre typologie\n\n\n\nAuteurs\nAddition\nInteraction\n\n\n\n\nUnique\nmonographie\n\n\n\nDouble\ncontrepoint\nDialogue\n\n\nMultiple\nPolygraphie\nThéatre\n\n\n\n\n\n3.1.2.3 les niveaux de langues\nIls varient aussi selon le niveaux de langues\n\ntexte savant, texte standard, texte vernaculaire\n\n\n\n\n\n3.1.3 des corpus bien préparés\nDans ce chapitre nous avons appris comment faire la cueillette dans les sources de textes et constituer matériellement un corpus. Il reste à traiter la question de la représentativité. La collecte doit rester raisonnée.\nUn corpus se construit. D’abord en fonction d’un objectif de recherche? Cherche-t-on à décrire, à comparer, à explique ?\nEnsuite en fonction d’une perspective d’écoute. par exemple prendre le corpus de ceux qui produisent de la publicité, ou le corpus de ce à quoi les consommateurs sont exposés ?\n\n3.1.3.1 Production et réception\nUnités de production et de réception, Un texte est produit et puis, peut-être, lu. Analyser le texte peut se faire dans deux perspectives, celle de la production et celle de la réception. Les corpus doivent être construits en fonction de ce critère.\n\n\n3.1.3.2 Les conditions de production des documents\nExaminer la question de l’engagement dans ce cadre est essentiel, certains acteurs sur un sujet donné sont amenés à produire plus que les autres, et participent donc de surcroît à une sur représentation statistique. La question du biais de sélection\nPrenons le cas des sites d’avis de consommateurs.\nla question des textes absents",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La notion de corpus</span>"
    ]
  },
  {
    "objectID": "q02_corpus.html#le-corpus-comme-objet-de-traitement",
    "href": "q02_corpus.html#le-corpus-comme-objet-de-traitement",
    "title": "3  La notion de corpus",
    "section": "3.2 Le corpus comme objet de traitement",
    "text": "3.2 Le corpus comme objet de traitement\nSur le plan pratique nos discussions dépendent largement des modalités opérationnelles que les langages de traitement de données nous proposent.\nDans l’univers r le package tm est fondateur, mais d’autres solutions sont offertes. On signalera d’abord stringr qui fait partie de la suite tidyverse.\nParmi elles on fait le choix des méthode de la suite Quanteda.\n\n3.2.1 Un exemple\nIl s’agit du corpus “TrumpArchive”.\n\n\nShow the code\ndf &lt;- read_csv(\"data/TrumpTwitterArchive01-08-2021.csv\")%&gt;%\n  select(id, text, favorites, retweets, date)\nhead(df)\n\n\n# A tibble: 6 × 5\n       id text                            favorites retweets date               \n    &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;    &lt;dbl&gt; &lt;dttm&gt;             \n1 9.85e16 Republicans and Democrats have…        49      255 2011-08-02 18:07:48\n2 1.23e18 I was thrilled to be back in t…     73748    17404 2020-03-03 01:34:50\n3 1.22e18 RT @CBS_Herridge: READ: Letter…         0     7396 2020-01-17 03:22:47\n4 1.30e18 The Unsolicited Mail In Ballot…     80527    23502 2020-09-12 20:10:58\n5 1.22e18 RT @MZHemingway: Very friendly…         0     9081 2020-01-17 13:13:59\n6 1.22e18 RT @WhiteHouse: President @rea…         0    25048 2020-01-17 00:11:56\n\n\nC’est le champs texte qui définit notre corpus, les autres variables / colonnes du tableau représentent les méta données.\n\n\nShow the code\ncorp &lt;- corpus(df$text, docvars = df)\nsummary(corp[1:5,])\n\n\nCorpus consisting of 5 documents, showing 5 documents:\n\n  Text Types Tokens Sentences           id\n text1    10     10         1 9.845497e+16\n text2    42     50         3 1.234653e+18\n text3    23     24         1 1.218011e+18\n text4    49     61         3 1.304875e+18\n text5    25     25         2 1.218160e+18\n                                                                                                                                                                                                                                                                                               text\n                                                                                                                                                                                                                                 Republicans and Democrats have both created our economic problems.\n            I was thrilled to be back in the Great city of Charlotte, North Carolina with thousands of hardworking American Patriots who love our Country, cherish our values, respect our laws, and always put AMERICA FIRST! Thank you for a wonderful evening!! #KAG2020 https://t.co/dNJZfRsl9y\n                                                                                                                                                       RT @CBS_Herridge: READ: Letter to surveillance court obtained by CBS News questions where there will be further disciplinary action and cho…\n The Unsolicited Mail In Ballot Scam is a major threat to our Democracy, &amp; the Democrats know it. Almost all recent elections using this system, even though much smaller &amp;  with far fewer Ballots to count, have ended up being a disaster. Large numbers of missing Ballots &amp; Fraud!\n                                                                                                                                                       RT @MZHemingway: Very friendly telling of events here about Comey's apparent leaking to compliant media. If you read those articles and tho…\n favorites retweets                date\n        49      255 2011-08-02 18:07:48\n     73748    17404 2020-03-03 01:34:50\n         0     7396 2020-01-17 03:22:47\n     80527    23502 2020-09-12 20:10:58\n         0     9081 2020-01-17 13:13:59\n\n\nUne fois le corpus constitué , on peut en sélectionner des sous-ensemble. Dans l’exemple suivant on isole les tweets qui ont été retwittés plus de 20 000 fois.\nIl va être temps de jouer avec les données !\n\n\nShow the code\n# on peut choisir un sous-corpus \ncorp_recent &lt;- corpus_subset(corp, retweets &gt;20000)\nndoc(corp_recent)\n\n\n[1] 7277\n\n\nShow the code\nhead(corp_recent, 10)\n\n\nCorpus consisting of 10 documents and 5 docvars.\ntext4 :\n\"The Unsolicited Mail In Ballot Scam is a major threat to our...\"\n\ntext6 :\n\"RT @WhiteHouse: President @realDonaldTrump announced histori...\"\n\ntext7 :\n\"Getting a little exercise this morning! https://t.co/fyAAcbh...\"\n\ntext9 :\n\"https://t.co/VlEu8yyovv\"\n\ntext11 :\n\"https://t.co/TQCQiDrVOB\"\n\ntext16 :\n\"As per your request, Joe... https://t.co/78mzcfLEsF https://...\"\n\n[ reached max_ndoc ... 4 more documents ]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La notion de corpus</span>"
    ]
  },
  {
    "objectID": "q02_corpus.html#conclusion",
    "href": "q02_corpus.html#conclusion",
    "title": "3  La notion de corpus",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nTechniquement c’est aussi simple que cela. D’un point de vue méthodologique la question de la constitution d’un corpus reste complexe, elle demande de l’intelligence dans l’exploitation des sources, une maîtrise technique des interfaces (les APIs).\nDans le chapitre suivant, nous explorons des aspects plus techniques : comment saisir la forme matérielles des documents, comment interroger les bases de données, et exploiter des Apis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La notion de corpus</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html",
    "href": "q08_Analyse_syntaxic.html",
    "title": "10  Annotations syntaxiques",
    "section": "",
    "text": "10.1 Le processus d’annotation\nPour aller au-delà de l’analyse du seul lexique et de l’analyse de la cooccurence des termes à travers les textes, comme le font les méthodes de typologie et d’analyse factorielle des correspondance depuis longtemps, il est nécessaire d’analyser le texte en tenant compte de ses propriétés syntaxiques. Depuis une dizaine d’années, des outils puissants, les annotateurs, sont proposés de manière accessible.\nLes plus connus sont Spacy, Stanford NLP ou UDpipe avec r .Dans l’environnement r différentes ressources sont disponibles : Quanteda, clean_nlp, Udpipe, …\nIls sont disponibles désormais dans de nombreuses langues même si la richesse et la précision obtenues varient d’une langue à l’autre\nIls s’appuient sur des corpus plus ou moins étendus et spécialisés d’annotations manuelle : les Treebanks.\nIls réalisent souvent plusieurs tâches dont les principales sont les suivantes :\nUn petit exemple avec le package UDpipe.\nShow the code\nlibrary(udpipe)\nfr &lt;- udpipe_download_model(language = \"french\")\nudmodel_french &lt;- udpipe_load_model(file = \"french-gsd-ud-2.5-191206.udpipe\")\nCitations &lt;- read_csv(\"./data/Citations.csv\")\n\nFlaubert&lt;-Citations %&gt;%\n  filter(doc==1)\n\nUD &lt;- udpipe_annotate(udmodel_french, x=Flaubert$text)\nx &lt;- as.data.frame(UD)\nfoo&lt;-x %&gt;% \n  select(doc_id,paragraph_id, sentence_id, token_id,token,lemma,head_token_id, upos,feats)%&gt;%filter(sentence_id==1)\nflextable(foo)\n\n\ndoc_idparagraph_idsentence_idtoken_idtokenlemmahead_token_iduposfeatsdoc1111Lele2DETDefinite=Def|Gender=Masc|Number=Sing|PronType=Artdoc1112lendemainlendemain9NOUNGender=Masc|Number=Singdoc1113futêtre9AUXMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Findoc1114,,6PUNCTdoc1115pourpour6ADPdoc1116EmmaEmma9PROPNdoc1117,,6PUNCTdoc1118uneun9DETDefinite=Ind|Gender=Fem|Number=Sing|PronType=Artdoc1119journéejournée0NOUNGender=Fem|Number=Singdoc11110funèbrefunèbre9ADJGender=Fem|Number=Singdoc11111..9PUNCT",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#le-processus-dannotation",
    "href": "q08_Analyse_syntaxic.html#le-processus-dannotation",
    "title": "10  Annotations syntaxiques",
    "section": "",
    "text": "Tokeniser : découper la chaîne de caractère en unité d’analyse( lettre syllabe, mot …)\nLemmatiser : associer à la forme fléchie, le mot racine, celui des dictionnaires courants.\nIdentifier les parts of speech\nIdentifier les dépendances syntaxiques\nIdentifier les entités nommées.\nIdentifier les co-reférences.\n\n\n\n\n10.1.1 lemmes, stem et synonymes\nSelon leurs catégories les mots peuvent soutenir des flexions. Le pluriel ou le singulier, le féminin ou le masculin, les desinances, les conjugaisons. Il conviendra\nc’est le fait de ne conserver que le radical des mots, pour regrouper sous le même radical toutes les variétés morphologique d’un même mot. Il suffit dont d’enlever les syllabes qui correspondent aux suffixes et aux flexions du mot (mode singulier ou pluriel, genre, desinences : conjugaison et déclinaison etc..). On parle aussi de racinisation.\nUn lemme est un mot racine (ne pas confondre avec le radical), sans inflexions de genre, de mode, de conjugaison ou de déclinaison. C’est généralement celui qu’on trouve dans les dictionnaires. Il s’agit de ramener un terme, à sa forme la plus simple qui en français est l’infinitif/masculin-singulier).Du point de vue des graphèmes des mots sont des variétés. Dans la série ” j’ai aimé” , ” J’aime”, “j’aimerais” , il y a un verbe dont on observe différente variation, la racine de ces variation est le lemme “aimer”. c’est une convention grammaticale, celle de prendre pour racine la forme infinitive.\nLa lemmatisation est le processus qui vise à réduire les morphologie à leur racine. Non pas une forme primitive du mot mais une forme catégorique.le cas de Wordnet et l’invention des synsets synonymes, antonymes, hyponyne, hyperonymes….. https://cran.r-project.org/web/packages/wordnet/vignettes/wordnet.pdf",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#part-of-speech-pos",
    "href": "q08_Analyse_syntaxic.html#part-of-speech-pos",
    "title": "10  Annotations syntaxiques",
    "section": "10.2 Part of speech (POS)",
    "text": "10.2 Part of speech (POS)\nle role du stanford project\nDans une phrase les mots n’ont pas la même valeur. Certains sont des nombres propres, ils se réfèrent à ce que nous venons de voir, c’est à dire des entitées nommées, d’autres désignent des catégories d’objet. Ce sont les noms communs qui se rapportent à des catégories de choses. Un marteau - si j’en avais un - peut être n’importe quel marteau, la masse qui casse la pierre, ou ce petit marteau qui me permet d’enfoncer un clou dans le cadre du tableau.\nDes typologies universelles ont été construites, elles recouvrent des typologies plus spécifiques à certaines langues. Les désinences du latin ont par exemple disparu du français. Cette forme est spécifiques au latin, on la retrouvera en allemand. La notion de morphosyntaxique désigne précisément que les variations de formes des mots dépendent d’une règle syntaxique. Prenons le verbe, et sa forme, “être”, dont la forme au passé simple est “était”. La forme des mots change, mais l’idée reste.\nUne catégorisation en 17 éléments est proposée. En voici les éléments et les définitions\nUn petit exemple avec le package UDpipe.\n\n10.2.1 application\n\n\n10.2.2 la detection d’auteurs et de styles",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#dépendances-syntaxiques",
    "href": "q08_Analyse_syntaxic.html#dépendances-syntaxiques",
    "title": "10  Annotations syntaxiques",
    "section": "10.3 Dépendances syntaxiques",
    "text": "10.3 Dépendances syntaxiques\nC’est à Lucien Tesnière que l’on doit l’idée de la grammaire de la dépendance qui est au cœur du NLP moderne. L’idée est de déterminer au niveau de la phrase les relations entre ses termes de manière hiérarchisée selon un principe de gouvernant à subordonné.\nVerdelhan-Bourgade (2020) résume son analyse de manière précise et concise :\n\n“Tous les mots n’ont pas le même statut. Les mots pleins, qui « expriment directement la pensée » (p. 59), relèvent de quatre catégories structurales : les substantifs (notés par O), les adjectifs (A), les verbes (I), les adverbes (E). Les mots dits vides (souvent désigné de manière pratique par les stopwords aujourd’hui) précisent le sens des autres, ou servent à marquer des relations.La connexion établit la relation entre mot régissant et mot subordonné. Lorsqu’ un régissant commande un subordonné, cela constitue un nœud, qui peut se faire à partir d’une des quatre espèces de mots pleins”.\n\nIl en donne l’exemple suivant : « Très souvent mon vieil ami chante cette fort jolie chanson à ma fille » où l’on peut repérer:\n\nun nœud verbal, central, qui commande des actants (ami, chanson, fille) et des circonstants (souvent). La valence est « le nombre de crochets par lesquels un verbe peut attraper des actants », à peu près équivalente à « voix ».\nles nœud substantivaux (ami, chanson, fille), qui commandent des compléments (mon, vieil, cette, jolie, ma)\nle nœud adjectival (jolie) qui commande ici le subordonné ‘fort’\nle nœud adverbial, très’ étant subordonné à ‘souvent’. ”\n\n\n\n10.3.1 Arbre syntaxique\nL’arbre syntaxique est obtenu en analysant les relations entre les termes. Nous poursuivons avec UDpipe, l’annotation précédente a déjà fait le travail. A chaque mot deux informations sont associées : la première est l’index du mot auxquel il se rapporte, la seconde est la nature de la relation.\nOn utilise ici une fonction écrite par [bnosac](http://www.bnosac.be/index.php/blog/93-dependency-parsing-with-udpipe) pour donner une représentation graphique de l’arbre.\n\n\nShow the code\nplot_annotation &lt;- function(x, size = 3){\n  stopifnot(is.data.frame(x) & all(c(\"doc_id\",\"paragraph_id\", \"sentence_id\", \"token_id\",\"token\",\"lemma\",\"head_token_id\", \"upos\",\"feats\", \"dep_rel\") %in% colnames(x)))\n  x &lt;- x[!is.na(x$head_token_id), ]\n  x &lt;- x[x$sentence_id %in% min(x$sentence_id), ]\n  edges &lt;- x[x$head_token_id != 0, c(\"token_id\", \"head_token_id\", \"dep_rel\")]\n  edges$label &lt;- edges$dep_rel\n  g &lt;- graph_from_data_frame(edges,\n                             vertices = x[, c(\"token_id\", \"token\", \"lemma\", \"upos\", \"xpos\", \"feats\")],\n                             directed = TRUE)\n  ggraph(g, layout = \"linear\") +\n    geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20),\n                  arrow = grid::arrow(length = unit(4, 'mm'), \n                                      ends = \"last\", type = \"closed\"),\n                  end_cap = ggraph::label_rect(\"wordswordswords\"),\n                  label_colour = \"red\", check_overlap = TRUE, label_size = size) +\n    geom_node_label(ggplot2::aes(label = token), col = \"darkgreen\", \n                    size = size, fontface = \"bold\") +\n    geom_node_text(ggplot2::aes(label = upos), nudge_y = -0.35, size = size)  +\n    labs(title = \"Tokenization, PoS & dependency relations\")\n}\n\nplot_annotation(x, size = 3)\n\n\n\n\n\narbre de dépendance\n\n\n\n\n\n\n10.3.2 les noms communs et leurs adjectifs\nl’approche facet value\n\n\n10.3.3 les verbes et les actions\nles verbes et la coordination\nl’action dans le langage se manifeste par des formes verbales que précise certain termes\n” allez vers” ” lutter contre”, développer l’exemple",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#reconnaissance-dentités-nommées",
    "href": "q08_Analyse_syntaxic.html#reconnaissance-dentités-nommées",
    "title": "10  Annotations syntaxiques",
    "section": "10.4 Reconnaissance d’entités nommées",
    "text": "10.4 Reconnaissance d’entités nommées\nEn français courant les entités nommées correspondent largement à l’idée de noms propres. Un nom propre à une entité. Une chose qui est est indépendamment des catégories qui peuvent l’étiqueter. John Dupont, né le 19 février 1898 à Glasgow et abattu à Verdun le 8 août 1917, est un personnage unique. John Dupont ne désigne par une catégorie, mais bien une personne singulière. La désignation peut cependant être ambiguë, il y a un “Paris, Texas.”, et un Paris sur Seine. La morphologie ne résout pas l’ambiguïté.\nles entités nommées appartiennent à différentes catégories d’objets : des noms de lieux, des noms de personnes, des noms de marques, des acronymes d’organisation,\nElles ne représentent jamais une catégorie mais une unité singulière.\ndes ressources : - nametager - tds -xx",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#co-reférence",
    "href": "q08_Analyse_syntaxic.html#co-reférence",
    "title": "10  Annotations syntaxiques",
    "section": "10.5 Co-reférence",
    "text": "10.5 Co-reférence\nEn linguistique, la co-référence est le phénomène qui consiste pour plusieurs syntagmes nominaux (SN) différents contenus dans une phrase ou dans un discours, à désigner la même entité. Par exemple une personne, un lieu, un évènement, ou encore une date. Dans la terminologie linguistique, on dit qu’une co-référence est reliée à son antécédent. Pour que les syntagmes se co-réfèrent, les deux expressions doivent porter les mêmes traits : ils doivent être en accord en genre, en nombre et en personne.\nC’est une tâche difficile, les ressources en français semble inexistante dans l’univers r.\nhttps://www.rdocumentation.org/packages/cleanNLP/versions/1.9.0/topics/get_coreference",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#conclusion",
    "href": "q08_Analyse_syntaxic.html#conclusion",
    "title": "10  Annotations syntaxiques",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\nLa performance des annotateurs. Toute la qualité de l’analyse dépend de la qualité des annotateurs ? celle ci est généralement bonne au delà des 95%, mais peut ne pas résister avec certains corpus, Par exemple quand la ponctuation est absente. Dans ces cas il faut utiliser d’autres outils qui calculent les ponctuations.\nC’est cependant un outil essentiel qui permet de se concentrer sur des cibles particulières : ce dont on parle, comment on le qualifie, quelles sont les actions engagées. Il permet d’entrer plus profondément dans la texture du texte.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#co-reférences",
    "href": "q08_Analyse_syntaxic.html#co-reférences",
    "title": "10  Annotations syntaxiques",
    "section": "10.5 Co-reférences",
    "text": "10.5 Co-reférences\nEn linguistique, la co-référence est le phénomène qui consiste pour plusieurs SyntagmeGroupe de morphèmes ou de mots qui se suivent avec un sens déterminé (ex. relire, sans s’arrêter).s nominaux (SN) différents contenus dans une phrase ou dans un discours, à désigner la même entité. Par exemple une personne, un lieu, un évènement, ou encore une date. Dans la terminologie linguistique, on dit qu’une co-référence est reliée à son antécédent. Pour que les syntagmes se co-réfèrent, les deux expressions doivent porter les mêmes traits : ils doivent être en accord en genre, en nombre et en personne.\nC’est une tâche difficile, les ressources en français semble inexistante dans l’univers r. Une piste en anglais",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q08_Analyse_syntaxic.html#références",
    "href": "q08_Analyse_syntaxic.html#références",
    "title": "10  Annotations syntaxiques",
    "section": "10.7 Références",
    "text": "10.7 Références\n\n\n\n\nVerdelhan-Bourgade, M. 2020. “Lucien Tesnière, Professeur de Linguistique à Montpellier de 1937 à 1954. L’aventure d’une Grammaire.” Bulletin de l’Academie Des Sciences Et Lettres de Montpellier 51 (4562).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Annotations syntaxiques</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html",
    "href": "q03_corpus_suite.html",
    "title": "4  Corpus : techniques avancées",
    "section": "",
    "text": "4.1 La gestion des documents numériques\nDans certains cas le matériau se présentera sous forme de documents numériques tels qu’un pdf, ou même de simples images.\nvoir aussi\nhttps://cran.r-project.org/web/packages/fulltext/fulltext.pdf",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html#la-gestion-des-documents-numériques",
    "href": "q03_corpus_suite.html#la-gestion-des-documents-numériques",
    "title": "4  Corpus : techniques avancées",
    "section": "",
    "text": "4.1.1 Extraire du texte des pdf\nLe package pdftools est parfaitement adapté à la tâche. Des fonctions simples extraient différents éléments du pdf :\n\nLes information relatives au document pdf lui-même\nLa liste des polices employées\nLes attachements\nLa table des matières (si elle a été encodée)\nLes chaînes de caractères constituant le texte dans un ordre de droite à gauche et ligne à ligne, reconnaissant cependant les retours chariot, et autres sauts de lignes séparant les paragraphes\n\nChaque page est contenue dans une ligne.\n\n\nShow the code\ninfo &lt;- pdf_info(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\ninfo\n\n\n$version\n[1] \"1.4\"\n\n$pages\n[1] 12\n\n$encrypted\n[1] FALSE\n\n$linearized\n[1] TRUE\n\n$keys\n$keys$Author\n[1] \"\"\n\n$keys$Creator\n[1] \"\"\n\n$keys$Keywords\n[1] \"\"\n\n$keys$Producer\n[1] \"TCPDF 6.2.12 (http://www.tcpdf.org)\"\n\n$keys$Subject\n[1] \"\"\n\n$keys$Title\n[1] \"Le néolibéralisme et l\\u0090art de gouverner\"\n\n$keys$Trapped\n[1] \"\"\n\n\n$created\n[1] \"2021-05-04 17:33:26 CEST\"\n\n$modified\n[1] \"2021-07-02 14:10:59 CEST\"\n\n$metadata\n[1] \"&lt;?xpacket begin=\\\"ï»¿\\\" id=\\\"W5M0MpCehiHzreSzNTczkc9d\\\"?&gt;\\n&lt;x:xmpmeta xmlns:x=\\\"adobe:ns:meta/\\\" x:xmptk=\\\"Adobe XMP Core 5.6-c017 91.164464, 2020/06/15-10:20:05        \\\"&gt;\\n   &lt;rdf:RDF xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\"&gt;\\n      &lt;rdf:Description rdf:about=\\\"\\\"\\n            xmlns:dc=\\\"http://purl.org/dc/elements/1.1/\\\"\\n            xmlns:xmp=\\\"http://ns.adobe.com/xap/1.0/\\\"\\n            xmlns:pdf=\\\"http://ns.adobe.com/pdf/1.3/\\\"\\n            xmlns:xmpMM=\\\"http://ns.adobe.com/xap/1.0/mm/\\\"\\n            xmlns:pdfaExtension=\\\"http://www.aiim.org/pdfa/ns/extension/\\\"\\n            xmlns:pdfaSchema=\\\"http://www.aiim.org/pdfa/ns/schema#\\\"\\n            xmlns:pdfaProperty=\\\"http://www.aiim.org/pdfa/ns/property#\\\"&gt;\\n         &lt;dc:format&gt;application/pdf&lt;/dc:format&gt;\\n         &lt;dc:title&gt;\\n            &lt;rdf:Alt&gt;\\n               &lt;rdf:li xml:lang=\\\"x-default\\\"&gt;Le nÃ©olibÃ©ralisme et lâ\\u0080\\u0099art de gouverner&lt;/rdf:li&gt;\\n            &lt;/rdf:Alt&gt;\\n         &lt;/dc:title&gt;\\n         &lt;dc:creator&gt;\\n            &lt;rdf:Seq&gt;\\n               &lt;rdf:li/&gt;\\n            &lt;/rdf:Seq&gt;\\n         &lt;/dc:creator&gt;\\n         &lt;dc:description&gt;\\n            &lt;rdf:Alt&gt;\\n               &lt;rdf:li xml:lang=\\\"x-default\\\"/&gt;\\n            &lt;/rdf:Alt&gt;\\n         &lt;/dc:description&gt;\\n         &lt;dc:subject&gt;\\n            &lt;rdf:Bag&gt;\\n               &lt;rdf:li/&gt;\\n            &lt;/rdf:Bag&gt;\\n         &lt;/dc:subject&gt;\\n         &lt;xmp:CreateDate&gt;2021-05-04T17:33:26+02:00&lt;/xmp:CreateDate&gt;\\n         &lt;xmp:CreatorTool/&gt;\\n         &lt;xmp:ModifyDate&gt;2021-07-02T14:10:59+02:00&lt;/xmp:ModifyDate&gt;\\n         &lt;xmp:MetadataDate&gt;2021-07-02T14:10:59+02:00&lt;/xmp:MetadataDate&gt;\\n         &lt;pdf:Keywords/&gt;\\n         &lt;pdf:Producer&gt;TCPDF 6.2.12 (http://www.tcpdf.org)&lt;/pdf:Producer&gt;\\n         &lt;xmpMM:DocumentID&gt;uuid:95265141-0cc7-e3b8-5dff-27561dd19960&lt;/xmpMM:DocumentID&gt;\\n         &lt;xmpMM:InstanceID&gt;uuid:71358868-c7aa-4ef5-8b30-f18fa6312a88&lt;/xmpMM:InstanceID&gt;\\n         &lt;pdfaExtension:schemas&gt;\\n            &lt;rdf:Bag&gt;\\n               &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                  &lt;pdfaSchema:namespaceURI&gt;http://ns.adobe.com/pdf/1.3/&lt;/pdfaSchema:namespaceURI&gt;\\n                  &lt;pdfaSchema:prefix&gt;pdf&lt;/pdfaSchema:prefix&gt;\\n                  &lt;pdfaSchema:schema&gt;Adobe PDF Schema&lt;/pdfaSchema:schema&gt;\\n               &lt;/rdf:li&gt;\\n               &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                  &lt;pdfaSchema:namespaceURI&gt;http://ns.adobe.com/xap/1.0/mm/&lt;/pdfaSchema:namespaceURI&gt;\\n                  &lt;pdfaSchema:prefix&gt;xmpMM&lt;/pdfaSchema:prefix&gt;\\n                  &lt;pdfaSchema:schema&gt;XMP Media Management Schema&lt;/pdfaSchema:schema&gt;\\n                  &lt;pdfaSchema:property&gt;\\n                     &lt;rdf:Seq&gt;\\n                        &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                           &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n                           &lt;pdfaProperty:description&gt;UUID based identifier for specific incarnation of a document&lt;/pdfaProperty:description&gt;\\n                           &lt;pdfaProperty:name&gt;InstanceID&lt;/pdfaProperty:name&gt;\\n                           &lt;pdfaProperty:valueType&gt;URI&lt;/pdfaProperty:valueType&gt;\\n                        &lt;/rdf:li&gt;\\n                     &lt;/rdf:Seq&gt;\\n                  &lt;/pdfaSchema:property&gt;\\n               &lt;/rdf:li&gt;\\n               &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                  &lt;pdfaSchema:namespaceURI&gt;http://www.aiim.org/pdfa/ns/id/&lt;/pdfaSchema:namespaceURI&gt;\\n                  &lt;pdfaSchema:prefix&gt;pdfaid&lt;/pdfaSchema:prefix&gt;\\n                  &lt;pdfaSchema:schema&gt;PDF/A ID Schema&lt;/pdfaSchema:schema&gt;\\n                  &lt;pdfaSchema:property&gt;\\n                     &lt;rdf:Seq&gt;\\n                        &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                           &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n                           &lt;pdfaProperty:description&gt;Part of PDF/A standard&lt;/pdfaProperty:description&gt;\\n                           &lt;pdfaProperty:name&gt;part&lt;/pdfaProperty:name&gt;\\n                           &lt;pdfaProperty:valueType&gt;Integer&lt;/pdfaProperty:valueType&gt;\\n                        &lt;/rdf:li&gt;\\n                        &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                           &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n                           &lt;pdfaProperty:description&gt;Amendment of PDF/A standard&lt;/pdfaProperty:description&gt;\\n                           &lt;pdfaProperty:name&gt;amd&lt;/pdfaProperty:name&gt;\\n                           &lt;pdfaProperty:valueType&gt;Text&lt;/pdfaProperty:valueType&gt;\\n                        &lt;/rdf:li&gt;\\n                        &lt;rdf:li rdf:parseType=\\\"Resource\\\"&gt;\\n                           &lt;pdfaProperty:category&gt;internal&lt;/pdfaProperty:category&gt;\\n                           &lt;pdfaProperty:description&gt;Conformance level of PDF/A standard&lt;/pdfaProperty:description&gt;\\n                           &lt;pdfaProperty:name&gt;conformance&lt;/pdfaProperty:name&gt;\\n                           &lt;pdfaProperty:valueType&gt;Text&lt;/pdfaProperty:valueType&gt;\\n                        &lt;/rdf:li&gt;\\n                     &lt;/rdf:Seq&gt;\\n                  &lt;/pdfaSchema:property&gt;\\n               &lt;/rdf:li&gt;\\n            &lt;/rdf:Bag&gt;\\n         &lt;/pdfaExtension:schemas&gt;\\n      &lt;/rdf:Description&gt;\\n   &lt;/rdf:RDF&gt;\\n&lt;/x:xmpmeta&gt;\\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                                                                                                    \\n                           \\n&lt;?xpacket end=\\\"w\\\"?&gt;\"\n\n$locked\n[1] FALSE\n\n$attachments\n[1] FALSE\n\n$layout\n[1] \"single_page\"\n\n\nShow the code\nfonts &lt;- pdf_fonts(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\n\nfiles &lt;- pdf_attachments(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\n\ntoc &lt;- pdf_toc(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\") #il n'y a pas de table des matière dans ce texte\n\ntext &lt;- pdf_text(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\ncat(text[[1]]) # pour afficher le texte de la page 1\n\n\nLe néolibéralisme\net l’art de gouverner\nÀ propos de Naissance de la biopolitique\nde Michel Foucault\nFrançois Meunier\n\n\n\n\nO          n dit parfois du métier de l’historien qu’il consiste avant tout\n           à découper en périodes, à indiquer les ruptures dans le temps\n           historique, à montrer les changements d’environnement et\nde paradigme. C’est à ce travail que se consacre Michel Foucault dans\nson célèbre cours de 1978‑1979 au Collège de France connu sous le\nnom de Naissance de la biopolitique 1. Il devait porter initialement sur la\n« biopolitique », un mot chatoyant recouvrant les pratiques politiques\ncontemporaines autour du vivant (santé, démographie, sexualité, etc.).\nMais Foucault voulait montrer d’abord à quel point la venue du libé‑\nralisme avait modifié en profondeur les pratiques gouvernementales.\nPremière rupture, celle advenue à la fin du xviiie siècle avec le libéralisme\néconomique classique, selon lequel le marché devient l’instance clé dans\nl’art de gouverner, donnant à l’action publique un lieu de légitimation en\nmême temps que des limites. Seconde rupture, celle qui sépare libéralisme\net néolibéralisme, que Foucault situe dans l’après‑guerre en Allemagne,\navec ce qu’on appelle « l’ordolibéralisme ».\n\n\nÉquivocité du néolibéralisme\nReprenant, quelque quarante ans après, le fil de ce cours, nous remettons\nici en cause le découpage historique. D’abord, il nous semble que ce\n\n1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris,\nEHESS/Seuil/Gallimard, 2004.\n\n\n\n\n· ESPRIT · Mai 2021                                                                                 83/\n\n\nIl va falloir traiter ce texte en analysant précisément sa composition. Pour ce faire, il s’agîra de définir une séquence d’opérations logiques qui permette un premier nettoyage du texte. Dans l’exemple nous allons de plus essayer de conserver la structure des paragraphes du texte.\n\nSuprimer haut et bas de pages\nSupprimer les sauts de ligne\nIdentifier les sauts de paragraphes\nEnlever les notes de bas de page\nCorriger l’hyphénation ()\nregrouper les document en un seul bloc de texte\nle splitter en autant de paragraphes.\n\nOn va utiliser des fonctions de traitement de chaines de caractère avec Stringret le recours à l’art ( ici simple) des regex auxquels on consacre un développement dans le chapitres X.\n\n\nShow the code\ntex&lt;- as.data.frame(text)\ntex[1,]\n\n\n[1] \"Le néolibéralisme\\net l’art de gouverner\\nÀ propos de Naissance de la biopolitique\\nde Michel Foucault\\nFrançois Meunier\\n\\n\\n\\n\\nO          n dit parfois du métier de l’historien qu’il consiste avant tout\\n           à découper en périodes, à indiquer les ruptures dans le temps\\n           historique, à montrer les changements d’environnement et\\nde paradigme. C’est à ce travail que se consacre Michel Foucault dans\\nson célèbre cours de 1978‑1979 au Collège de France connu sous le\\nnom de Naissance de la biopolitique 1. Il devait porter initialement sur la\\n« biopolitique », un mot chatoyant recouvrant les pratiques politiques\\ncontemporaines autour du vivant (santé, démographie, sexualité, etc.).\\nMais Foucault voulait montrer d’abord à quel point la venue du libé‑\\nralisme avait modifié en profondeur les pratiques gouvernementales.\\nPremière rupture, celle advenue à la fin du xviiie siècle avec le libéralisme\\néconomique classique, selon lequel le marché devient l’instance clé dans\\nl’art de gouverner, donnant à l’action publique un lieu de légitimation en\\nmême temps que des limites. Seconde rupture, celle qui sépare libéralisme\\net néolibéralisme, que Foucault situe dans l’après‑guerre en Allemagne,\\navec ce qu’on appelle « l’ordolibéralisme ».\\n\\n\\nÉquivocité du néolibéralisme\\nReprenant, quelque quarante ans après, le fil de ce cours, nous remettons\\nici en cause le découpage historique. D’abord, il nous semble que ce\\n\\n1 - Michel Foucault, Naissance de la biopolitique. Cours au Collège de France (1978-1979), Paris,\\nEHESS/Seuil/Gallimard, 2004.\\n\\n\\n\\n\\n· ESPRIT · Mai 2021                                                                                 83/\\n\"\n\n\nShow the code\nt_reg&lt;-str_replace(tex$text,\"[\\\\s+].*Meunier[\\n]+\", \" \") # entete droite\n## on selectionne tout bloc de texte qui commence par un nombre indéterminée de blanc qui s'achève par n'importe quel caractère répétés mais terimé par la séquence Meunier suivie de sauts de ligne.\nt_reg&lt;-str_replace(t_reg,\"[\\\\s+].*gouverner[\\n]+\", \" \") # entete gauche\nt_reg&lt;-str_replace_all(t_reg,\"[\\\\s+].*2021[\\n]\", \" \") # bas de page  gauche\nt_reg&lt;-str_replace_all(t_reg,\"ESPRIT.*[\\n]\", \" \") # bas de page droit\n\n#on marque les paragraphes avec la chaine XXX pour les splitter dans un second temps\n\n\nt_reg&lt;-str_replace_all(t_reg,\"\\n\\n\\n\", \"XXX\") \n\n# On supprime les saut de ligne en les remplaçant par un espace\n\nt_reg&lt;-str_replace_all(t_reg,\"[\\n]\", \" \")\n\n#on enlève les notes de bas de page\nt_reg&lt;-str_replace_all(t_reg,\"\\\\d\\\\s[\\\\-].*XXX\", \"XXX\")\n\n#on regroupe les pages\n\nt&lt;-paste(unlist(t(t_reg)), collapse=\" \")\n\n\n\n#on enlève les notes dans le texte\n\nt&lt;-str_replace_all(t,\"[A-Z|a-z]+\\\\d\\\\s[\\\\-]\", \" \")\n\nt&lt;-str_replace_all(t,\"\\\\d\\\\d\\\\s[\\\\-]\", \" \")\n\n#hyphenation\n\nt&lt;-str_replace_all(t,\"[A-Z|a-z]+[\\\\-]\\\\s\", \"\")\n\n#pour enlever les espaces excedentaires\n\nt&lt;-str_squish(t)\nt\n\n\n[1] \"Le néolibéralisme À propos de Naissance de la biopolitique de Michel Foucault O n dit parfois du métier de l’historien qu’il consiste avant tout à découper en périodes, à indiquer les ruptures dans le temps historique, à montrer les changements d’environnement et de paradigme. C’est à ce travail que se consacre Michel Foucault dans son célèbre cours de 1978‑1979 au Collège de France connu sous le nom de Naissance de la biopolitique 1. Il devait porter initialement sur la « biopolitique », un mot chatoyant recouvrant les pratiques politiques contemporaines autour du vivant (santé, démographie, sexualité, etc.). Mais Foucault voulait montrer d’abord à quel point la venue du libé‑ ralisme avait modifié en profondeur les pratiques gouvernementales. Première rupture, celle advenue à la fin du xviiie siècle avec le libéralisme économique classique, selon lequel le marché devient l’instance clé dans l’art de gouverner, donnant à l’action publique un lieu de légitimation en même temps que des limites. Seconde rupture, celle qui sépare libéralisme et néolibéralisme, que Foucault situe dans l’après‑guerre en Allemagne, avec ce qu’on appelle « l’ordolibéralisme ».XXXÉquivocité du néolibéralisme Reprenant, quelque quarante ans après, le fil de ce cours, nous remettons ici en cause le découpage historique. D’abord, il nous semble que ce XXX · n’est pas autour de la notion de marché qu’il faut attacher la genèse du libéralisme classique, mais plutôt autour de l’idée d’une société capable de s’organiser en dehors du prince. Ensuite, la rupture constituante du néolibéralisme se situe postérieurement à l’arrivée de Reagan et Thatcher au pouvoir, lorsqu’on aura théorisé et mis en pratique la financiarisation de l’économie comme instance ordonnatrice (cela donc après le cours de Foucault). S’agissant de l’ordolibéralisme, il présente de fortes continuités avec le libéralisme classique, même s’il garde les marques d’un mer‑ cantilisme qui s’est développé tardivement en Allemagne. Il n’est guère étonnant que ce courant se soit fondu si aisément dans le modèle social de marché auquel on associe davantage la social‑démocratie allemande que le libéralisme débridé. Cela devrait aider à mieux caractériser ce qu’il faut entendre par « néo‑ libéralisme », un mot devenu équivoque. Si l’on peut créditer Foucault d’être parmi ceux qui l’ont inventé, c’est plus chez lui par commodité verbale2. Lorsqu’il rédigea le résumé du cours au terme de son année, c’est significativement le seul mot de « libéralisme » qu’il a retenu. Le cours bénéficie toujours d’une forte aura. En effet, il est la seule incursion de Foucault dans l’histoire contemporaine ; il introduit le concept de gouvernementalité, qui a acquis une certaine place dans la science politique. Son style attire, mélange d’écrit et d’oral où la pensée se construit par bonds successifs et inattendus, avançant « en écrevisse » comme il le dit. Le texte déroute aussi parce qu’il ne cherche pas à construire un contre‑modèle quand il analyse le courant intellectuel libéral. La dissection des textes s’accompagne à l’évidence chez lui d’une certaine fascination. Il rabroue même son public quand celui‑ci voudrait le voir glisser vers des objections trop faciles au libéralisme3. Il n’est pas étonnant que des milieux se réclamant du libéralisme, y compris poli‑ tique, s’en réclament presque autant que ses adversaires.XXX XXX Enfin et surtout, le texte se concentre pour l’essentiel sur l’Allemagne, un pays que Foucault connaissait très bien pour y avoir enseigné : « Dans cette seconde moitié du xxe siècle, le libéralisme est un mot qui nous vient d’Allemagne. » C’est l’ordolibéralisme qu’il désigne comme « néolibéral ». Il traite assez peu, de façon surprenante, de ce qu’on appelle l’École de Chicago, que Foucault appelle l’anarcho‑libéralisme, née dans les années 1930, dont l’influence a été majeure dans le renversement idéologique opéré à l’époque de Reagan aux États‑Unis : « Je ne suis pas sûr d’avoir le temps de parler des Américains. »XXXL’âge classique L’économie politique, à partir de Turgot et Smith, s’est bâtie sur une critique du régime mercantiliste. Le mercantilisme, c’est l’idéologie d’un État en constitution, qui organise l’hégémonie du prince, qui pour cela capte des richesses, s’organise administrativement, privilégie la bonne collecte des impôts et l’exportation, et où un ordre juridique se substitue au droit divin du souverain. Cette critique avait commencé sur le plan des idées politiques. Chez Locke et Spinoza, le citoyen naissait et la liberté politique était réclamée. Mais on ne touchait pas encore à l’organisation sociale du royaume. Le pas en avant fait par l’économie politique a été de donner toute sa place à la nouvelle classe des marchands, consciente désormais de contribuer à l’enrichissement du royaume. On quittait une vision assez prédatrice, où l’échange était essentiellement un jeu à somme nulle, où ce que gagnait un pays était une perte pour l’autre et où le talent du prince consistait à ce que son pays se sorte bien de cette confrontation. La vision classique est inverse : il y a possibilité d’un échange équitable, qui se fait fina‑ lement au bénéfice – « à l’intérêt » – des deux parties4. Il y a possibilité d’une croissance endogène où le marchand réinvestit son profit dans des activités nouvelles, une morale que le puritain anglais avait parfaitement intériorisée. Il s’introduit à plein la notion de concurrence qui gomme les situations de rente. Naissait dans la foulée la notion d’intérêt général et de bien commun, davantage au centre des intérêts individuels dans la XXX · tradition britannique que l’expression d’une souveraineté première dans la tradition française. Foucault décrit cette transition mais force le trait quand il indique, dans une phrase significative, que cet âge classique est celui où le marché devient un principe de régulation politique en remplacement de l’ordre juridique qui prévalait auparavant. Selon lui, c’est désormais la légi‑ timité marchande qui non seulement limite mais organise et structure la décision publique. Elle devient le « lieu de véridiction » de l’action gou‑ vernementale : « Ce lieu de formation de la vérité […], il faut le laisser jouer avec le moins d’interventions possibles pour qu’il puisse et formuler sa vérité et la proposer comme règle et norme à la pratique gouvernementale. Ce lieu de vérité, c’est bien entendu non pas la tête des économistes, mais le marché 5. » Mais Foucault anticipe de près de deux siècles. D’une part, la prévalence du droit est plus manifeste encore à l’époque classique qu’à âge préclassique ; les structures de marché s’approfondissent et s’appuient sur des contrats établis, le mot étant d’ailleurs repris dans la notion de contrat social qui naît à cette période. D’autre part, il n’y a pas pour les classiques une rupture dans la conception du marché. Les prix auraient été avant cette période, dit Foucault, des prix ordonnés selon des critères d’équité ou de stabilité sociale, des prix « justes » et non, comme postérieurement, des prix régis par la loi de l’offre et la demande. C’est ce que dément une bonne part de l’historiographie moderne, à partir d’auteurs comme de Roover ou Todeschini6, pour qui le juste prix n’était rien d’autre que le prix de marché, mais d’un marché mis en position de bien fonctionner, une idée qui fera apparaître progressivement le concept de concurrence, très net chez les auteurs de la seconde scolastique dans l’Espagne de la fin du xvie siècle, celui de monopole étant déjà ancien. La rupture, majeure, avec le mercantilisme existe, mais elle est que la société peut se libérer du prince, que l’économie peut fonctionner malgré sa dispersion, sans coordination venue d’en haut. La fameuse « main invisible », dans le seul passage de La Richesse des nations où Smith la mentionne, y est comme une image pour résoudre ce paradoxe d’une XXX économie qui évite le chaos alors que les centres de décision (de pouvoir) sont dispersés, chacun d’eux ne prenant nullement en compte la décision des autres. Il s’agit là d’une notion commune qui exprime la différence entre la cause finale d’une action et l’intention des agents7. Sur ce point, nos économistes restent même en retrait du libéralisme politique d’un Locke ou d’un Montesquieu pour qui la société fonctionne non pas malgré sa dispersion mais grâce à une dispersion des pouvoirs et des centres de décision, qui devient l’élément régulateur par lequel on atteint le bien commun. Dans ce contexte, le gouvernement prend une place très différente. La société connaît des contraintes et interactions multiples et l’art de gou‑ verner consiste à les prendre en compte. Il s’introduit une rationalité différente, une rationalité des fins, que Foucault décrit très bien. La question posée est celle de l’utilité ou de l’efficacité des effets induits d’une mesure. Mais ce qui est important Ce qui est important et moins et moins bien vu par Foucault est que bien vu par Foucault est que cette rationalité nouvelle peut justifier cette rationalité nouvelle peut autant l’intervention publique que son justifier autant l’intervention retrait. On pourra soutenir dans un cas publique que son retrait. que l’autonomie de la société signifie que le gouvernement est en trop, qu’il perturbe l’ordre économique ou qu’il interfère de façon coercitive sur la liberté individuelle. Mais on peut alternativement affirmer que le gouvernement a en main des guides et préceptes, une rationalité économique, qui l’autorisent et qui même le poussent à intervenir dans l’ordre économique. On voit poindre ici une bifurcation profonde, toujours actuelle, qui verra tout à la fois un Hayek libertarien ou un Keynes interventionniste se réclamer de la tradition économique libérale. À vrai dire, pour ce second courant libéral, on peut corriger la phrase de Foucault citée plus haut : autant que dans le marché, le lieu de vérité est dans la tête des économistes. Ils affichent déjà leur prétention en venant XXX · comme des ingénieurs sociaux, comme les Locke et Montesquieu l’ont été en matière d’institutions politiques, devisant le bon mécanisme ou le bon montage. On le voit déjà chez Quesnay, un physiocrate qui précède Smith, dans ses réflexions sur ce que doit être un « bon impôt », c’est‑à‑dire un impôt efficace, dans le sens de l’intérêt bien compris du royaume. Turgot le sera plus encore. Et au siècle suivant, des économistes comme Cournot ou Bertrand introduiront l’idée d’un calcul économique aux fins d’une utilité sociale maximale, et cela dans des domaines comme la décision de construire un pont, où le marché est absent et n’a rien à dire.XXX L’ordolibéralisme en Allemagne C’est un sujet d’étonnement, mal couvert par les historiens, que le libé‑ ralisme ait eu une place si réduite dans la riche tradition intellectuelle allemande, Kant faisant bien sûr une immense exception. Remontant hardiment dans le temps, l’une des explications peut tenir au luthéria‑ nisme dans sa version allemande, qui a été tout autant une école d’indi‑ vidualisme face au divin que de soumission face au pouvoir temporel, un terrain peu propice à l’élaboration d’un pacte collectif donnant sa légitimité au pouvoir, comme dans la tradition anglaise ou française. Un autre facteur déterminant tient au retard allemand à construire son État national. Il l’a fait tout du long du xviiie siècle (s’agissant de la Prusse) et du xixe siècle sous la houlette prussienne, avec un vif ressen‑ timent vis-à-vis des autres États européens déjà fortement constitués, et particulièrement de la Grande‑Bretagne qui imposait son libéralisme à coups de libre‑échange. Le romantisme, au début de ce siècle, en a été le pendant culturel, avec une forte dimension nationaliste, contredisant l’idéal libéral des Lumières. Le Zollverein (une union douanière entre États allemands mise sur pied dans les années 1830) exprimait bien le vœu des élites libérales à l’ouest de l’Allemagne, mais était dans la réalité un projet d’essence protectionniste actionné par la Prusse et théorisé par Friedrich List comme l’affirmation d’une souveraineté propre. Il a échoué en tant que projet politique. Au fond, l’Allemagne a connu son moment mercantiliste, mais avec un siècle et demi de retard. Ce sont les notions de richesse de l’État, de levées d’impôt, de contrôle de la monnaie, de commerce extérieurXXX devenu l’enjeu d’âpres batailles avec les pays voisins qui prévalaient. Un épisode intellectuel intéressant, qui allait préfigurer les tenants de l’libéralisme et les distinguer des libéraux tant classiques qu’états‑uniens, a été celui des « sciences camérales ». Il s’agissait d’écoles administratives mises en place par le roi de Prusse pour former le personnel capable d’assurer la puissance du souverain, non par des moyens militaires (cela allait venir plus tard), mais par une gestion réglée du pays, par voie de « police », selon le mot retenu à l’époque et commenté longuement par Foucault. Cette tradition s’est poursuivie à l’époque de Bismarck puis sous Weimar avec ce grand pré‑keynésien qu’a été Rathenau. Les penseurs de l’ordolibéralisme ont tout à la fois bousculé cette tradition et en ont subi l’influence. Les deux grands noms sont Wilhelm Röpke et Walter Eucken, suivis par Ludwig Erhard, futur chancelier, qui a donné au mouvement sa consistance politique. Ils ont bâti toute l’ossature idéologique du Parti chrétien‑démocrate allemand, sous le terme d’« éco‑ nomie sociale de marché », un terme que le SPD allait adopter lui aussi, soucieux, devant la popularité de la notion, de ne pas être durablement exclu du pouvoir. L’ordolibéralisme se caractérise d’abord par un rejet de l’intervention directe de l’État dans le jeu de l’économie : ni rôle stabilisateur, ni rôle redistributeur. Second principe, proche du premier, le gouvernant doit écouter ce que dit le système des prix, c’est‑à‑dire l’information qu’apporte le fonctionnement d’un marché concurrentiel. Mais il s’agit ici d’un choix presque autant forcé qu’idéologique. Tony Judt, dans son his‑ toire de l’après‑guerre8, signale cet épisode déterminant qu’a été le début de la guerre froide. Les États‑Unis ont imposé à la Grande‑Bretagne et à la France de se réarmer. Mais bien sûr pas à l’Allemagne. Or celle‑ci gardait largement intact son appareil industriel de guerre, qui s’est mis naturellement à tourner pour nourrir le reste des pays européens en produits venus de la mécanique ou de la chimie. Le modèle extraverti propre à l’Allemagne était lancé, une voie qu’ont trouvée plus tard le Japon et les autres pays asiatiques. S’appuyer sur la logique du marché international devenait alors l’élément clé d’une stratégie imposée par l’environnement autant que par la doctrine. Notons le contraste avec la XXX · France : trente ans après, au moment où Foucault faisait son cours, il y avait encore des prix administrés. Mais écouter le système des prix suppose que le marché fonctionne bien, qu’il repose sur un socle approprié : une fixité de la monnaie d’une part, et surtout une action délibérée de l’État pour imposer le principe de concurrence. Le marché n’est pas un ordre transcendant que toute initiative de l’État irait perturber ; il y a au contraire l’idée qu’il est fragile, qu’il y a une pente naturelle vers la formation de monopoles et de col‑ lusions, qu’il ne fonctionne pas naturellement sans un cadre très rigide que précisément l’État apporte9. Pour préserver la concurrence, il faut d’ailleurs encourager les entreprises familiales (le Mittelstand dont on sait aujourd’hui le succès économique), l’artisanat et une agriculture formée de petites exploitations. On n’est pas surpris alors du compromis trouvé entre la France et l’Allemagne au moment du traité de Rome : un pivot venu de Paris, à savoir la politique agricole commune qu’acceptaient de plus ou moins bon gré nos ordolibéraux, et, venue de Bonn, une solide autorité de la concurrence, avec un soutien plus mitigé de Paris. Ainsi, l’État intervient en amont, sur la structure plutôt que sur ses effets. Il pose la « règle », un mot toujours très fort pour les Allemands. La structure, ce sera davantage l’entreprise que le consommateur, ce sera davantage le droit que les dispositifs économiques et fiscaux. Par cohérence, l’ordolibéralisme avait une vue très restrictive de ce que devait être l’État social, bien loin du projet bismarckien. Cela en raison du primat donné aux prix et à l’entreprise. Une santé, une éducation et une culture socialisées ou gratuites, c’était intervenir à rebours puisqu’on niait l’apport du système de prix dans l’allocation des ressources. La protection sociale était pour eux l’affaire des individus. Si l’on devait aider, c’était uni‑ quement par le jeu des revenus, en préservant les mécanismes de marché. Cela n’a bien sûr pas résisté aux contraintes politiques du moment, sous l’influence notamment du SPD, de la tradition bismarckienne et des milieux catholiques, si l’on se rappelle l’influence qu’avait eue longuement le premier parti social‑chrétien d’Europe, le Zentrum, né en 1870. Mais, à elle seule, la concurrence ne saurait suffire. Röpke, cité par Foucault, le disait avec force : « Ne demandons pas à la concurrence plus qu’elle XXX ne peut donner. Elle est un principe d’ordre et de direction dans le domaine particulier de l’économie de marché et de la division du travail, mais non un principe sur lequel il serait possible d’ériger la société tout entière. Moralement et sociologiquement, elle est un principe dangereux, plutôt dissolvant qu’unifiant. Si la concurrence ne doit pas agir comme un explosif social ni dégénérer en même temps, elle présuppose un encadrement d’autant plus fort, en dehors de l’économie, un cadre politique et moral d’autant plus solide 10. » Foucault use de litote quand il interprète cette phrase comme une « ambiguïté » du libéralisme à l’allemande. On voit ici la différence avec l’autre versant, disons keynésien ou acti‑ viste, dans la bifurcation libérale mentionnée plus haut. Le camp activiste voyait le rôle de l’État en creux en quelque sorte, dans les défaillances du marché qu’il fallait compenser, n’hésitant pas à se substituer à lui s’il le fallait. L’ordolibéralisme insistait au contraire pour une action de l’État en amont, consistant à créer les conditions par lequel le marché continuerait à jouer pleinement son rôle, pour éviter les interventions en aval. Ce débat persiste pleinement aujourd’hui. Le libéralisme des écono‑ mistes américains, pour y venir, était plus extrême : c’est parce qu’il y a inévitablement des défaillances de l’État qu’il faut y substituer le marché.XXXL’École de Chicago Foucault ne traite que cursivement des représentants de l’école améri‑ caine du libéralisme économique, dont Frank Knight, Henry Simons, George Stigler et, plus tard, Milton Friedman. Ces économistes allaient transposer dans l’ordre social, en le poussant à l’extrême, une autre tradition économique libérale, née dans les années 1870, appelée néo‑ classique ou marginaliste. Dans un marché bien réglé, selon ce courant, les prix, le taux d’intérêt ou les salaires s’imposent de façon transcendante aux entreprises et aux ménages. Ceux‑ci sont immergés dans un monde dont les paramètres leur échappent. Ils reçoivent des informations externes et y répondent, méca‑ niquement, en ajustant leur comportement. La règle du profit maximum n’est qu’une règle de survie de l’entreprise. L’individu est représenté via un modèle très sommaire, l’homo œconomicus, égoïste et optimisateur, dont on ne trouve pas la moindre trace chez les pionniers du libéralisme 1XXX · économique, pas plus que chez les fondateurs de l’ordolibéralisme ou d’ailleurs chez Hayek. Le système des prix est comme le système nerveux, celui qui transmet les informations aux agents, qui les motive et les stimule. Chez ces libéraux américains, toute perturbation à son endroit est a priori néfaste. Par exemple, une organisation en syndicats ou un salaire minimum, parce qu’ils sortent du marché « libre », se retournent finalement contre les travailleurs (et les plus modestes, pour faire bonne mesure) en créant du chômage. On voit la différence avec Adam Smith, qui recommande que les travailleurs s’unissent dans la négociation sala‑ riale face à des patrons qui ont toute facilité, vu leur faible nombre, d’organiser la collusion entre eux11 ; de même qu’avec l’ordolibéralisme allemand qui, dès 1951, promulguait les premières lois d’après‑guerre sur la codétermination et les comités d’entreprise12. Les agents étant emmaillotés dans un tissu complexe d’incitations exo‑ gènes, leur autonomie est extraordinairement réduite. Foucault disait de l’âge classique que « le nouvel art gouvernemental consomme de la liberté ». Ici, il n’y a plus une once de liberté. L’individu est comme une molécule réa‑ gissant, selon des lois d’optimisation, aux impulsions externes fournies par le marché. L’économiste devient celui qui dit : « Incitations ! » La gouvernementalité par le marché est radicale et l’analyse de Foucault commence ici à prendre son sens : « L’ homo œconomicus, c’est celui qui est éminemment gouvernable. » Les règles de marché deviennent les étalons d’une bonne action publique ; elles en donnent les codes. Foucault introduit ici une distinction entre ce qu’il appelle le sujet de droit et le sujet d’intérêt : le sujet d’intérêt répond à des incitations venues de l’extérieur et renvoie rationnellement sa réponse ; le sujet de droit est mû par des motivations intrinsèques13. Par exemple, un interdit légal, assorti d’une sanction ou d’une peine, est un interdit pour le sujet de droit, mais un coût pour le sujet d’intérêt. Le délinquant chez Gary Becker, un économiste de Chicago, calculera les avantages et les coûts de son acte (dont l’amende ou la prison) et prendra sa décision en conséquence (et en cela n’est pas délinquant, ou alors nous le sommes tous).XXX 1XXX On retrouvera toutefois au sein de cette école américaine la bifurcation décrite plus haut entre libertariens et activistes. On n’abandonnera pas forcément l’ingénierie sociale chère à l’économiste, mais imbriquée dans l’ordre du marché : s’il y a, par exemple, une discrimination par l’argent dans l’accès à l’éducation, on distribuera des coupons pour permettre aux gens de tous les milieux de se présenter à l’école privée de leur choix, préservant ainsi la concurrence. Friedman recommandait le revenu uni‑ versel de base. Un courant intéressant a aussi émergé sous la désignation de market design, à savoir comment structurer un marché afin qu’il réponde à certains objectifs de politique économique ou sociale, un marché censé donc être asservi à la cause gouvernementale, à faire délibérément partie de sa panoplie d’instruments14. La relation État‑marché est à deux voies. Si d’ailleurs les individus sont soumis aux incitations et qu’un Léviathan apprend à bien les manier, tout lui devient possible. Un pas sera franchi quelque dix ans après le cours de Foucault : celui de la montée en régime d’un nouvel « espace de véridiction », le marché financier. Le voici qui, mieux que le marché des biens et du travail, pourra allouer les ressources au sein de l’économie et répartir le risque, avec des frictions minimales. C’est la valorisation en continu des actifs sur les marchés financiers qui est le juge de paix, y compris dans l’allocation de capital aux entreprises, y compris, prétend‑on, dans la gestion des entreprises. S’il faut caractériser en quelques mots ce qu’est le néolibéralisme dans le champ économique, on dira qu’il est l’addition du primat donné aux messages des prix, de la mise en retrait de l’État, de la plus grande fluidité donnée aux marchés financiers et de l’instrument-marché pour remédier aux défaillances que le marché peut entraîner et que l’action de l’État entraîne à coup sûr. On est très loin alors du libéralisme classique, très loin aussi de sa variante allemande d’après‑guerre, lesquels nous donnent, à n’en pas douter, de meilleures pistes pour affronter les défis de l’éco‑ nomie globale de demain.XXX 1XXX · Powered by TCPDF (www.tcpdf.org)\"\n\n\nShow the code\n#On découpe en paragraphes\nt&lt;- str_split(t, \"XXX\",simplify = TRUE)\nt2&lt;-as.data.frame(t(t))\n\n\nPlus les textes sont standardisés et plus simple est le processus d’importation des pdf. Si l’on souhaite aller plus loin on recommande par exemple https://ropensci.org/blog/2018/12/14/pdftools-20/ pour extraire un tableau. ( à développer en 4 ou 5 lignes avec des références)\n\n\n4.1.2 La numérisation et l’OCR\nD’immenses archives sont numérisées, ce qui signifie qu’elles sont stockées sous forme d’image. L’information est contenu dans les pixels, et l’enjeu est d’y reconnaitre des formes caractéristiques : alphabet, ponctuation à travers de multiples variations. Les plus fortes sont celles manuscrites, mais l’écriture typographique est aussi très variables dans ses formes. C’est un enjeu industriel ancien. La reconnaissance optique des caractères a cependant fait d’immense progrès et atteint des niveaux de performance élevés.\nLe traitement des adresses a sans doute été le problème principal qui a stimulé les technologies de l’OCR. La qualité du matériau est essentiel, et s’assurer que les expéditeurs choisissent un modèle conventiel et normé de rédaction de l’adresse est une condition de leur succcès. La situation idéale ressemble à ceci.\n\n\n\nModèle de rédaction correcte d’une adresse postale\n\n\nLa réalité ressemble souvent à celà\n\n\n\nà çà\n\n\nDans un environnement en science sociale la situation est moins complexe, les documents analysés ne seront le plus souvent pas des documents mansuscrits ( sauf pour les médiévistes), mais un scan de document plus structuré. Par exemple les jpg\nUne solution pour R est tesseract. C’est un package qui permet d’accéder au programme du même nom, développé à l’origine chez Hewlett-Packard Laboratories entre 1985 et 1994, avec quelques modifications supplémentaires apportées en 1996 pour le portage sur Windows, et sur C en 1998. Tesseract a été mis en open Source par HP, en 2005, puis de 2006 à novembre 2018, a continué d’être développé par Google. Il s’appuie sur des réseaux neuronaux de type LSTM. C’est une petite, mais puissante intelligence artificielle qui supporte plus d’une centaine de langues.\nTestons-le sans attendre avec le texte suivant. Extrait du premier article du premier numéro de la revue ” Etalages” Publiée en France de 1909 à 1938. L’image est un extrait du document numérisé fournit par la BNF.\n\n\n\nLettre de motivation\n\n\nhttps://gabriben.github.io/NLP.html#introduction\n\n\nShow the code\nlibrary(tesseract)\ntesseract_download(\"fra\") #pour télécharger le modèle de langage\n\n\n[1] \"C:\\\\Users\\\\33623\\\\AppData\\\\Local\\\\tesseract5\\\\tesseract5\\\\tessdata/fra.traineddata\"\n\n\nShow the code\nt1&lt;-Sys.time()\ntext &lt;- tesseract::ocr(\"./data/N1_avril1909b.jpeg\", engine = \"fra\")\nt2&lt;-Sys.time()\nt&lt;- t2-t1 #pour compter le temps de calcul\ncat(text) #pour afficher le texte avec sa mise en page\n\n\nEn fondant la ‘Revue Internationale de\nl'Etalage, nous avons cédé à une ambition\nqui. peut s'exprimer en trois mots : Faire\nœuvre ulile.\n\nFaire œuvre utile en facilitant la tâche du\ncommerçant détaillant par une documen-\ntation sérieuse et raisonnée se rapportant aux\nchoses de sa profession, et notamment aux\nmeilleurs procédés pour attirer la clientèle.\n\nFaire œuvre utile surtout en vulgarisant\npratiquement l’art appliqué à l'Etalage.\n\nSi notre carrière de publiciste s'honore sans\nfausse modestie d’avoir doté d’autres corpo-\nrations de revues professionnelles qui rendent\ndes services, nous devons déclarer en toute\nsincérité que la fondation d'aucune de celles- ;\nci ne répondait à un tel besoin.\n\nAussi est-ce d’un pied ferme que nous\nnous engageons dans le vaste champ d’ac-\ntions qui s'ouvre devant nous.\n\nL'Etalage!.. Quel sujet intéressant et\ncaptivant au premier chef. N’est-il pas vrai-\nment incompréhensible qu'aucuñe plume\nautorisée n’ait jamais tenté, en France, d'en\nétablir les règles, d’en vulgariser les principes.\nEt pourtant, en envisageant la chose à un\npoint de vue général, ne peut-on pas dire que\nl'Etalage est un des ornements de nos cités\nqui met sans cesse en évidence, aux yeux\nde tous, la supériorité du goût et les apti-\ntudes artistiques d’un pays ! Tandis qu’au\npoint dé vue de leurs propres intérêts, nos\nlecteurs ne doivent-ils pas considérer l’Eta-\nlage comme l'aimant qui attire les affaires !\n\nOn le comprend si bien aujourd’hui que le\ncommerçant qui se désintéresse de son éta-\nlage est une exception appelée de plus en\nplus à disparaître.\n\nEt cependant, parmi ceux qui se rendent\nun compte exact de l’importance du rôle de\nVEtalage dans les affaires de détail, combien\npeu possèdent à fond l’art de présenter leurs\nproduits sous leur meilleur jour et de la façon\nla plus favorable. ‘\n\nQuoi de surprenant d’ailleurs à cela. En\n\n\nShow the code\n#tesseract_info() #voir les langues disponibles\nt\n\n\nTime difference of 1.677044 secs\n\n\nPour améliorer la performance qui peut se mesurer au niveau des lettres, mais doit surtout l’être au niveau des mots, deux stratégies sont possibles. La première de pre-processing, la seconde de post-processing avec un mécanisme de détection et de correction d’erreurs. Le pre-processing consiste à traiter l’image en renforçant les contrastes , en éliminant le bruit, on en rend les pixels mieux digestes pour tesseract. C’est ce à quoi s’attache le package magick qui offre un bouquet de fonctions à cette fin. Nous laissons le lecteur le tester seul.\nLe post-processing sert à introduire des mécanismes de correction d’erreurs au niveau des mots. Pour une idée de ce type de développement voir Gabriel, Yadir, Xiaojie, Mingyu\nNaturellement, un paramètre important est la vitesse de traitement des images. Dans un projet complet on peut être amener à traiter des centaines images en boucle. Dans notre exemple la durée est de t secondes, autrement dit 6 images à la minute ou 360 à l’heure…\n\n\n4.1.3 Du speech au texte\nLa tradition méthodologique de la sociologie est celle de l’entretien, avec toute sorte d’acteurs. Elle aboutit à la production de transcriptions, plus ou moins détaillées et précises. Mais des textes\nOn peut désormais enregistrer la paroles des interfaces vocales. Le speech to text est de plus en plus efficace, voir l’API de google. Il existe déja des packages sur R qui permettent d’accéder aux solutions de google langage qui nécessite une clé d’API.\nhttps://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html\nune autre solution\nOn ne fait qu’entre-ouvrir le sujet, mais il est certainement un des futurs du NLP.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html#lexploitation-de-base-de-données-textuelles",
    "href": "q03_corpus_suite.html#lexploitation-de-base-de-données-textuelles",
    "title": "4  Corpus : techniques avancées",
    "section": "4.2 L’exploitation de base de données textuelles",
    "text": "4.2 L’exploitation de base de données textuelles\nLes bases de données textuelles sont désormais nombreuses , variées et normalisées. Elles permettent de constituer rapidement des corpus étoffés\n\n4.2.1 le cas Europresse\nOn commence par un exemple simple en utilisant la base europresse. L’objectif est de constituer un fichier de références bibliographiques, exploitable via R.\nDans Europresse , nous avons fait une recherche sur les articles comprenant le terme ” vaccination” dans la presse nationale françaises, constituées de 14 titres. On retient les 400 derniers articles du mois de Juillet 2021, le 12 le Président faisait une allocution.\nOn utilise revtools pour sa fonction d’importation des fichiers au format .ris et de transformation en data frame structuré\n\n\nShow the code\nlibrary(revtools)\ndf &lt;- read_bibliography(iconv(\"./data/vaccination.ris\"))%&gt;%\n  mutate(jour=str_sub(DA, 9,10))\n\nflextable(head(df,3))\n\n\nlabeltypetitleauthorjournalabstractinstitutionyearpageslanguageurlDAjourPaul.Ortoli_2021_LeMoNEWSLa Corse redoute une gÃ©nÃ©ralisation de la reprise Ã©pidÃ©miquePaul OrtoliLe MondeAlors que la Balagne, en Haute-Corse, a atteint vendredi 16 juillet un taux d'incidence de 1 066 cas pour 100 000 habitants, et un taux de positivit&lt;c3&gt;&lt;a9&gt; de 10 ...Ajaccio correspondant20219FranÃ§aishttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210720%c2%b7LM%c2%b744166282021/07/20/20X2021_LeMoNEWSEnquÃªte ouverte aprÃ¨s l'incendie d'un centre de vaccinationLe MondeLe centre de vaccination contre le Covid-19 de la commune d'Urrugne, dans les Pyr&lt;c3&gt;&lt;a9&gt;n&lt;c3&gt;&lt;a9&gt;es-Atlantiques, d&lt;c3&gt;&lt;a9&gt;partement o&lt;c3&gt;&lt;b9&gt; le taux d'incidence est sup&lt;c3&gt;&lt;a9&gt;rieur au seuil d'alerte, a &lt;c3&gt;&lt;a9&gt;t&lt;c3&gt;&lt;a9&gt; ...202112FranÃ§aishttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210720%c2%b7LM%c2%b755160672021/07/20/20Catherine.Pacary_2021_LeMoNEWSEnquÃªte sur le record de vitesse des vaccins contre le Covid-19Catherine PacaryLe MondeC'est un film qui tombe &lt;c3&gt;&lt;a0&gt; point. Alors que la vaccination se r&lt;c3&gt;&lt;a9&gt;v&lt;c3&gt;&lt;a8&gt;le indispensable pour lutter contre la pand&lt;c3&gt;&lt;a9&gt;mie de Covid-19, il r&lt;c3&gt;&lt;a9&gt;pond &lt;c3&gt;&lt;a0&gt; une interrogation r&lt;c3&gt;&lt;a9&gt;pandue : comment les ...Documentaire2021TEL21FranÃ§aishttps://nouveau.europresse.com/Link/PARIS10T_1/news%c2%b720210720%c2%b7LM%c2%b703778922021/07/20/20\n\n\nShow the code\ng22&lt;-ggplot(df, aes(x=jour))+\n  geom_bar()+\n  labs(x= \"Jour du mois de Juillet 2021\",y=NULL)+\n  geom_vline(xintercept=6, linetype=\"dashed\", color = \"red\")+\n  facet_grid(vars(journal))\ng22\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Jouer avec les bases bibliographiques\nFulltext doit être développé.\nhttps://books.ropensci.org/fulltext/data-sources.html\nOn propose un cas à partir de scopus ( demander à Olivier)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html#lire-le-web-scrapping",
    "href": "q03_corpus_suite.html#lire-le-web-scrapping",
    "title": "4  Corpus : techniques avancées",
    "section": "4.3 Lire le web : Scrapping",
    "text": "4.3 Lire le web : Scrapping\nLe scrapping correspond à un internet sauvage où la collecte d’informations se traduit par une technique de chasseurs-cueilleurs, le glanage. C’est l’activité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un navigateur (on préfère l’expression des québécois : des butineurs).\nElle consiste à construire un robot capable de lire et d’enregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie d’exploration du web préalablement définie. En réalité le scrapping pose deux problèmes :\n\nCelui de la structure de recherche. C’est le problème que relève les spiders, des robots qui recherchent dans les pages des liens, et vont de proche en proche, de lien en lien, pour explorer un domaine. Ils peuvent être plus systématiques et prendre davantage de l’organisation et structure d’un site web pour énumérer les pages.\nCelui de la collecte de l’information sur chacune des pages. Il s’appuie sur le principe que le langage html est un langage à balise où le contenu et le contenant sont clairement séparés. Par exemple, dans le corps de texte d’une page on définira un titre par la balise\n\ndont l’instruction s’achève par la balise\n\n. On sépare ainsi clairement le contenu de la forme.\n\n`\n\n\nUn titre de niveau 1 (un gros titre)\n\n&lt;p&gt;Un paragraphe.&lt;/p&gt;\n\n&lt;h2&gt;Un titre de niveau 2 (un sous titre)&lt;/h2&gt;\n  &lt;p&gt;Un paragraphe.&lt;/p&gt;\n\n  &lt;h3&gt;Un titre de niveau 3 (un sous-sous titre)&lt;/h3&gt;\n    &lt;p&gt;Etc.&lt;/p&gt;\n\n`\nUltérieurement on pourra définir les propriétés graphiques d’une balise par des CSS. Par exemple avec ceci, les paragraphes seront publiés en caractère bleu.\np{     color: blue; }\nCe qui nous intéresse n’est pas la décoration, mais le fait que les développeurs définissent des balises spécifiques pour chacun des éléments de leurs pages web, et que si nous savons les repérer , nous avons le moyen de mieux lire le texte. Les balises sont la cible du scrapping. Ces dernières peuvent néanmoins être protégées par les développeurs et encapsulées par d’autres langages informatique rendant leur butinage impossible. L’information n’est alors plus contenue dans la balise et le code source d’une page web.\nDe nombreuses ressources sont disponibles, mais pour en rester à R , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages.\nUne application rvest :\nhttps://www.r-bloggers.com/2018/10/first-release-and-update-dates-of-r-packages-statistics/\nLe package rvest est générique :\nhttps://community.rstudio.com/t/scraping-messages-in-forum-using-rvest/27846/2\nVoici un petit exemple sur du contenu d’un forum de VTC, un thread relatif à la question de la vaccination obligatoire.\n\n\nShow the code\nlibrary(rvest)\n\n# Scrape thread titles, thread links, authors and number of views\n\nstart &lt;- \"https://uberzone.fr/threads/si-la-vaccination-devient-obligatoire-vous-feriez-vous-vacciner-ou-changeriez-vous-de-corps-de-metier.17425\"\n\nx&lt;-c(\"/page-2\", \"/page-3\", \"/page-4\")\n\nfor (val in x){\n  url&lt;-paste0(start,val)\n  h &lt;- read_html(url)\n\npost &lt;- h %&gt;%\n  html_nodes(\".bbWrapper\") %&gt;%\n  html_text()%&gt;%\n      str_replace_all(pattern = \"\\t|\\r|\\n\", replacement = \"\")\npost\n#authors &lt;- h %&gt;%\n#  html_nodes(\".username--style2 \") %&gt;%\n#  html_text() %&gt;%\n#  str_replace_all(pattern = \"\\t|\\r|\\n\", replacement = \"\")\n\n# Create master dataset (and scrape messages in each thread in process)\n\nmaster_data &lt;- \n  tibble(post)\nrds_name&lt;-paste0(\"./data/df_\",substr(val,2,6),\".rds\")\nsaveRDS(master_data,rds_name)\n}\n\nhead(master_data)\n\n\n# A tibble: 6 × 1\n  post                                                                          \n  &lt;chr&gt;                                                                         \n1 \"Je comprends pas pourquoi persistez-vous à vouloir convaincre alors que vous…\n2 \"Shibani a dit:Je comprends pas pourquoi persistez-vous à vouloir convaincre …\n3 \"*****\\\"Celui qui ne pète pas et ne rote pas explose\\\"*****\"                  \n4 \"mez a dit:Ta cirrhose et ton  Cancer du poumon (je te les souhaite pas faut …\n5 \"Shibani a dit:Et puis sache que comme je l’ai déjà dit tu peux acheter ton p…\n6 \"*****\\\"Celui qui ne pète pas et ne rote pas explose\\\"*****\"                  \n\n\n\n4.3.1 Des problèmes pratiques, juridiques et éthiques\nLa pratique du scrapping se heurte d’abord à une question technique. Ce n’est pas un excercice facile, et il doit être confier à des spécialistes. Il se heurte aussi à différents problèmes d’ordre éthique et juridique. Si la pratique n’est pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques\nEn termes pratiques, le scrapping crée des risques pour les sites :\n\nLe risque de deny of service, c’est à dire de saturer ou de parasiter un système et de s’exposer à ses contre-mesures, comme par exemple, des protections.\nIl contribue à la complexification du web, et implique une consommation excessive de ressources énergétiques.\n\nEt des risques pour la qualité du recueil de données\n\nLe risque d’information parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. L’exemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier l’information temporelle.\nLe risque matériel de mal lire les informations, pour des raisons d’encodage approximatifs.\n\nEn termes de droits même les conditions légales relèvent de différents droits :\n\nDe la propriété intellectuelle,\nDu respect de la vie privée,\nDu droit de la concurrence qui sans l’interdire, condamne la copie laissant espérer qu’une transformation des données fasse qu’il y échappe.\n\nCependant des facilités et tolérances sont souvent accordées quand c’est dans un objectif de recherche et que des précautions minimales d’anonymisation ou de pseudonymisation sont prises, que les règles de conservation et de destruction des données sont précisées.\nEn termes éthiques\n\nUn principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la société dans son ensemble, hors cette technique participe à la “robotisation” du web (plus de 50% du trafic résulterait de la circulation des spiders , scrapers, sniffers et autres bots, comme dans la forêt une éthique écologique revient à préveler le minimal nécessaire pour l’étude entreprise",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html#limportance-croissante-des-api",
    "href": "q03_corpus_suite.html#limportance-croissante-des-api",
    "title": "4  Corpus : techniques avancées",
    "section": "4.4 L’importance croissante des API",
    "text": "4.4 L’importance croissante des API\nLes API doivent être considérées comme la voie normale d’accès à l’information, du moins en droit. Elles relèvent du contrat. Le recours aux APIs est civilisé, ne serait-ce parce qu’on introduit une sorte d’étiquette, des règles de courtoisie, un système de reconnaissance réciproque et d’attribution de droits.\nSur le plan méthodologique elles présentent l’avantage de donner aux requêtes un caractère reproductible , mêmes si les bases visées peuvent varier. Elles asurent une grande fiabilité des données.\nL’utilisation d’API lève l’ambiguïté légale qui accompagne le scraping et peut ainsi paraître comme plus “civilisée”. Elle nécessite naturellement que le gestionnaire de la base de données fournisse les moyens de s’identifier et de requêter, elle peut avoir l’inconvénient d’être coûteuse quand l’accès est payant, ce qui sera de plus en plus le cas.\n\n4.4.1 Un tour d’horizon\nLa plus part des grandes plateformes offrent des API plus ou moins ouvertes, examinons-en quelques une pour comprendre plus clairement leur intérêt méthodologique. On va se concentrer sur trois exemples : le firehose de tweeter, l’API de google maps, la Crunchbase.\nTwitter n’est pas qu’un réseau social, c’est une gigantesque base de données qui enregistre les engagements et les humeurs de 500 millions d’humains à travers la planète et les centres d’intérêts. Elle permet potentiellement de saisir les opinions à différentes échelles géographiques et temporelles, y compris les plus locales et les plus courtes. Elle a le défaut de souffrir fortement de biais de sélection, le premier étant le biais d’engagement. Les passionnés d’un sujet parlent plus que les autres, d’une parole mieux contrôlée.\nLe cas de Google maps est passionnant à plus d’un égard. le premier d’entre eux est que dans l’effort d’indicer chaque objet de la planête, la base de données devient un référentiel universel, plus qu’une représentation intéressée du monde. Quand l’utilisateur commun cherche un chemin optimal, l’analyste de données y trouve un socle pour ordonner le monde.\nLa Crunchbase construite par le média Techcrunch repertorie les créations de start-up et les levées de fonds qu’elles ont obtenues. Elle recense les dirigeants, les acquisitions, décrit les business model.\nintégrité des bases de données, universalité des élément, interopérabilité, disponibilité\nLes problèmes posés :\n\nJustesse , précision et représentativité. Leur constitution n’est pas aléatoire, leurs couvertures restent partielle.\nAccessibilité, la privatisation du commun. Si pour le chercheur les APIs sont sur un plan de principe une merveille, elles instaurent sur un plan plus social des inégalités d’accès énormes aux données qui permettent de valoriser la connaissance. Ce mécanisme opère via deux canaux. Le premier est celui de la tarification; qui ségrège les chercheurs en fonction des ressources dont ils disposent. Le second passe par la couverture du champ, les données les plus précises et les plus denses se trouvent dans les régions les plus riches.\nDes catégorisations peu délibérées\n\n\n\n4.4.2 un point de vue plus technique\nhttps://www.dataquest.io/blog/r-api-tutorial/\n\n\n4.4.3 Un exemple avec Rtweet\nLes changement de règles de twitter rende l’exemple obsolète, on le garde pour mémoire.\nhttps://cran.r-project.org/web/packages/rtweet/vignettes/intro.html\nPlusieurs packages de R permettent d’interroger le firehose ( la bouche d’incendie!) de twitter.\nhttps://www.rdocumentation.org/packages/rtweet/versions/0.7.0\nL’authentification ne nécessite pas de clé d’API, il suffit d’avoir son compte Twitter ouvert. Cependant la fonction lookup_coords requiert d’avoir une clé d’API ou google cloud map. Elle permet de sélectionner et conditionner l’extraction sur un critère géographique.\nhttps://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api\n\n\nShow the code\n#une boucle pour multiplier les hashtag \n\n#x&lt;-c(\"#getaround\",\"#Uber\", \"#heetch\")\n\n#for (val in x) {\n#  tweets &lt;- search_tweets(val,n=20000,retryonratelimit = TRUE)%&gt;% #geocode = lookup_coords(\"france\")\n#      mutate(search=val)\n#  write_rds(tweets,paste0(\"tweets_\",substring(val,2),\".rds\"))\n#}\n\n#df_blablacar&lt;-readRDS(\"./data/tweets_blablacar.rds\")\n#df_uber&lt;-readRDS(\"./data/tweets_uber.rds\")\n#df_heetch&lt;-readRDS(\"./data/tweets_heetch.rds\")\n\n#df&lt;-rbind(df_blablacar,df_uber )\n\n#ls(df_blablacar)\n\n#foo&lt;-df %&gt;% select(account_lang, geo_coords,country_code, country, account_lang,place_name)\n\n\nOn laisse le lecteur explorer les différentes fonctionnalités du package. On aime cependant celle-ci qui échantillonne le flux courant au taux annoncé de 1%. Voici l’extraction de ce qui se dit en France pendant 10 mn (600s). La procédure peut s’apparenter à une sorte de benchmark auquel on peut comparer une recherche plus spécifique.\n\n\nShow the code\n#rt &lt;- stream_tweets(lookup_coords(\"france\"), timeout = 600)\n\n\n\n\n4.4.4 Un autre exemple\ngoogle map serait bien mais leur API fermée, il en faut une ouverte. discogs .?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q03_corpus_suite.html#conclusion",
    "href": "q03_corpus_suite.html#conclusion",
    "title": "4  Corpus : techniques avancées",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nDans ce chapitre nous aurons égratigné des sujets techniques de constitution de corpus en envisageant différents moyens d’accès\nOn soulignera la technicité croissante et spécifique de chacun ces moyens de collecte.\nOn observera l’étendue des domaines à exploiter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corpus : techniques avancées</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html",
    "href": "q09_Analyse_Cooccurences.html",
    "title": "11  Analyse des co-occurrences",
    "section": "",
    "text": "11.1 Des co-occurences aux similarités\nOn continue avec la boite à outil quanteda\nOn peut naturellement faire varier les critères de cooccurence en définition la fen^rtre commune de deux mots. Au lieu de prendre l’ensemble du texte.\nUn tel tableau est un tableau de similarité, et il sera généralement utile de traiter un tableau de dis-similarités.\nUne",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html#approche-par-classification-hiérarchique",
    "href": "q09_Analyse_Cooccurences.html#approche-par-classification-hiérarchique",
    "title": "11  Analyse des co-occurrences",
    "section": "11.2 Approche par classification hiérarchique",
    "text": "11.2 Approche par classification hiérarchique\nUne approche évidente de ce type d’objet est celle des méthodes de classification hiérarchiques dont il faut rappeler l’origine biologiques. Dans le problèmes de la construction d’un arbre évolutionnaire des espèces ou des variétés, la méthode générale s’appuie sur le calcul de distances, qui peuvent être phénotypiques, ou génomiques.\n\n11.2.1 Un peu d’histoire et de théorie\nsur ce plan On doit beaucoup à sokal et sneath\nhttps://www.amazon.com/Principles-Numerical-Taxonomy-Robert-Sneath/dp/B009SA478O\npour un commentaire, car celà appartient à l’histoire des sciences\nhttps://www.jstor.org/stable/4026976\nCes méthodes généralement différent par deux critère\nle mode de clacul de la distance\nla méthode d’agrgation\n\n\n11.2.2 Un exemple simple\n\n\n11.2.3 Des variantes pertinentes relativement au texte\nles approches descendantes ( et les arbres de décision) rheinhart\ndbscan de bertopic",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html#cartes-de-positionnement",
    "href": "q09_Analyse_Cooccurences.html#cartes-de-positionnement",
    "title": "11  Analyse des co-occurrences",
    "section": "11.3 Cartes de positionnement",
    "text": "11.3 Cartes de positionnement\nLe problème de la représentation des distance entre un certain nombres de point est abordé depuis longtemps par les méthodes d’analyses de similarités, qui cependant n’étaient pas forcément adaptées au traitement d’un grand nombre d’objets. Des méthodes de forces ont été introduites notamment pour résoudre les problème de visualisation. Aujourd’hui deux méthodes ce sont imposée et sont les standards dans les domaines du texte, ou les termes sont très nombreux. Il s’agit de Tsne et d’Umap.\n\n11.3.1 MDS et PCA\n\nTorgerson, Warren S. (1958). Theory & Methods of Scaling. New York: Wiley. ISBN 978-0-89874-722-5.\n\nKruskal, J. B. (1964). “Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”. Psychometrika. 29 (1): 1–27. doi:10.1007/BF02289565. S2CID 48165675.\nMead, A (1992). “Review of the Development of Multidimensional Scaling Methods”. Journal of the Royal Statistical Society. Series D (The Statistician). 41 (1): 27–39. JSTOR 2348634. Abstract. Multidimensional scaling methods are now a common statistical tool in psychophysics and sensory analysis. The development of these methods is charted, from the original research of Torgerson (metric scaling), Shepard and Kruskal (non-metric scaling) through individual differences scaling and the maximum likelihood methods proposed by Ramsay.\nGreen, P. (January 1975). “Marketing applications of MDS: Assessment and outlook”. Journal of Marketing. 39 (1): 24–31. doi:10.2307/1250799. JSTOR 1250799.\n\n\n\n\n11.3.2 les modèles de forces\nattraction répulsion\nune esthétique de la dynamaique\n\n\n11.3.3 tsne\n\n\n11.3.4 UMAP",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html#analyse-de-réseau",
    "href": "q09_Analyse_Cooccurences.html#analyse-de-réseau",
    "title": "11  Analyse des co-occurrences",
    "section": "11.4 Analyse de réseau",
    "text": "11.4 Analyse de réseau\n\n11.4.1 la grammaire des réseaux\n\n\n11.4.2 visualisation\n\n\n11.4.3 Indices de centralités\n\n\n11.4.4 Détection des communauté\n\n\n11.4.5 D’autres manières d’aborder le réseaux\nune approche bi grammes",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html#en-3d",
    "href": "q09_Analyse_Cooccurences.html#en-3d",
    "title": "11  Analyse des co-occurrences",
    "section": "11.5 en 3D",
    "text": "11.5 en 3D\nIl y a une dynamique des réseaux, mais un problème majeur, d’un période à l’autre les configurations peut être très différentes.\nil y a une solution générale. Ensuite ce ne sont que des questions techniques.\n\n11.5.1 Analyse procustéenne\n\n\n11.5.2 Des technique vidéo.\n\n\n11.5.3 Vers des représentation interactives\nthe shiny way\nexploration of sémantic space",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q09_Analyse_Cooccurences.html#conclusion",
    "href": "q09_Analyse_Cooccurences.html#conclusion",
    "title": "11  Analyse des co-occurrences",
    "section": "11.6 Conclusion",
    "text": "11.6 Conclusion\nDans ce chapitre nous avons introduit une large gamme de techniques destinées à mesurer et à représenter les relation entre un ensemble importants de termes. L’échelle va de quelques dizaines à plusieurs milliers. Nous aurons au passage souligné qu’on peut réduire l’analyse sur des registres homogènes de termes\nÉpistémologiquement nous nous appuyons sur l’idée simple que le sens vient de la fréquences des termes associés, c’est l’idée de hirth que nous avons évoqué dans le chapitre intoductif. Et nous pouvons constater que la mise en scène de cette idée c’est largement sophistiquée. On gardera cependant en tête, fort de la connaissance de la nature de la distribution des mots, selon une loi puissance, que ces méthodes ne valent qu’à certaines échelles.\nOn soulignera l’importance de la qualité des visualisation qui permettent à l’analyse d’inférer des histoires. Ces méthodes sont frtement qualitatives , même si les métriques de réseaux et de distances laisse espérer qu’on puisse capturer des propriété déterminantes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q01_introduction.html",
    "href": "q01_introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 De la linguistique à l’approche computationnelle\nOn ne doit pas se faire aveugler par l’éclat d’une apparente nouveauté de ces méthodes. Les techniques d’aujourd’hui dépendent d’idées semées depuis longtemps dans champs de recherche : la linguistique et l’informatique.\nOn peut alors en synthétiser l’idée avec cette figure annotée. elle en exprime deux veines principales. La première, est une tension du champs entre la langue comme structure, quand la seconde considère le langage en tant que capacité et usage.\n![Les domaines de la linguistique](./images/linguistique.jpg)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "q01_introduction.html#de-la-linguistique-à-lapproche-computationnelle",
    "href": "q01_introduction.html#de-la-linguistique-à-lapproche-computationnelle",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1.1 d’abord persuader\nPenser la langue est un effort constant qui a commencé il y a de nombreuses années, certainement avec les sophistes, et l’idée qu’en maniant le langage, il est possible de convaincre, en construisant une logique propre (protagoras, Diogène…). Pour les sophistes : plier le langage à ses intérêt est une première sciences du langage qui témoigne d’une connaissance des dispositifs les plus efficaces. Pas sûr que cette discipline ait trouvé une “episteme” reconnue, mais elle n’en reste pas moins commune et contemporaine : c’est l’art de la publicité. La rhétorique n’est pas une discipline morte, elle se développe de manière concrète dans toute les agences publicitaires.\nDonnons quelques points de repère en commençant par quelques définitions préliminaires, avant de se concentrer sur trois idées essentielles qui vont prospérer avec le développement de la linguistique computationnelle et de l’intelligence artificielle. Ces trois idées sont relatives aux principales branches de la linguistique : à savoir la syntaxe, la sémantique et la pragmatique. Nous resterons ici silencieux sur la phonologie (étude de la formation des sons et de la phonétique) dont l’importance est considérable quand il s’agit de traiter la production et les interactions orales. Pour ne donner qu’un exemple, la prosodie (le rythme données aux phrases) est un objet d’étude essentiel dans les courants de recherches en informatique affective.\n\n\n2.1.2 Langue, langage, texte et parole\nLa langue se définit comme un ensemble de règles plus ou moins formelles que constitue une parole : ce qui se dit de l’un à l’autre ou de l’un aux autres. Le langage est la capacité à produire cette parole. La constitution de cette parole par l’écriture constitue le texte. Le miracle du passage de la parole au signe est celui du symbole. Parmi les distinctions terminologiques proposées par Ferdinand de Saussure au début de siècle dernier, autour de la langue, du langage et de la parole se sont révélées particulièrement pertinentes et restent toujours utilisées de nos jours.\nLe Langage : faculté inhérente et universelle de l’humain de construire des langues (des codes) pour communiquer. (Leclerc 1989:15)\nLe langage réfère à des facultés psychologiques permettant de communiquer à l’aide d’un système de communication quelconque. Le langage est inné.\nLa Langue : système de communication conventionnel particulier. Par « système », il faut comprendre que ce n’est pas seulement une collection d’éléments mais bien un ensemble structuré composé d’éléments et de règles permettant de décrire un comportement régulier (Pensez à la conjugaison de verbes en français par exemple). La langue est acquise.\nLe langage et la langue s’opposent donc par le fait que la langue est la manifestation d’une faculté propre à l’humain, qui n’est autre que le langage.\nLa Parole: une des deux composantes du langage qui consiste en l’utilisation de la langue. La parole est en fait le résultat de l’utilisation de la langue et du langage, et constitue ce qui est produit lorsque l’on communique avec nos pairs.\nLe texte : Il est la transcription de la parole, même si le plus souvent, sa production est directe sans étape intermédiaire de traduction du langage oral.\n\n\n2.1.3 Syntaxe et grammaire générative\nNous nous référerons ici à Chomsky et sa grammaire générative. En dépit de leur très grande diversité, le projet s’appuie sur l’idée qu’un nombre fini de règles doit produire une infinité d’énoncés. Une grammaire est générative dans la mesure où elle possède cette propriété.\nL’analyse est ainsi tournée vers la compétence, et le linguiste s’intéresse à l’idéal qu’un locuteur qui, en connaissant ces règles, serait en mesure de produire une pluralité de discours.\nObservant que les enfants apprennent, enracinant le phénomène linguistique dans la cortalisation du langage, il apporte une idée forte et structuraliste d’une équivalence entre les langues. Sous la lumière de Tesnière et de ses arbres syntaxiques, les treebanks contemporains s’inscrivent dans cette perspective et nourrissent les analyseurs (parser) syntaxiques du langage naturel qui constituent désormais la première couche d’un traitement de données textuelles.La grammaire générative a conduit la linguistique dans un tournant\nformel où la langue est étudiée indépendamment de ses locuteurs. On pourra méditer sur le “pourquoi” des algorithmes génératifs contemporains de deep learning (le fameux GPT3) qui réussissent à former des phrases syntaxiquement correctes mais absurdes.\n\n\n2.1.4 Sémantique : Une conception distributionnelle\nLa tradition lexicologique file le lexique comme une affaire ancienne. Le français est aidé par des institutions fondamentales : le Littré, l’Académie Française et les premiers dictionnaires des éditeurs. Pour étudier un langage il faut se rapporter à des formes stables, les dictionnaires les documentent et renseignent ces normes pour les coder . Un moment clé a été de penser le signe, Saussure apporte alors cette idée fondamentale que dans le symbole, le signe et le signifiant sont les deux faces d’une même monnaie, qu’il existe une relation entre l’artefact et l’idée. En d’autres termes; il est possible qu’un signe particulier puisse signifier une idée : c’est un penseur de la correspondance.\nSelon Saussure, la langue est le résultat d’une convention sociale transmise par la société à l’individu et sur laquelle ce dernier n’a qu’un rôle accessoire. Par opposition, la parole est l’utilisation personnelle de la langue (toutes les variantes personnelles possibles: style, rythme, syntaxe, prononciation, etc.). Le changement de la langue relève d’un individu mais son acceptation relève de la communauté et des institutions. ex.: le verbe « jouer » conjugué «jousent » est pour l’instant à considérer comme une variante individuelle (parole), une exception, et il le demeurera tant qu’il ne sera pas accepté dans la communauté (les locuteurs du français dans ce cas-ci). Sa conception du signe répond à cette approche conventionnelle : la dualité du signe comme signifiant et signifié est opérée.\nDans le traitement des données textuelles le “signifié” est le terme cible de l’analyse, pour en découvrir son signifié on se tourne vers son contexte : l’ensemble des signifiés. C’est une idée ancienne qu’a proposé Firth dans les années 30. Firth (1957) construisant ainsi la genèse du paradigme distributionnel. Un mot trouve son sens dans ceux qui lui sont le plus associés. C’est, dans cette veine, le contexte qui donne alors le sens. Cette idée va être retrouvée avec la notion fondamentale des embeddings.\nL’idée de quantifier le langage n’est pas particulièrement innovante, et ce, moins encore s’il s’agît de compter les mots et leurs co-occurences. Le premier a adopté cette méthodes est certainement Zipf, le père ce cette loi fameuse qui réduit le produit de la fréquence des mots et de leur rang à une constance, étudiant empiriquement cette distribution d’homère à James Joyce de 1934 à 1949 ( On lira avec bonjeur la synthèse de Bully). Derrière la régularité statistique il avance un argument celui du locuteur qui tend à utilisé le mois de mots et les plus simple, et celui de l’interlocuteur qui en a besoin de plus précis pour décoder le message.\nUn vaste mouvement s’est formé dans les années soixante autour de la lexicologue, stimulé par l’école française de l’analyse de données ( benzecri). Le descendant de ce mouvement se retrouve dans l’excellent iramuteq de l’équipe de Toulouse, précédé par le fameux Alceste, et maintenant durablement intégré dans le package R Rainette.\nNous y consacrerons un chapitre complet sur le plan technique. Il reste important de souligner que cette école française de l’analyse textuelle ne se limite pas au comptage d’entités. Un logiciel comme trope qui d’ailleurs ne connait aucun équivalent dans l’écosystème que nous allons explorer, manifeste aussi cette inventivité, où s’exprime pleinement la logique distributionnelle.\n\n\n2.1.5 L’approche pragmatique : les fonctions et acte du langage\nSi la grammaire générative se tourne délibérément plutôt vers la compétence et ignore la performance, c’est à dire la production d’énoncés par les humains en situation d’interaction plus que sur les effets de l’énoncé lui-même, un autre courant de la linguistique s’est emparé de la question, le courant dît de la pragmatique du langage.\nLe grand classique de ce courant est la théorie des fonctions du langage, qui sous-tendent la production d’un message : l’acte de parole, proposée par Jakobson. Inspiré par la cybernétique, la structure de son modèle est celle d’un acte de communication. Jackobson identifie les éléments de l’évènement discursif (speech event) et les fonctions qui lui sont associées. Pour le paraphraser, un LOCUTEUR envoie un MESSAGE à un ou plusieurs INTERLOCUTEUR(S) qui, afin d’être compris, requiert un CONTEXTE dont les acteurs de l’évènement discursif sont capables de saisir et de verbaliser, ce qui suppose l’existence d’un CODE au moins partiellement commun et d’un CONTACT, canal physique ainsi que d’une connection psychologique. On listera alors, au sens du théoricien (jakobson_linguistics_1981?) à lire [ici](https://pure.mpg.de/rest/items/item_2350615/component/file_2350614/content)\n\nLa fonction référentielle ou représentative (aussi dénommée sémiotique ou symbolique), où l’énoncé donne l’état des choses , où\nle message dénote un contexte. Jakobson emploie aussi les termes de dénotatif ou cognitif.\nLa fonction expressive (émotive), où le sujet exprime son attitude propre à l’égard de ce dont il parle.\nLa fonction conative, lorsque l’énoncé vise à agir sur le destinataire : elle s’exprime grammaticalement par l’impératif ou le vocatif.\nLa fonction phatique, empruntée à Malinoswki ( ref) où l’énoncé révèle les liens ou maintient les contacts entre le locuteur et l’interlocuteur.\nLa fonction métalinguistique ou métacommunicative, qui fait référence au code linguistique lui-même, qu’il soit théorisé ou internalisé par le locuteur, comme la prose de Monsieur Jourdain.\nLa fonction poétique, lorsque l’énoncé est doté d’une valeur en tant que tel, valeur apportant un pouvoir créateur et dont Jakobson illustre avec l’exemple de la jeune fille qui a l’habitude de désigner Harry par “Horrible Harry” sans pouvoir expliquer pourquoi\n\nil ne serait pas l’odieux, le dégoûtant, ou le terrible Harry, alors que sans s’en rendre compte, elle emploie une paronomasie/alliteration : la ressemblance phonologique et prosodique des mots produit un puissant effet poétique. John Langshaw Austin s’intéressant à la fonction conative développe le concept d’acte de langage, introduisant l’idée fondamentale que les actes de langage (la production d’un énoncé) ne sont pas uniquement destinés à décrire le monde tel qu’il est, mais bien à agir sur le monde par le biais du locuteur et du destinataire. Parler devient alors également, faire. La théorie des actes de langage est d’abord une catégorisation des différents actes. Il distingue trois types de réalisations s’opérant au travers du langage :\nLe locutoire : cette dimension du langage est réalisée à partir du moment où un énoncé, est juste grammaticalement, dans les règles de la langue dans laquelle il est émis. Prononcer à table, la phrase :\n“Est-ce-qu’il y a du sel ?”, est une construction langagière correcte, et se réalise dans sa première dimension. Cependant, dans la théorie linguistique de John Langshaw Austin, le message convoyé par un énoncé va au-delà de son sens immédiat, et s’intègre dans une seconde fonction.\nL’illocutoire : L’exemple précédent n’a pas, du seul fait de sa formulation, uniquement pour fonction de s’informer sur la présence de sel dans la maison (ou dans le plat, contenu locutoire de l’énoncé). Il exprime plutôt que l’on voudrait saler son plat (fonction illocutoire) et se traduit généralement par le fait que l’un des convives réagisse, par exemple en passant la salière au locuteur. Ce faisant, le langage performe un troisième niveau de discours, qu’Austin nomme la dimension perlocutoire..\nLe perlocutoire : Cette dimension finale se conjugue donc avec les deux précédentes, mais son produit n’est pas commutatif, dans le sens où les actions et interprétations sujettes de notre énoncé 1 dépendent desfonctions plus basses de ces derniers, et s’enracinent également dans le contexte de ceux-ci et d’éléments plus ou moins extra langagiers. Cette idée est au cœur des sous-bassements de la théorie des actes de langage d’Austin, qui se détache donc fortement d’un langage uniquement communicatif, au détriment d’une vision de ce dernier comme un outil plus performatif que descriptif. Dire devient alors faire, car le langage agit et transforme l’univers des interlocuteurs.\n\n\n2.1.6 le texte\nLa lanque étudiée en-soi, ne prête guère l’attention à ses support, elle est abstraite et sa matérialisation langagière se traduit par une parole. L’énoncé que le linguistique dissèque est une parole, ce qui sors de la bouche du locuteur.\nMais depuis quelques livres fondateurs et la récolution de l’imprimerie, l’expression de la langue et du langage s’est logé dans le texte. Des convention de caractères, de lettres, de ponctuations et des règles de grammaire et de typographie. La langue se socialise, s’instituitionnalise.\nLes textes ne sont pas isolés, ils se répondent l’un à l’autre. La note de bas de page, la référence bibliographique,\nGenette et l’intertextualité, le palimpseste. c’est une question de sens, le sens d’un texte vient de ses prédécesseurs et de ceux à qui ils se réfèrent. Les auteurs au travers des textes se répondent l’un l’autre, et ce n’est pas dans leur contenu qu’on trouvera une vérité mais dans le rapport qu’ils établissent avec leurs prédécesseurs, par l’appareil des notes et des bibliographies, des références et mises en perspectives. Cette approche vient questionner l’apparente et rassurante téléologie naturelle que chacun est tenté de voir, dans la remise en continuité d’éléments qui sont alors détachés de leurs contextes de production.\n\n\n2.1.7 La narrativité .\nL’acte de parole se réalise dans un lieu à un moment, avec des protagonistes, dans une atmosphère, avec une histoire, les mots qui s’en échappent ne sont que des traces, autant que des photographies. Ces données se sédimentent dans les grands bassins du cloud hybride et dans les corpus constitués historiquement et méthodiquement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "q01_introduction.html#linguistique-computationnelle",
    "href": "q01_introduction.html#linguistique-computationnelle",
    "title": "2  Introduction",
    "section": "2.2 Linguistique computationnelle",
    "text": "2.2 Linguistique computationnelle\nLes points de contact entre linguistique et informatique se produisent en rapport à diverses questions pratiques, portées sur le traitement et la computation d’éléments langagiers recensés selon des sources tant orales qu’écrites, pour diverses finalités opérationnelles.(traductions, analyses, transcriptions, synthèses…)\nLes apports de la fouille de données les nomenclatures\nune convergence nécessaire\nLe monde des bibliothèques et celui de la GED.\n\n2.2.1 Les facteurs de développement de l’usage en sciences sociales\nCes développements sont favorisés par un environnement fertile où trois facteurs se renforcent mutuellement. Ils conduisent à l’élaboration de nouvelles méthodes.\n\nLa naissance et généralisation de langues informatiques universelles\nL’émergence de vastes ensembles de données textuelles\nLa naissance d’une communauté épistémique, de pratique et de\n\n\n2.2.1.1 Une lingua franca\nLe premier facteur de développement est l’expansion de la programmation orientée objet (POO). Plus spécifiquement dans le cas de la manipulation des données, deux langages de programmation se distinguent particulièrement, dans un usage proprement statistique pour R et plus généraliste en ce qui concerne Python. Le propre de ces langages est, prenons le cas de R, de permettre d’accéder à des interfaces et fonctions mathématiques, dont un ensemble cohérent pour réaliser certaines tâches peut être rassemblé dans une bibliothèque appelée “package” ou “librairie”. Ces bibliothèques de fonctions se chargent en mémoire facilement via la commande R suivante : library(nomdupackage). On dispose désormais de milliers de packages (17 788 sur le CRAN) destinés à résoudre un nombre incalculable de tâches. Une petite représentation ci-dessous témoigne de l’évolution exponentielle des outils mis à dispositions de la communauté R :\n![hornik](./Images/number_CRAN_packages.png)\nDévelopper et concevoir le code d’une analyse revient ainsi à jouer avec un immense jeu de briques, similaires aux Lego de notre enfance, dont de nombreuses pièces bas niveau sont déjà pré-moulées. D’un point de vue pratique, les lignes d’écritures sont fortement simplifiées, permettant à un chercheur non spécialisé en programmation d’effectuer simplement des opérations complexes. En retour, cette facilitation de l’analyse abonde le stock de solutions.\n\n\n2.2.1.2 La multiplication des sources de données.\nLe second facteur d’évolution est la multiplication des sources de données et leur facilité d’accès.\n\nLe contenu écrit des réseaux sociaux,\n\nL’acte de parole se réalise dans un lieu à un moment, avec des protagonistes, dans une atmosphère, avec une histoire, les mots qui s’en échappent ne sont que des traces, autant que des photographies. Ces données se sédimentent dans les grands bassins du cloud hybride et dans les corpus constitués historiquement et méthodiquement. Les rapports d’activités des entreprises,\n\nLes compte-rendus archivés de réunion,\nLes avis des consommateurs sur les catalogues de produits,\nLes articles et les revues scientifiques,\nLes livres numériques…\n\nLes sources les plus évidentes sont proposées par les bases d’articles de presse telles que presseurop ou factiva. Les bases de données bibliographiques sont dans la même veine particulièrement intéressantes et pensées pour ces usages.\nLes données privées, et en particulier celles des réseaux sociaux, même si un péage doit être payé pour accéder aux plateformes via différentes APIs, popularisent le traitement de données massives. Les forums et sites d’avis de consommateurs sont pour les sociologues de la consommation et les spécialistes du comportement de consommation une ressource directe et précieuse.\nLe mouvement des données ouvertes (open data) proposent et facilitent l’accès à des milliers de corpus de données : grand débat, EuropeanSurvey…\n\n\n2.2.1.3 Une communauté\nLe troisième facteur de développement , intimement lié au premier, est la constitution d’une large communauté de développeurs et d’utilisateurs qui se retrouvent aujourd’hui dans des plateformes diverses. Le savoir, autrement dit des codes commentés se trouvent dans une variété importante\nde lieux :\n\nDes plateformes de dépôts telles que Github, qui rassemblent une trentaine de millions de développeurs et data scientists.\nDes plateformes de Q&A (question et réponses) telles que Stalk OverFlow,\nDes tutoriaux de toute sortes : cours, vidéos et autres Mooc\nDes blogs ou des fédérations de blogs (BloggeR),\nDes revues (Journal of Statistical Software) et de bookdown.\n\nDes ressources abondantes sont ainsi disponibles et facilitent l’auto-formation des chercheurs et des data scientists, en proposant des ressources pour la résolution de leurs problèmes pratiques. Quiconque n’arrive pas à résoudre un problème a une bonne chance de trouver la solution d’un autre, à un degré de circonstances près. Elles sont d’autant plus utiles que certaines règles ou conventions s’imposent progressivement pour fluidifier l’échange et les projets individuels :La principale démarche est alors celle de l’exemple reproductible.\nLa seconde est le maintien d’une éthique du partage qui encourage à partager le code, et dont une littérature importante étudie l’effet positif sur les performances économiques et la durabilité [rauter]. Les\nexternalités de réseaux y sont fortes.\nToutes les conditions sont réunies pour engendrer une effervescence créative. Python ou R, sont dans cet univers en rapide expansion, les langues véhiculaires qui favorise une innovation constante. Les\nstatistiques de Github en témoigne : près de 50 millions d’utilisateurs, 128 millions de “repositories” et 23 millions de propriétaires. ![source](./Images/github.png)\nvoir aussi &lt;https://towardsdatascience.com/githubs-path-to-128m-public-repositories-f6f656ab56b1&gt;\n\n\n\n2.2.2 De nouvelles méthodologies pour les sciences sociales\nPour les chercheurs en sciences sociales, et donc nécessairement, pour les chercheurs en sciences de gestion, lieu de rencontre entre toutes les sciences sociales, cette révolution textuelle offre de nouvelles opportunités d’obtenir et d’analyser des données solides pour vérifier leur hypothèses et mener leurs enquêtes. Ce sont de nouveaux terrains, de nouvelles méthodes et un nouvel objet de recherche qui se dessine dans le développement du champ scientfique contemporain.\n\n2.2.2.1 Nouveaux terrains :\nLa multiplication des sources de données précitées, associées à leur progressive normalisation, permet une prolifération de techniques provenant de multiples courants disciplinaires, convergeant toutes vers un langage commun. En ce sens, la production abondante d’avis de consommateurs, de discours de dirigeants, de compte-rendus de conseils et colloques, d’articles techniques, de travaux en linguistique computationnelle, de diverses fouilles de données, des moteurs de recommandation, de la traduction automatique, offre des ressources nouvelles et précieuses pour traiter l’abondance des données générées.\n\n\n2.2.2.2 Nouvelles méthodes :\nUn nouveau paradigme méthodologique se construit à la croisée de données abondantes et de techniques intelligentes de traitement . Il permet d’aller plus loin que l’analyse lexicale traditionnelle en incorporant des éléments syntaxiques, sémantiques, et pragmatiques, proposés par l’ensemble des outils de traitement du langage naturel. Il se dessine surtout une nouvelle approche méthodologique qui prend place entre l’analyse qualitative, et les traditionnelles enquêtes par questionnaires capables de traiter des corpus d’une taille inédite. Le travail de Humphreys and Wang (2018) en donne une première synthèse dans le cadre d’un processus qui s’articule autour de 6 différentes\nphases d’une recherche :\n\nLa formulation de la question de recherche\nLa définition des construits,\nLa récolte des données\nL’opérationnalisation des construits\nL’interprétation et l’analyse,\nLa validation des résultats obtenus.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "q01_introduction.html#un-nouvel-objet",
    "href": "q01_introduction.html#un-nouvel-objet",
    "title": "2  Introduction",
    "section": "2.3 Un nouvel objet :",
    "text": "2.3 Un nouvel objet :\nOn pourrait croire qu’avec des données massives et des techniques “intelligentes” nous assistons à un retour du positivisme qui bénéficierait enfin des instruments de mesures et de calculs ayant permis à certains chercheurs au plus proche de la matière des succès majeurs. Sans nul doute, l’administration de la preuve va être faciliter par ces techniques et va encourager l’evidence based policy (REF) afin de résoudre en partie la crise de la réplication et de la reproductibilité des travaux de recherche.\nCependant, à mesure que se développe l’appareillage de méthode et de données, moins l’on peut supposer que l’observateur reste neutre. En effet, ni les télescopes géants, ni les synchrotrons, n’affectent les galaxies lointaines ou les atomes proches. Le propre des données que l’on est amené à étudier est de résulter de la confrontation d’un système d’observation (certains préfèrent alors parler de surveillance), à un agent, doué de buts, d’une connaissance, de biais, et de ressources. Le dispositif de mesure est en lui-même performatif.\nL’exemple le plus évident est celui des systèmes de notation, qui sous\nprétexte de transparence donne la distribution des répondants\nprécédents. L’agent qui va noter choisit la valeur en fonction d’une\nnorme apparente - la note majoritaire- et de sa propre intention - se manifester ou se confondre à la foule. Pour se donner une idée plus précise de ce mouvement, examinons quelques\npublications récentes dans les champs qui nous concernent.\n\n2.3.1 Sociologie et histoire\nclasses sociales avec word to vec en sociologie\nKozlowski, Taddy, and Evans (2019)\nL’article révolution française\nOn citera cependant jean-baptiste Coulmont et son obstination à étudier les entités nommées, prénoms et autres marqueurs culturels de l’identité et des classes.et au luxembourg\n\n\n2.3.2 Psychologie\nTrès tôt la psychogie s’est intéressée au langage, pas seulement comme produit des processus psychologiques, mais comme expression de ceux-ci.Dès les années 1960 dans le champ de la psychologie de l’éducation, douéd’une forte motivation positiviste, s’est posée la question de la mesurede la difficulté d’un texte pour un niveau d’éducation donné. La mesure de la lisibilité des textes s’est alors développée, profitant à d’autresecteurs tels que ceux de la propagande. Dans cette même perspective,l’approche scientifique de la richesse lexicographique comme concept représentant les compétences a à son tour développé de nouvelles instrumentations.\nJames W. Pennebaker a développé son approche à partir de l’étude des traumas; donnant une grande importance à la production discursive des patients. Sa contribution majeure est l’établissement d’un ensemble de dictionnaires destinés à mesurer des caractéristiques du discours. Un instrument qu’on présentera dans le chapitre 7 (à vérifier) Tausczik and Pennebaker (2010). Son approche se poursuit en psychiatrie avec l’analyse des troubles du langage, et a connu un coup d’éclat avec la démonstration que l’analyse des messages sur les réseaux sociaux comme facebook permet de détecter des risques de dépression.@eichstaedt_facebook_2018.\n\n\n2.3.3 Management\nLa finance et l’analyse du sentiment\nDans le champ du management, on trouvera des synthèses pour la recherche\nen éthique Lock and Seele (2015), en comportement du consommateur\nHumphreys and Wang (2018) en management public\nAnastasopoulos, Moldogaziev, and Scott (2017) ou en organisation\nKobayashi et al. (2018)\n\n\n2.3.4 Economie\néconomie des brevets intervention des institutions mesure de l’innovation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "q09_token_to_dfm.html",
    "href": "q09_token_to_dfm.html",
    "title": "11  Des tokens à leurs co-occurences.",
    "section": "",
    "text": "11.1 des tokens au DFM",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Des tokens à leurs co-occurences.</span>"
    ]
  },
  {
    "objectID": "q09_token_to_dfm.html#des-tokens-au-dfm",
    "href": "q09_token_to_dfm.html#des-tokens-au-dfm",
    "title": "11  Des tokens à leurs co-occurences.",
    "section": "",
    "text": "11.1.1 La construction du tableau\n\n\n11.1.2 Son l’analyse par la vénérable AFC\n\n\n11.1.3 Pondérer par la rareté parmi les documents",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Des tokens à leurs co-occurences.</span>"
    ]
  },
  {
    "objectID": "q09_token_to_dfm.html#calculer-les-coocurrences",
    "href": "q09_token_to_dfm.html#calculer-les-coocurrences",
    "title": "11  Des tokens à leurs co-occurences.",
    "section": "11.2 Calculer les coocurrences",
    "text": "11.2 Calculer les coocurrences\n\n11.2.1 Un simple exemple\n\n\n11.2.2 plus compliqué\n\n\n11.2.3 ou des distances",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Des tokens à leurs co-occurences.</span>"
    ]
  },
  {
    "objectID": "q09_token_to_dfm.html#références",
    "href": "q09_token_to_dfm.html#références",
    "title": "11  Des tokens à leurs co-occurences.",
    "section": "11.3 Références",
    "text": "11.3 Références",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Des tokens à leurs co-occurences.</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html",
    "href": "q10_Analyse_Cooccurences.html",
    "title": "12  Analyse des co-occurrences",
    "section": "",
    "text": "12.1 Des co-occurences aux similarités\nOn continue avec la boite à outil quanteda\nOn peut naturellement faire varier les critères de cooccurence en définition la fen^rtre commune de deux mots. Au lieu de prendre l’ensemble du texte.\nUn tel tableau est un tableau de similarité, et il sera généralement utile de traiter un tableau de dis-similarités.\nUne",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html#approche-par-classification-hiérarchique",
    "href": "q10_Analyse_Cooccurences.html#approche-par-classification-hiérarchique",
    "title": "12  Analyse des co-occurrences",
    "section": "12.2 Approche par classification hiérarchique",
    "text": "12.2 Approche par classification hiérarchique\nUne approche évidente de ce type d’objet est celle des méthodes de classification hiérarchiques dont il faut rappeler l’origine biologiques. Dans le problèmes de la construction d’un arbre évolutionnaire des espèces ou des variétés, la méthode générale s’appuie sur le calcul de distances, qui peuvent être phénotypiques, ou génomiques.\n\n12.2.1 Un peu d’histoire et de théorie\nsur ce plan On doit beaucoup à sokal et sneath\nhttps://www.amazon.com/Principles-Numerical-Taxonomy-Robert-Sneath/dp/B009SA478O\npour un commentaire, car celà appartient à l’histoire des sciences\nhttps://www.jstor.org/stable/4026976\nCes méthodes généralement différent par deux critère\nle mode de clacul de la distance\nla méthode d’agrgation\n\n\n12.2.2 Un exemple simple\n\n\n12.2.3 Des variantes pertinentes relativement au texte\nles approches descendantes ( et les arbres de décision) rheinhart\ndbscan de bertopic",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html#cartes-de-positionnement",
    "href": "q10_Analyse_Cooccurences.html#cartes-de-positionnement",
    "title": "12  Analyse des co-occurrences",
    "section": "12.3 Cartes de positionnement",
    "text": "12.3 Cartes de positionnement\nLe problème de la représentation des distance entre un certain nombres de point est abordé depuis longtemps par les méthodes d’analyses de similarités, qui cependant n’étaient pas forcément adaptées au traitement d’un grand nombre d’objets. Des méthodes de forces ont été introduites notamment pour résoudre les problème de visualisation. Aujourd’hui deux méthodes ce sont imposée et sont les standards dans les domaines du texte, ou les termes sont très nombreux. Il s’agit de Tsne et d’Umap.\n\n12.3.1 MDS et PCA\n\nTorgerson, Warren S. (1958). Theory & Methods of Scaling. New York: Wiley. ISBN 978-0-89874-722-5.\n\nKruskal, J. B. (1964). “Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”. Psychometrika. 29 (1): 1–27. doi:10.1007/BF02289565. S2CID 48165675.\nMead, A (1992). “Review of the Development of Multidimensional Scaling Methods”. Journal of the Royal Statistical Society. Series D (The Statistician). 41 (1): 27–39. JSTOR 2348634. Abstract. Multidimensional scaling methods are now a common statistical tool in psychophysics and sensory analysis. The development of these methods is charted, from the original research of Torgerson (metric scaling), Shepard and Kruskal (non-metric scaling) through individual differences scaling and the maximum likelihood methods proposed by Ramsay.\nGreen, P. (January 1975). “Marketing applications of MDS: Assessment and outlook”. Journal of Marketing. 39 (1): 24–31. doi:10.2307/1250799. JSTOR 1250799.\n\n\n\n\n12.3.2 les modèles de forces\nattraction répulsion\nune esthétique de la dynamaique\n\n\n12.3.3 tsne\n\n\n12.3.4 UMAP",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html#analyse-de-réseau",
    "href": "q10_Analyse_Cooccurences.html#analyse-de-réseau",
    "title": "12  Analyse des co-occurrences",
    "section": "12.4 Analyse de réseau",
    "text": "12.4 Analyse de réseau\n\n12.4.1 la grammaire des réseaux\n\n\n12.4.2 visualisation\n\n\n12.4.3 Indices de centralités\n\n\n12.4.4 Détection des communauté\n\n\n12.4.5 D’autres manières d’aborder le réseaux\nune approche bi grammes",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html#en-3d",
    "href": "q10_Analyse_Cooccurences.html#en-3d",
    "title": "12  Analyse des co-occurrences",
    "section": "12.5 en 3D",
    "text": "12.5 en 3D\nIl y a une dynamique des réseaux, mais un problème majeur, d’un période à l’autre les configurations peut être très différentes.\nil y a une solution générale. Ensuite ce ne sont que des questions techniques.\n\n12.5.1 Analyse procustéenne\n\n\n12.5.2 Des technique vidéo.\n\n\n12.5.3 Vers des représentation interactives\nthe shiny way\nexploration of sémantic space",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q10_Analyse_Cooccurences.html#conclusion",
    "href": "q10_Analyse_Cooccurences.html#conclusion",
    "title": "12  Analyse des co-occurrences",
    "section": "12.6 Conclusion",
    "text": "12.6 Conclusion\nDans ce chapitre nous avons introduit une large gamme de techniques destinées à mesurer et à représenter les relation entre un ensemble importants de termes. L’échelle va de quelques dizaines à plusieurs milliers. Nous aurons au passage souligné qu’on peut réduire l’analyse sur des registres homogènes de termes\nÉpistémologiquement nous nous appuyons sur l’idée simple que le sens vient de la fréquences des termes associés, c’est l’idée de hirth que nous avons évoqué dans le chapitre intoductif. Et nous pouvons constater que la mise en scène de cette idée c’est largement sophistiquée. On gardera cependant en tête, fort de la connaissance de la nature de la distribution des mots, selon une loi puissance, que ces méthodes ne valent qu’à certaines échelles.\nOn soulignera l’importance de la qualité des visualisation qui permettent à l’analyse d’inférer des histoires. Ces méthodes sont frtement qualitatives , même si les métriques de réseaux et de distances laisse espérer qu’on puisse capturer des propriété déterminantes.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analyse des co-occurrences</span>"
    ]
  },
  {
    "objectID": "q11_Topic_Model.html",
    "href": "q11_Topic_Model.html",
    "title": "13  Modèles de Topic",
    "section": "",
    "text": "13.1 LSA et les autres\nL’analyse des classes latentes ? est-ce nécessaire ?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modèles de Topic</span>"
    ]
  },
  {
    "objectID": "q11_Topic_Model.html#le-modèle-fondateur-lda",
    "href": "q11_Topic_Model.html#le-modèle-fondateur-lda",
    "title": "13  Modèles de Topic",
    "section": "13.2 Le modèle fondateur : LDA",
    "text": "13.2 Le modèle fondateur : LDA\nsur ce plan On doit beaucoup à Blei, Ng, and Jordan (2003)\n\n13.2.1 la struture du modèle\nil appuie sur une représentation statistique où les mots se distribuent selon une loi de probabilité particulière en occurrence la loi de Dirichlet. loi de Dirichletla loi de Dirichlet, souvent notée Dir(α), est une famille de lois de probabilité continues pour des variables aléatoires multinomiales.Ex. probabilités d’un mot parmi les trois milles d’usage courant\n\n\n13.2.2 mise en oeuvre",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modèles de Topic</span>"
    ]
  },
  {
    "objectID": "q11_Topic_Model.html#une-variante-utile-le-modèle-stm",
    "href": "q11_Topic_Model.html#une-variante-utile-le-modèle-stm",
    "title": "13  Modèles de Topic",
    "section": "13.3 Une variante utile : le modèle STM",
    "text": "13.3 Une variante utile : le modèle STM",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modèles de Topic</span>"
    ]
  },
  {
    "objectID": "q11_Topic_Model.html#conclusion",
    "href": "q11_Topic_Model.html#conclusion",
    "title": "13  Modèles de Topic",
    "section": "13.4 Conclusion",
    "text": "13.4 Conclusion\n\n\n\n\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” J. Mach. Learn. Res. 3 (March): 993–1022. http://dl.acm.org/citation.cfm?id=944919.944937.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modèles de Topic</span>"
    ]
  }
]