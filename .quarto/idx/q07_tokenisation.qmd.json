{"title":"Jouer avec les jetons","markdown":{"headingText":"Jouer avec les jetons","containsRefs":false,"markdown":"\n```{r 600}\n#les librairies du chapître\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(tokenizers)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(flextable)\n\ntheme_set(theme_minimal()) \n\nset_flextable_defaults(\n  font.size = 10, theme_fun = theme_vanilla,\n  padding = 6,\n  background.color = \"#EFEFEF\")\n```\n\n**Objectifs du chapitre :**\n\n<div>\n\n*L'analyse du texte commence par découper les chaines de caractères en unités pertinentes, c'est souvent le mot, ce peut être la syllabe, la phrase, ou la lettre. Ces unités sont des jetons, les pièces élémentaire d'un traitement plus sophistiqué. cette opération permet de quantifier le texte et de le représenter sous la forme de dfm.*\n\n</div>\n\nL'étape initiale de toute analyse textuelle est de découper le texte en unités d'analyse, les *tokens*, ou jetons en français, ce qui transforme le texte écrit pour la compréhension humaine en données interprétables par l'ordinateur. Les *tokens* utilisés peuvent varier selon les objectifs de l'analyse et la nature du corpus, la granularité peut être plus ou moins fine. Les *tokens* peuvent ainsi être :\n\n-   des lettres : c'est l'unité insécable.\n-   des syllabes : ça permet de s'intéresser aux phonèmes.mais aussi d'extraire d'un mot les suffixes et préfixes, ainsi que les radicaux ( la racine du mot, ex : dés-espéré-ment).\n-   des mots : il s'agit du niveau le plus évident et le plus courant, que l'on privilégiera tout au long de ce livre, il présente aussi des difficultés car il ne coincide par avec l'unité de sens, par exemple avec les locutions: \" Président de la République\", ni avec les lexiques établis, ils peuvent prendre souvent la caractéristiques de [mots fantômes](http://stella.atilf.fr/MotsFantomes/)\n-   des phrases : c'est l'unité de langage, lui correspond un argument, une proposition ; l'usage du point suivi d'un espace et d'une majuscule est assez général pour les identifier.\n-   des paragraphes : c'est une unité plus générale, qui souvent développe une idée.\n-   des sections, des chapitres, ou des livres : selon la nature des documents, cela permet de découper le corpus en sous-unités.\n\nLes *tokenizers* sont les outils indispensables à cette tâche. Dans cet ouvrage, nous nous concentrons sur l'étude des mots. Lors de cette étude, un certain nombre de mots apparaissent de nombreuses fois, pour permettre de donner du sens au langage humain, mais ils ne portent pas en eux d'informations particulièrement pertinentes pour l'analyse : ce sont les *stopwords*, qu'il conviendra souvent d'éliminer.\n\nLes n-grammes, quant à eux, représentent des suites de n *tokens*. Un unigramme est donc équivalent à un *token*, un digramme[^q07_tokenisation-1] est une suite de deux *tokens*, etc. L'identification des n-grammes permet de détecter des suites de *tokens* qui reviennent plus souvent que leur probabilité d'occurrences. Si l'on se concentre sur les mots, nous sommes alors face à une unité sémantique, comme on le comprend facilement avec le digramme 'Assemblée Nationale' dont le sens est plus que ses constituants.\n\n[^q07_tokenisation-1]: En français digramme est la meilleure traduction de bigram, une bigramme en français est un mot dont les lettres peuvent former deux autres mot.\n\nTokeniser revient donc à découper le texte pour en construire une représentation quantitative.C'est d'ailleurs une des spécificités des grands modèles de langage qui s'intréesse moins aux mots, qu'à ses décompositions, telles qu'un nombre limité( ~30 000) permette de générer la plupart des graphies et au passage de supporter les dysgraphies. \n\n## Quelques exercices de tokenization\n\n### Les lettres\n\nCommençons par un exemple simple, à l'aide d'une courte citation de Max Weber. On choisit les lettres pour unité de découpe, et l'on utilise le package ['tokenizer'](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html). Automatiquement, 'tokenizer' met le texte en minuscule et élimine la ponctuation\n\n```{r 601}\n\n#Les données\nMaxWeber <- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.\")\n\n#On tokenise, plus on transforme en dataframe le résultat.\ntoc_maxweber<-tokenize_characters(MaxWeber)%>%\n        as.data.frame()%>%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo<-toc_maxweber %>% \n        group_by(tokens)%>% \n        summarise(n=n())%>%\n        filter(n>0)\n\n#On représente par un diagramme en barre cette distribution des occuences d'apparition, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=10, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+\n        labs(title = \"Fréquence des tokens, unité = lettres\", \n             x=\"tokens\", \n             y=\"nombre d'occurences\", \n             caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n```\n\n### les mots\n\n```{r 602}\n#Les données\n\nMaxWeber <- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains. La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de l’intérêt général.\")\n\n#On tokenise, plus on transforme en dataframe le résultat. le strp_punc écarte la ponctuation du processus.\ntoc_maxweber<-tokenize_words(MaxWeber,strip_punct=TRUE)%>%\n        as.data.frame()%>%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo<-toc_maxweber %>%mutate(n=1) %>% \n        group_by(tokens)%>% \n        summarise(n=sum(n))\n\n#On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=4, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"Fréquence des tokens, unité = mots\", x=\"tokens\", y=\"nombre d'occurences\", caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n\n```\n\nOn peut également constater que certains mots sont proches, par exemple les deux derniers sur le graphiques précédents qui sont des déclinaisons du verbe appliquer. Il peut alors être pertinent de regrouper ces différentes formes verbales (comme un mot au singulier et au pluriel, au féminin et au masculin, ou conjugué sous différentes formes), pour faciliter l'analyse. C'est ce qu'on fait avec les opérations de *stemming* ou de lemmatisation, présentées au chapitre 8.\n\n### Les phrases\n\nOn reproduit les mêmes opérations, mais cette fois sur les phrases de l'exemple précédent.\n\n```{r}\ntokenize_sentences(MaxWeber)%>%\n  as.data.frame()%>%\n  rename(tokens=1)%>%\n  flextable(cwidth = 6)\n\n```\n\n## N-grammes\n\nLes n-grammes sont des séquences de n *tokens*, généralement consécutifs. Sur la d'un base d'un corpus important on peut calculer la probabilité d'apparition d'un n-gramme. C'est l'exercice auquel une équipe de google s'est attelé avec le [Books Ngram Viewer](https://books.google.com/ngrams/). et dont nous encourageons l'étude détaillée.\n\nL'article original doit aussi être lu. Sur la base de 5 millions de livres numérisés en 2011 et représentant 4% du nombre total de livres à jamais publiés à ce moment, être lu aussi, il montre que, pour l'anglais, chaque année 8500 mots entrent dans le lexique, sans que les dictionnaires, pour des raisons évidentes de concision, n'en font l'inventaire. Le webster comprend 300 000 mots, alors que pour l'anglais l'estimation du nombre de termes différents, est passé de l'ordre de 500 000 en 1900, à plus d'1.5 million en 2000. Il donne aussi une dimension au lexique : celle de sa densité. Dans un corpus, à un moment donné quelle est la probabilité d'observer un token donné (au plus simple un mot). Sur un corpus de 500 milliards de termes (pour toutes les langues traitées), un mot qui apparaît une fois sur 1000 ( 10\\^-3), apparaît 5 millions de fois, s'il apparaît une fois sur 1 million, il apparaît 5000 fois. L'échelle de mesure est ainsi la fréquence : 10-3, 10-5, 10-6 ....\n\nLa notion de ngram au-delà de son apparence de tautologie possède aussi un intérêt théorique majeure. Les ngram sont des chaines de markov. Intérêt théorique des n gram : ce sont des chaines de markov.\n\n[Processus de markov](https://fr.wikipedia.org/wiki/N-gramme)\n\nPour comprendre l'importance de ce concept on peut considérer au moins 3 applications remarquables\n\nApplication à l'auto-complétion . Son efficacité dépend naturellement de la taille des données récoltées. On comprend que pour google il est aisé d'être précis dans l'estimation de ces probabilités!\n\nMais mieux encore, ces propriété markovienne ( probabilistique) permettent aussi la correction d'erreur avec l'algorithme de Viterbi https://fr.wikipedia.org/wiki/Algorithme_de_Viterbi\n\n-   Le principe de 'textcat' est fondée sur ces n-grammes de lettre. Chaque langue se caractérise par une distribution particulière des n-grammes. Pour décider de l'appartenance d'un texte à une langue, si on dispose des profils de distribution, on compare la distribution des n-grammes du texte à ces références. On peut ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche.\n\n### Mise en oeuvre\n\nSi la notion est simple, sa mise en oeuvre l'est presque autant. Nous l'illustrons avec une séries d'exemples et le package \"tokenizer\" qui a ici un avantage pédagogique. On étudiera plus loin les ressources de \"quanteda\".\n\nOn commence de suite par un exemple sur les lettres. c'est de pure forme. On sélectionne les ngrammes (k\\>1 et k\\>4) dont la fréquence est supérieure à trois.\n\n```{r}\n#tokenization des lettres\ntoc_maxweber<-tokenize_character_shingles(MaxWeber,n=3, n_min=2) %>%\n        as.data.frame()%>%\n  rename(tokens=1)\n\nflextable(head(toc_maxweber, n=20))\n\nfoo<-toc_maxweber %>%mutate(n=1) %>% \n        group_by(tokens)%>% \n        summarise(n=sum(n))%>%\n  filter(n>3)\n\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n  annotate(\"text\", x=5,y=11, label=paste(\"nombre total de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"digrammes et trigrammes des lettres\", x=\"n-gramme\", y=\"nombre d'occurences\")\n\n```\n\nOn peut faire la même chose sur les mots, et en plus en éliminant les *stopwords*. l'intérêt de la procédure est de se concentrer sur les idées, des paires de mots consistants du point de vue sémantique. Un ordre ce dégage ! la bureaucratie est un moyen, puis une rationalité qui dépend d'un exercice.\n\nOn note qu'il n'y a pas besoin de beaucoup de mots pour produire du sens par l'analyse statistique de leurs distributions.\n\n```{r}\ntoc_maxweber<-tokenize_ngrams(MaxWeber,\n                              n=3, \n                              n_min=2, \n                              stopwords = stopwords('fr')) %>%\n  as.data.frame()%>%\n  rename(tokens=1)\n\nqflextable(head(toc_maxweber, n=19))\n\n```\n\nOn peut également s'intéresser aux n-grammes non directement consécutifs mais séparés par k *tokens*. C'est sans doute un moyen de saisir des corrélations de mots à plus grande distance. La possibilité formelle est là , nous avouons ne pas savoir à quel usage elle correspond. Nous serions sur ce point ravi d'avoir des réponses.\n\n```{r }\ntoc_maxweber<-tokenize_skip_ngrams(MaxWeber,\n                                   n=3, \n                                   n_min=2, \n                                   k=2, \n                                   stopwords = stopwords('fr')) %>%\n        as.data.frame()%>%rename(tokens=1)\nqflextable(head(toc_maxweber, n=19))\n\n```\n\nDans cet exemple, aucun n-gramme n'est répété, mais c'est rarement le cas avec des corpus plus importants. Dans ce cas, une forte répétition de n-grammes est un indice d'une unité sémantique composée de plusieurs *tokens* que l'on peut alors regrouper en un seul et même *token*. C'est ce que l'on verra dans la section suivante, avec la méthodes des collocation,\n\n## Choisir des n-grammes pertinents\n\nDans ce e-book l'unité principale d'analyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur d'un mot, une valeur sémantique, par exemple, l'expression \"Assemblée Nationale\". Ces deux mots réunis constituent un syntagme, une unité de sens. La question qui se pose est alors de savoir comment les identifier dans le flot des n-grammes ?\n\nLa technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités d'apparition laisse espérer, c'est qu'ils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non.\n\nLe package quanteda propose une bonne solution à ce problème avec la fonction collocation.\n\n### Créer les *tokens* avec 'quanteda'\n\nÀ partir du corpus des commentaires de TripAdvisor concernant les hôtels de Polynésie Française,\n\n```{r 608}\n#| tbl-cap: corpus\n\n#les données\nAvisTripadvisor<-read_rds(\"./data/AvisTripadvisor.rds\")\nAvisTripadvisor$Taille_hotel<-as.character(AvisTripadvisor$Taille_hotel)\nAvisTripadvisor$Taille_hotel[is.na(AvisTripadvisor$Taille_hotel)]<-\"Autre\"\n\n\n#création du corpus\ncorpus<-corpus(AvisTripadvisor,docid_field = \"ID\",text_field = \"Commetaire\", docvars =AvisTripadvisor)\nft<- head(corpus,3) %>% as.data.frame()%>%\n  rename(tokens=1)%>%\n  flextable(cwidth = 6)\nft\n```\n\nOn crée un objet de format *token*, avec la fonction tokens de quanteda, et on choisit d'enlèver la ponctuation, les symboles et les nombres avec les arguments correspondants.\n\nNotre tokenizer ici a quelques problèmes . Si le \".\" n'est pas suivi d'un espace, il ne peut distinguer les mots, il ne saisit pas non plus l'ellipse de \"l'équipe\", qui devrait distinguer le determinant \"la\" de \"équipe. Souvent il faudra au préalable remanier les chaines de caractère pour une meilleure qualité de tokenisation.\n\n```{r 608c, fig.cap='Mots les plus fréquents du corpus'}\n#| tbl-cap: token simple\n\n\n#transformation en objet token\ntok<-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)\n\nhead(tok,3) \n\n```\n\nAu passage, on peut aussi enlever les *stopwords*. C'est a dire les mots d'usages courants mais sans signification intrinsèque : les conjonctions, les déterminants, etc, qui se présentent sous la formes de dictionnaires.\n\nPour que les n-grammes très fréquents restent des syntagmes signifiants, on laisse apparentes les positions des *stopwords*, avec l'option `padding= TRUE`.\n\n```{r 608b, fig.cap='Mots les plus fréquents du corpus'}\n#| tbl-cap: token with stopwords\n\n#enlever les stopwords\ntok<-tokens_remove(tok,stopwords('fr'),padding=FALSE)\nhead(tok, 3)\n\n```\n\n### Application à la détection des entités nommées\n\nOn cherche ici à identifier les noms propres présents dans le corpus.\n\n```{r}\nlibrary(quanteda.textstats)\n#on sélectionne les mots commençant par une majuscule\ntoks_cap <- tokens_select(tok, \n                               pattern = \"^[A-Z]\",\n                               valuetype = \"regex\",\n                               case_insensitive = FALSE, \n                               padding = TRUE)\n\n#on cherche les collocations\ntstat_col_cap <- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE)\n\nflextable(head(as.data.frame(tstat_col_cap)))\n\n```\n\n### Composer des *tokens* à partir d'expressions multi-mots : collocation\n\nDans ce corpus, les noms propres correspondent aux noms des îles et des hôtels, et aux prénoms composés. La valeur du lambda montre la force de l'association entre les mots, on retiendra d'une manière générale un lambda au moins supérieur à 3 pour remplacer les *tokens* d'origine par leurs n-grammes.\n\n```{r 611 }\ntoks_comp <- tokens_compound(tok, pattern = tstat_col_cap[tstat_col_cap$z > 3,], \n                             case_insensitive = FALSE)\n\nhead(toks_comp)\n\n```\n\n### Identifier les autres concepts\n\nDans ce corpus, on peut aussi s'attendre à voir apparaître d'autres expressions multi-mots qui représentent des concepts, telles que \"petit déjeuner\".\n\n```{r}\ncol<-textstat_collocations(toks_comp, min_count = 10)\n\nflextable(head(as.data.frame(col)))\n\n```\n\nAu vue de la diversité des collocations, on choisit un lambda supérieur à 7 pour retenir les concepts les plus pertinents.\n\n```{r 812}\ntoks_comp <- tokens_compound(tok, pattern = col[col$z > 7,])\n\nhead(toks_comp)\n\n\n```\n\n## Des tokens au dtm\n\nD'un point de vue pratique le nombre de colonne est de l'ordre de plusieurs milliers. Dans le cas de texte courts, quelques dizaines, ou même centaine de mots, moins de quelques pour-cent des lignes auront une valeur. C'est un tableau creux (sparse matrix) qui peut nécessiter des représentations particulières et plus économes.\n\n```{r 609, fig.cap='Mots les plus fréquents du corpus'}\n#on transforme en document-features matrix pour des représentations graphiques \n\ndfm<-dfm(toks_comp,remove_padding=TRUE) \n\ndfm <- dfm_remove(dfm, stopwords(\"fr\"))\n\ndfm_top <- topfeatures(dfm, n = 80, decreasing = TRUE,  scheme =\"count\")\n\ndfm_top\n\n\n\n#un nuage de mots rapide library(quanteda.textplots) \n\ntextplot_wordcloud(dfm, max_word=80, color = rev(RColorBrewer::brewer.pal(6, \"RdBu\")))\n\ndfm2<-dfm %>% dfm_subset(!is.na(Taille_hotel))%>%\n  dfm_group(groups = Taille_hotel)\n\n\ntextplot_wordcloud(dfm2, comparison = TRUE, max_words = 200,\n                   color = c(\"blue\", \"red\"))\n\n\n```\n\n## La question des mots distinctifs\n\nPlus que comparer la fréquence des mots on peux souhaiter identifier ceux qui sont les plus discrimants, les plus distinctifs. \n\n### Keyness index\n\n\n### Find keywords based on the Textrank algorithm\n\nLe Textrank est un algorithme implémenté dans le paquetage R textrank. Cet algorithme permet de résumer un texte et d'en extraire des mots-clés. Pour ce faire, il construit un réseau de mots en vérifiant si les mots se suivent. Sur ce réseau, l'algorithme \"Google Pagerank\" est appliqué pour extraire les mots pertinents, après quoi les mots pertinents qui se suivent sont combinés pour obtenir des mots-clés. Dans l'exemple ci-dessous, nous souhaitons trouver des mots-clés à l'aide de cet algorithme pour les noms ou les adjectifs qui se suivent. Le graphique ci-dessous montre que les mots-clés combinent les mots pour former des expressions à plusieurs mots.\n\n### Rake \n\nL'algorithme de base suivant s'appelle RAKE, acronyme de Rapid Automatic Keyword Extraction (extraction automatique rapide de mots clés). Il recherche des mots-clés en examinant une séquence contiguë de mots qui ne contient pas de mots non pertinents. En d'autres termes, il\n\nen calculant un score pour chaque mot faisant partie d'un mot-clé candidat.\nparmi les mots des mots-clés candidats, l'algorithme examine le nombre d'occurrences de chaque mot et le nombre de cooccurrences avec d'autres mots. Chaque mot obtient un score qui est le rapport entre le degré du mot (le nombre de fois qu'il cooccurre avec d'autres mots) et la fréquence du mot. un score RAKE pour le mot-clé candidat complet est calculé en additionnant les scores de chacun des mots qui définissent le mot-clé candidat.\n\n\n## Conclusion\n\nDans ce chapitre, nous avons vu comment découper un corpus en unités, les *tokens*. Nous avons abordé le sujet des n-grammes, et vu comment composer des *tokens* à partir de concepts multi-mots, identifiés par des n-grammes adjacents.\n\nOn conclue sur l'idée que la distribution des termes d'un texte doit se comprendre dans deux dimensions : la fréquence des termes parmi tous les termes, leur fréquence à travers les documents.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"q07_tokenisation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","bibliography":["references.bib"],"editor":"visual","theme":"cosmo","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"q07_tokenisation.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}