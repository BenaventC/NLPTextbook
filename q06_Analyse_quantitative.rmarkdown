# Analyse Quantitative du corpus


```{r 600}
#les librairies du chapître
library(tidyverse)
#accessoires de ggplot
library(ggridges)
library(ggrepel)
library(ggwordcloud)

theme_set(theme_minimal()) 

#NLP
library(tokenizers)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats )
#tableau
library(flextable)
set_flextable_defaults(
  font.size = 10, theme_fun = theme_vanilla,
  padding = 6,
  background.color = "#EFEFEF")

#palettes

library(wesanderson)
#names(wes_palettes)
col_1<-c("#85D4E3")
col_1b<-c("#F4B5BD")
col_2<-c <-c("#85D4E3", "#F4B5BD")
col_4<-c("#85D4E3", "#F4B5BD", "#9C964A", "#CDC08C")

```


**Objectifs du chapitre :**

<div>

-   Avant de se plonger sans l'analyse du lexique on peut étudier le corpus de manière quantitative d'autant plus s'il se distribue dans le temps. La fréquence des textes, leur longueur, la longueur des mots, leur diversité.

</div>

Une première manière d'aborder un texte ou un corpus est volumétrique. Quel volume de texte? Quelle longueur ? Combien de mots ? Quelles variations?

A cette fin on utilise le cas des tweets de Donald Trump. Des premiers aux derniers, jusqu'au moment de son bannissement en Janvier 2021, après sa défaite aux élections présidentielles. Chargeons le fichier de données.( on retrouvera la description du fichier en préambule)


```{r 601}

df <- read_csv("./data/TrumpTwitterArchive01-08-2021.csv")
nrow<-nrow(df) #nombre de ligne
ncol<-ncol(df) #nombre de colonne
df$nb_mots<-str_count(df$text, " ")+1 # l'astuce : compter les espaces et ajouter 1, pour compter les mots
sum_mots<-sum(df$nb_mots)             #ON COMPTE LE NOMBRE DE MOTS

```


## Comptons les mots

Il y `r nrow` tweets et `r ncol` variables et `r sum_mots` mots cumulés.

On peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de `stringr`, un package précieux que nous allons étudier de plus dans le chapitres suivant : `str_count`.

On note immédiatement la bi-modalité de la distribution qui correspond au changement de règle par Twitter en matière de non de caractères utilisés, étendu de 180 à 280.


```{r 602}

ggplot(df, aes(x=nb_mots))+
  geom_histogram(fill=col_1, binwidth = 10)+
  labs(title=paste0("Nombre total de mots du corpus : ",sum_mots), 
       x="Nombre de mots par post", 
       y="Fréquence")

```


La bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de l'album de Joy Division : un graphique en crêtes (ridges plot) avec [`ggridges`](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).

Le résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à l'occasion d'autre contenu en 50 mots environ.


```{r 603}

df$Year<-format(df$date, format = "%Y") #on extrait l'année de la date
foo<- df %>% 
  filter(Year!="2021") #parce qu'il n'y a quelques quelques jours en janvier, le compte a été interrompu vers le 5 janavier

ggplot(foo,aes(x = nb_mots, y = Year, group = Year)) +
  geom_density_ridges(scale = 4, color="grey90",fill=col_1, alpha=.7)+
  theme_ridges() +
  scale_x_continuous(limits = c(1, 60)) +
  labs(x="Nombre de mots par post",
       y=NULL)
  
```


## La production dans le temps

Examinons le nombre de tweets produit au cours du temps.

On se rappellera qu'après une carrière immobilière menée dans les casinos, le golf et les hôtels, l'appétit médiatique de Trump s'est réalisé dans "the apprentice", de 2004 à 2015. C'est un pro de la TV, il a une formation de Popstar. Il sera élu en Décembre 2016 pour prendre le pouvoir en Janvier.


```{r 604}

df$date2<-format(df$date, "%Y-%m-%d")
df$date3<-as.POSIXct(df$date2, format = "%Y-%m-%d")
df$isRetweet<-as.character(df$isRetweet)

foo<-df %>% filter(Year!=2021)%>%
  group_by(date3, isRetweet)%>%
  summarise(n=n())

## plot time series of tweets

ggplot(foo, aes(x=date3, y=n,group=isRetweet))+
  geom_line(aes(color=isRetweet), alpha=.2)+
  geom_smooth(aes(color=isRetweet),span=0.5)+
    scale_x_datetime(date_breaks = "1 year", labels = scales::label_date_short())+
  labs(y="nombre de tweets par jour", x=NULL)+
  scale_color_manual(values = col_2)

#raf : labeliser avec les dates clés
```


## Popularité des tweets

Dans ce set de donnée si l'auteur est unique, la réception est multiple, rappelons que près de 90 millions de personnes suivaient Trump. On possède deux indicateurs : le nombre de retweet et de RT.


```{r 605}

foo<-df %>% select(Year, favorites, retweets)%>%
  filter(!is.na(favorites) & Year!=2021)%>%
  group_by(Year)%>%
  summarize(sum_rt=sum(retweets),
            sum_fav=sum(favorites),
            mean_rt=mean(retweets),
            mean_fav=mean(favorites)
            ) %>%
  pivot_longer(-Year,names_to = "variable", values_to = "values")

ggplot(foo, aes(x=Year, y=values,group=variable))+
  geom_line(aes(color=variable), size=1,alpha=.8)+
  labs(y="nombre de tweets par jour", x=NULL)+
  scale_color_manual(values = col_4)+ 
  facet_wrap(vars(variable), ncol=2, scale="free")+
  scale_y_log10()


```


On s'aperçoit d'un changement de régime, quand l'audience est limitée RT et fac sont fortement corrélés, l'entrée en politique de Trump se caractérise par un changement de régime. La décorrélation peut s'expliquer par l'usage d'agent d'influences, qui retweetent plus qu'ils n'approuvent. Ils contribuent à la décorrélation.


```{r 606}

foo<-df %>% select(Year, favorites, retweets)%>%
  filter(!is.na(favorites) & Year!=2021)%>%
  group_by(Year)%>%
  summarize(cor(favorites, retweets)) %>%
  rename(correlation =2)
            
ggplot(foo, aes(x=Year, y=correlation, group=1))+
  geom_line(size=1,alpha=.8, color=col_1b)+
  geom_smooth(color=col_1b, size=1.5, fill="grey90")+
  labs(y="Corrélation entre fav et rt", x=NULL)

```

```{r 607}

foo<-df %>% select(Year, favorites, retweets)%>%
  filter(!is.na(favorites) & Year==2020)


m <- ggplot(foo, aes(x = favorites, y = retweets)) +
 geom_point() +
scale_x_log10()+
scale_y_log10()

# contour lines
m + geom_density_2d()


```

```{r 608}


df$date2<-format(df$date, "%Y-%m-%d")
df$date3<-as.POSIXct(df$date2, format = "%Y-%m-%d")
df$isRetweet<-as.character(df$isRetweet)

foo<-df %>% filter(Year!=2021)%>%
  group_by(date3, isRetweet)%>%
  summarise(n=n())

## plot time series of tweets

ggplot(foo, aes(x=date3, y=n,group=isRetweet))+
  geom_line(aes(color=isRetweet), alpha=.2)+
  geom_smooth(aes(color=isRetweet),span=0.5)+
    scale_x_datetime(date_breaks = "1 year", labels = scales::label_date_short())+
  labs(y="nombre de tweets par jour", x=NULL)+
  scale_color_manual(values = col_2)

#raf : labeliser avec les dates clés
```


## Lisibilité et complexité lexicale

Pour aller un peu plus loin - nous savons désormais que Trump aime une forme courte en 21 mots, et que son expérience de twitter est longue, on peut s'intéresser à des paramètres clés relatifs aux conditions de la réception: les textes sont-ils aisés à lire ? sont-ils sophistiqués ?

Introduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. Ce sont des classiques, les critères initiaux de l'analyse quantitative du texte. Ils sont toujours utiles.

### Les indices de lisibilité

La lisibilité est une notion ancienne tout autant que sa mesure (par exemple @coleman_computer_1975). Elle répond à la question du degré de maîtrise requis pour lire un texte en s'appuyant sur les caractéristiques objectives du texte plutôt que sur sa perception. Il s'agissait donc d'évaluer la complexité d'un texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes, et la complexité des phrases mesurée par le nombre de mots.

A partir de ces deux paramètres, une multitudes d'indicateurs ont été proposés. Dans l'exemple qui va suivre, on se contente d'un grand classique, le plus ancien, l'indice de Flesch [@flesch_new_1948] et de ses constituants : le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase.

On les retrouve, avec une autre grande variété, dans le package compagnon de quanteda , [`quanteda.textstats`](https://quanteda.io/reference/textstat_readability.html) , qui en fournit des dizaines de variantes.


```{r 609}
#on sélectionne les tweets originaux

foo<-df %>% filter(isRetweet==FALSE) # on ne prend pas en compte les RT

#la fonction de calcul de lisibilité
readability<-textstat_readability(foo$text, 
                                  measure = c("Flesch","meanSentenceLength", "meanWordSyllables"),
                                  min_sentence_length = 3,
                                  max_sentence_length = 1000) 
# on joint les données
foo<-cbind(foo,readability[,2:4])

#on agrège par année
foo1<-foo %>% filter(Year!=2021) %>%
  group_by(Year) %>%
  summarise(all_tweet=n(),
            Flesch=mean(Flesch, na.rm=TRUE), 
            SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
            WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %>%
  gather(variable, value, -Year)

#visualisation

ggplot(foo1,aes(x=Year, y=value, group=variable))+
  geom_line(size=1.2, aes(color=variable), stat="identity")+
  facet_wrap(vars(variable), scale="free", ncol=1)+
  labs(title = "Evolution de la lisibilité des tweets de Trump", x=NULL, y=NULL)
```


Pour aider le lecteur à donner un sens, voici l'abaque proposée par [Flesch](http://www.appstate.edu/~steelekm/classes/psy2664/Flesch.htm) lui-même.

![Flesch](./image/ReadabilityFlesch.JPG)

On peut aussi prendre pour références les éléments suivants:

"All Plain English examples in this book score at least 60. Here are the scores of some reading materials I've tested. These are average scores of random samples." ( source ?)

-   Comics 92

-   Consumer ads in magazines 82

-   Reader's Digest 65

-   Time 52

-   Wall Street Journal 43

-   Harvard Business Review 43

-   Harvard Law Review 32

-   Auto insurance policy 10

Trump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Le Reader's Digest est beaucoup plus simple, il se situe au dessus de la Harvard Business Review !

### Les indices de complexité lexicale

La complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, l'indicateur marque plus cette variété que les variations de complexité lexicale.[@tweedie_how_1998]

Dans notre univers trumpesque, ce n'est pas trop sensible, d'autant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweet, une autre approche pourrait être de concaténer l'ensemble des tweets d'une période (un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvre l'ensemble des sujets d'intérêt de Trump, que les tweets fractionnent nécessairement. Ce qui en en question dans la mise en pratique n'est pas seulement la question du choix de l'indice mais aussi la définition de l'unité de calcul. La diversité lexicale concerne sans doute plus le discours que la phrase.

On choisit de ne travailler sur deux des multiples indicateurs disponibles :

-   le CTTR de Caroll qui rapporte le nombre de mots distincts ( V) sur le nombre de mots exprimés. Avec ce critère la diversité maximale est obtenue quand le nombre de mots différents est égal au nombre de mots exprimés.

$$
CTTR = \frac{V}{\sqrt{2N}}
$$

-   le Mass supposé être moins sensible à la longueur des textes. (voir Torruella et Capsada 2013 ou )

$$
M = \frac{log(n) - log(t)}{log² (n)}
$$

On notera que le problème de la longueur variables des textes dans un corpus produit un biais : les textes seront mécaniquement plus divers que les textes court, ce qui conduit à des approches segmentées, où la mesure de diversité est une moyenne des moyenne pour chacun des segments de texte . On emploie ici le MATTR, dont le MA signifie moyenne mobile (moving average), et le TTR le token/type ratio.

(attention un pb de log dans le calcul)


```{r 610}
#on retient les tweets de plus de 5 mots
foo<-df %>%
  filter(nb_mots>10 & isRetweet=="FALSE" & Year!=2021)

#la fonction de calcul de diversité
t1=Sys.time()
lexdiv<-tokens(foo$text)%>%
  textstat_lexdiv(foo$text, measure = c("CTTR", "Maas"),  log.base = 10,
                  remove_numbers = TRUE,  
                  remove_punct = TRUE,  
                  remove_symbols = TRUE,
                  remove_hyphens = TRUE) 
t2=Sys.time()
t<- t2-t1
t
#On combine les données et on aggrège sur l'année
foo<-cbind(foo,lexdiv[,2:3])
foo1<-foo %>% 
  group_by(Year) %>%
  summarise(CTTR=mean(CTTR, na.rm=TRUE), 
            Maas=mean(Maas, na.rm=TRUE)) %>%
  gather(variable, value, -Year)

ggplot(foo1,aes(x=Year, y=value, group=variable))+
  geom_line(size=1.2, aes(color=variable), stat="identity")+
  facet_wrap(vars(variable), scale="free", ncol=1)+
  labs(title = "Evolution de la diversité lexicale des tweets de Trump", x=NULL, y=NULL)
```


## La mesure de la concentration des termes

Le langage d'un point de vue quantitatif a été caractérisé depuis bien longtemps et il est caractérisé par la loi de Zipf distribution 1949 . Estout est un des premiers à avoir proposé la loi, mais un sténographe fameux en aurait mis en évidence empiriquement avant 1916.

### Un débat théorique

Celle-ci s'exprime de la manière suivante : si on classe les termes par ordre de fréquence r, le produit de leur rang par la fréquence est égale à une constante. Autrement dit dans une métrique log, la relation entre rang et fréquence est linéaire et sa pente est négative.

$$f(r).{r}={K} $$ Mandelbrot la généralise

les études empiriques

On peut encore débattre de sa signification. Une économie cognitive? Une conséquence de la théorie de l'information ? Un signature pour identifier un discours \## Les lois de distribution du langage

Les lois puissances qui déterminent cet univers et marque dans le registre de l'analyse quantitative une évolution sérieuse. Alors que le modèle dominant de la statistique est la distribution gaussienne, le traitement du langage doit se familiariser avec des distributions puissance.

La régularité statistique nécessite une interprétation, il n'y en a pas qu'une

-   la loi du moindre de effort qui va aboutir à un débat entre Mandelbrot et Simon.

-   Economie de l'information, un mot puis deux, puis trois, puis des mots spécifique à un certain registre d'action. Le sens est une affaire d'échelle, du concret au général. Le plus concret est celui des mots scientifiques telles qu'on les retrouve en biologie pour décrire une espèce, en droit pour décrire une propriété, ou en médecine pour désigner une pathologie ou un élément d'anatomie. ( espace de connaissance - épistémologique)

-   la distribution des locuteurs et de leurs élocution. Dans le langage vernaculaire celui de nos sociabilités, peu de mots sont employés, à mesure qu'on les enregistre ils prennent une place plus importance d'un point de de vue statistique dans les corpus. Les mots rares sont employés par peu de personne. (espace des population - démographique)

### Application

Examinons plus concrètement

On reprend la procédure de comptage des mots par groupe, mais sans filtrer sur la fréquence de ces mots. On en obtient `n_w` mots distincts. En examinant avec plus de détail il y a 5 élément d'information : a ) le trait (feature), c'est à dire ici le mot étudié, b) le nombre de fois où il apparaît dans le corpus, c)le rank qui lui correspond, d)le nombre de documents dans lesquels il apparaît.

Le tableau nous donne les 10 premiers. Il y a 28000 éléments, dont une simple inspection montrera qu'il sont des liens ou d'autres mentions. Il est nécessaire de retraiter le corpus pour éliminer ces éléments. Nous étudierons comment faire dans les chapitres suivants. A ce stade on se contente d'enlever les *apax*[^q06_analyse_quantitative-1] dont on ne peut véritablement calculer un rang ( ils ont tous le dernier). Il nous reste 10854 termes.

[^q06_analyse_quantitative-1]: les apax sont les termes qui n'apparaissent qu'une seule fois dans un corpus.


```{r 611}
#| tbl-cap: Mots les plus fréquents
foo<-df %>% 
  filter(isRetweet==FALSE) %>%
  filter( Year %in% c("2016","2017","2018","2019","2020"))# on ne prend pas en compte les RT

toks<- tokens(foo$text) %>% 
  dfm(remove_punct = TRUE,  remove = stopwords("english"))
freq_g <- textstat_frequency(toks)%>%
  as.data.frame() %>%
  filter(feature!="amp") %>%
  filter(frequency>1)

flextable(head(freq_g, 15))

```


On peut aussi représenter cela graphiquement. La loi de zipf s'observe jusqu'au 500 premiers mots, ensuite elle suit une autre pente. C'est assez caractéristique, la loi de zipf est incomplète, d'un point de vue empirique il semble qu'il y ait deux lois qui se superposes, la loi des mots communs, et celle des mots singuliers. C'est du moins hypothèse que nous proposons.


```{r 612}
ggplot(freq_g, aes(x=rank, y=frequency))+
  geom_point(size=.1)+
  scale_x_log10()+
  scale_y_log10() + 
  geom_smooth(method="gam", color=col_1)

foo<-freq_g %>%filter(rank <500)
ggplot(foo, aes(x=rank, y=frequency))+
  geom_point(size=.1)+
  scale_x_log10()+
  scale_y_log10() + 
  geom_smooth(method="lm", color=col_1)

```


Pour être plus qualitatif Une première manière de représenter la diversité du corpus est de représenter les mots les plus fréquents, selon cette fréquente et le ratio fréquence par document. Plus ce dernier est élevé plus il fait du mot un mot spécifique.


```{r 613}

foo<- freq_g %>%
  filter(frequency>150 &frequency<1500 )

ggplot(foo, aes(x=docfreq, y= frequency/docfreq))+
  geom_text_repel(aes(label=feature), size=2, max.overlaps = 30)+
  scale_x_log10()+
  scale_y_log10()
```


## Comptons les mots

Il est temps de compter les mots, chacun d'entre eux, de se faire une idée une idée de leurs fréquences, de leur distribution.

Souvent on éliminera ceux qui apparaissent de manière occasionnelle, mais aussi ceux qui apparaissent systématiquement dans tous les textes. Une fois ces deux filtrages effectués, le lexique est généralement de l'ordre de 500 à 10000 mots.

Deux outils sont disponibles: les nuages de mots et les lollyplots. Les premiers donnent une idée immédiates, les seconds se prêtent mieux à une analyse systématique

### Les nuages de mots

Ils sont devenus extrêmement populaires même si l'effet esthétique est plus important que leur utilité analytique.

[`ggwordcloud`](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html)

Pour l'application on prépare les données avec quanteda : on tokenise et on construit le dfm (pour le détail voir chapitre tokenization), ce qui nous permets notamment d'éliminer la ponctuation et les mots courants (articles, déterminant etc) qui apportent peu de signification.


```{r 614}

foo<-df %>% filter(isRetweet==FALSE) %>%
  filter( Year %in% c("2016","2017","2018","2019","2020"))# on ne prend pas en compte les RT

toks<- tokens(foo$text) %>% 
  dfm(remove_punct = TRUE,  remove = stopwords("english"))

docvars(toks,"Year")<-foo$Year

#on se concentre du les termes utilisés 300 fois.  

foo<-toks %>% 
    dfm_trim(min_termfreq = 250, verbose = FALSE)

freq <- textstat_frequency(foo)


ggplot(freq, aes(label = feature)) +
  geom_text_wordcloud(aes(size=frequency, color=rank)) +
  theme_minimal() +  scale_size_area(max_size = 10) + 
  scale_color_gradient(low = col_1, high = col_1b)

ggsave("image/g0.jpg", plot=last_plot(), width = 27, height = 19, units = "cm")


```


Et pour faire des comparaisons, entre l'année 2016 qui le conduit à être élu, 2018 une année de midterm et 2020 année de sa défaite, on utilise la même procédure mais on distingue un comptage de fréquence de mot par période. Le ggplot est identique aux précédents mais comprend en plus une géométrie "facet_wrap" qui éclate le nuages de mot selon les 3 périodes étudiées.


```{r 615}


foo<-toks %>% 
  dfm_group(groups = Year) %>%
    dfm_trim(min_termfreq = 1, verbose = FALSE)

url_regex <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
  
#pour compter la fréquence des mots par année
freq <- textstat_frequency(foo, group =Year) %>%
  mutate(feature=str_remove_all(feature, url_regex))|>
  filter(!is.na(feature) & frequency>200 )%>%
  filter(feature!="amp")
           

set.seed(42)
library(ggwordcloud)

ggplot(freq, aes(label = feature)) +
  geom_text_wordcloud(aes(size=frequency, color=rank)) +
  theme_minimal()+
  facet_wrap(vars(group)) +  
  scale_size_area(max_size = 8) 


ggsave("./image/g1.jpg", plot=last_plot(), width = 27, height = 19, units = "cm")

```


## Conclusion

Nous aurons appris à

-   Compter le nombre de documents et leurs longueurs
-   Mesurer la complexité du langage
-   Mesurer la diversité de son vocabulaire.
-   A évaluer la concentration des sources
-   A se donner une première idée de la lexicographie

Ces mesures n'ont ne sens que si elles peuvent être l'objet de comparaison :

-   De manière interne la comparaison se fait dans dans le temps et à travers des segments. On s'intéresse moins au niveau, qu'aux différences entre les niveaux.
-   De manière externe elle requiert un étalonnage. Comparer par rapport au français courant, à un niveau de langue soutenu, ou relâché. L'étalonnage revient à caractériser des types de corpus : presses, écriture savante, réseaux sociaux, publications officielles etc. On ne peut que souhaiter que des comparaisons systématiques soient engagées et compilées pour donner des points de repère précis quand on étudie un corpus particulier.

Elles participent à un premier niveau d'analyse du texte, en surface, visant à apprécier la dynamique de sa production, à établir les échelles d'analyse, à repérer les éléments structurels.

Le texte est une matière qui a un poids (le nombre de mot), une variété (le nombre d'expressions), une complexité (les règles qui l'organisent). nous venons de nous doter des premiers outils d'analyse, il est temps de passer à la suite.

