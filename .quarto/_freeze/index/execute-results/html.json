{
  "hash": "6fa65edbd3f39c6f4826aa1a99ca3b6b",
  "result": {
    "engine": "knitr",
    "markdown": "# Préambule\n\n\"Au commencement était le verbe\", et désormais le verbe est partout, non pas l'esprit de Dieu, mais la parole humaine qui s'éteignait avec le vent, la rumeur mais qui désormais, accumule ses traces imprimées de manière systématique, non plus des petits mots passés de main en main, mais l'enregistrement des transactions informatiques.\n\nLe verbe est désormais une copie du monde, moins l'affirmation d'une vision, que la stratification de nos manifestations sociales.\n\nC'est bien sur le fruit d'un développement technique engagés depuis 10 000 ans et dont l'imprimerie est une nouvelle étape. La révolution actuelle réside dans la colonisation de l'espace social, chaque bribes de parole est enregistrée , transcrite, archivée. Le livre n'est plus qu'un îlot dans un océan de texte.\n\nIl y a un défi empirique pour les sciences sociales, la société produit massivement la documentation de son développement. L'exploitation de ces sources est un enjeu majeur pour la psychologique, sociologique, économique, le droit, les sciences de gestion en ouvrant un nouvel accès au données.\n\nA mesure que ces données prolifèrent, des méthodes pour les analyser se sont développées à grande vitesse, moins pour des motivations d'études que pour répondre à des besoins opérationnels. Aujourd'hui, dans le sillage de l'invention des embeddings, nous sommes à l'ère des grands modèles de langages qui ouvrent de nouveaux horizons dans la capacité de produire du texte (par exemple des descriptions d'images), de transformer le texte (résumé ou traduction), et surtout de l'annoter ( classification, NER...)\n\n## Le but de l'ouvrage\n\nCe e-book est le syllabus d'un cours que nous dispensons sous différents formats et avec différents degrés d'approfondissement. Il fait l'inventaire des méthodes les plus courantes incluant le développement des embeddings et aboutit à l'usage des LLM pour différentes formes d'annotations.\n\nIl a une vocation pratique , des cas, des codes.\n\n## Les contributeurs\n\nOn ne peut (encore) en  produire la liste exhaustives, mais plusieurs générations d'étudiants ont contribué à ce travail en explorant un certain nombre de jeux de données proposés ici.\n\n## Le plan du cours\n\nIl s'organise selon les grandes étapes du processus d'analyse et des types de problèmes posés par le traitement et l'analyse du texte. Ce processus va de l'acquisition des données à leurs représentations en passant par des phases de transformation, mais aussi un ordre de complexité des modèles et des ressources.\n\nIl suit ainsi une sorte de développement historique qui se construit par l'accumulation de différentes philosophies d'analyses et de méthodes et qui peut se présenter en trois grandes périodes:\n\n-   compter les mots et jouer avec leur co-occurences\n\n-   annoter des mots en s'appuyant sur leurs régularités syntaxiques.\n\n-   encoder les mots dans un espace vectoriel, cette perspective a été ouverte par word2vec , étendue avec les transformers, systématisée par les grands modèles de langages tels que gpt4 ou Bloom.\n\nVoici les raisons qui organisent le cours en 20 chapitres (c'est à ajuster au cours de la rédaction)\n\n1.  Préambule (ce que vous êtes en train de lire !)\n\n2.  Chapitre 1 : une introduction générale à des éléments linguistique et technique et sociaux de l'analyse du langage naturel et de ses fulgurantes évolutions au cours de la dernière décennie.\n\n3.  Chapitre 2 : Constituer des corpus. Pour les sciences sociales, le texte est un document qu'on étudie par collection. Il faut aussi penser à la notion de corpus.\n\n4.  Chapitre 4 : corpus - techniques avancées D'un point de vue matériel le corpus est aussi\n\n5.  Chapitre 5 : Explorer et naviguer dans le corpus\n\n6.  Chapitre 6 : Analyse quantitative du corpus\n\n7.  Chapitre 7 : Tokenisation et dtm\n\n8.  Chapitre 8 : Analyse des co-occurences\n\n9.  Chapitre 9 : au début il y avait l'analyse factorielle\n\n10. Chapitre 10 : SVD et LSA :\n\n11. Chapitre 11 : Topic model\n\n12. Chapitre 12 : Supervised modeling\n\n13. Chapitre 13 : word2vec to doc2vec\n\n14. Chapitre 14 : Transformers\n\n15. Chapitre 15 : tasks , Zsl, abstracting\n\n16. chapitre 16 : generative models , un monde à inventer.\n\n\n## Data sets\n\nOn s'appuiera sur des data test \"réel\" et publics\n\n-   [Trump Tweeter Archive](https://www.thetrumparchive.com/) : c'est un site qui rassemble tous les tweets émis par Donald Trump depuis 2010, et propose un outil de navigation.\n\n-   Tripadvisor en Polynésie\n\n-   Airbnb Paris 2019\n\n-   PMP40ans : ce set de données comprend tous les résumés d'articles publiés par la revue Politique et Management Public sur une période de 40 années de sa naissance en 1983 jusqu'en 2023. C'est un observatoire intéressant d'une discipline naissance et de l'évolution des normes professionnelles de la recherche en gestion.\n\n-   yyy\n\n-   zzz\n\n## Packages\n\nCe cours avec les ressources de r, de rstudio et de Quarto. Les apports externes le sont sous forme d'images. Zotero pour la bibliographie.\n\nIl n'y a jamais une solution unique, nous avons fait des choix de méthodes qui se reflètent dans celui des packages dont voici la liste commentée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#environnement de travail\nlibrary(tidyverse) # the perfect tool box.\nlibrary(glossary)\nglossary_path(\"glossary.yml\")\n\n# un exemple pour ajouter des termes au cours de la rédaction\n#glossary_add(term = \"loi de Dirichlet\",\n#             def = \"la loi de Dirichlet, souvent notée Dir(α), est une famille de lois de probabilité continues pour des variables #aléatoires multinomiales.Ex. probabilités d'un mot parmi les trois milles d'usage courant\")\n\n#ses compléments  \nlibrary(ggwordcloud)\n\nlibrary(flextable) # pour produire des tableaux élégants\n\n#ressource nlp  \nlibrary(quanteda) \nlibrary(udpipe)\n\n#analyse de données\nlibrary(FactoMineR)\nlibrary(factoextra)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}