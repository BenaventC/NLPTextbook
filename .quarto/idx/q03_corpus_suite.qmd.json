{"title":"Corpus : techniques avancées","markdown":{"headingText":"Corpus : techniques avancées","containsRefs":false,"markdown":"\n```{r 200}\n#les librairies du chapître\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(pdftools)\nlibrary(flextable)\n\n\ntheme_set(theme_minimal()) \n\nset_flextable_defaults(\n  font.size = 10, theme_fun = theme_vanilla,\n  padding = 6,\n  background.color = \"#EFEFEF\")\n\n```\n\n**Objectifs du chapitre :**\n\n-   <div>\n\n    -   Explorer différentes techniques de collectes de données : exploitation de bases textuelles, méthodes de scrapping, APIs, extraction de document pdf, extraction de textes dans des images, et une perspective oral avec les techniques de speech2tex.\n\n    </div>\n\nLa constitution d'un corpus est la première étape d'un projet NLP. Il se définit d'abord par la constitution d'une collection de textes dont la provenance est la nature peut être diverse.\n\nDans ce chapitre on va examiner plusieurs techniques de collecte, puis conclure avec quelques réflexions sur la question de la constitution de l'échantillon.\n\n-   L'exploitation de bases textuelles\n-   Les méthodes de scrapping\n-   Le recours aux APIs\n-   La collection de documents pas uniquement textuelle\n-   Les sources orales\n\n## La gestion des documents numériques\n\nDans certains cas le matériau se présentera sous forme de documents numériques tels qu'un pdf, ou même de simples images.\n\nvoir aussi\n\n<https://cran.r-project.org/web/packages/fulltext/fulltext.pdf>\n\n### Extraire du texte des pdf\n\nLe package [pdftools](https://ropensci.org/blog/2016/03/01/pdftools-and-jeroen/) est parfaitement adapté à la tâche. Des fonctions simples extraient différents éléments du pdf :\n\n-   Les information relatives au document pdf lui-même\n\n-   La liste des polices employées\n\n-   Les attachements\n\n-   La table des matières (si elle a été encodée)\n\n-   Les chaînes de caractères constituant le texte dans un ordre de droite à gauche et ligne à ligne, reconnaissant cependant les retours chariot, et autres sauts de lignes séparant les paragraphes\n\nChaque page est contenue dans une ligne.\n\n```{r 301}\n\ninfo <- pdf_info(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\ninfo\n\nfonts <- pdf_fonts(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\n\nfiles <- pdf_attachments(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\n\ntoc <- pdf_toc(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\") #il n'y a pas de table des matière dans ce texte\n\ntext <- pdf_text(\"./data/2021neoliberalismegouverner_Meunier_Esprit.pdf\")\ncat(text[[1]]) # pour afficher le texte de la page 1\n\n\n```\n\nIl va falloir traiter ce texte en analysant précisément sa composition. Pour ce faire, il s'agîra de définir une séquence d'opérations logiques qui permette un premier nettoyage du texte. Dans l'exemple nous allons de plus essayer de conserver la structure des paragraphes du texte.\n\n-   Suprimer haut et bas de pages\n-   Supprimer les sauts de ligne\n-   Identifier les sauts de paragraphes\n-   Enlever les notes de bas de page\n-   Corriger l'hyphénation ()\n-   regrouper les document en un seul bloc de texte\n-   le splitter en autant de paragraphes.\n\nOn va utiliser des fonctions de traitement de chaines de caractère avec Stringret le recours à l'art ( ici simple) des regex auxquels on consacre un développement dans le chapitres X.\n\n```{r 302, fig.cap='', out.width='80%', fig.asp=1, fig.align='center', fig.width=9}\ntex<- as.data.frame(text)\ntex[1,]\nt_reg<-str_replace(tex$text,\"[\\\\s+].*Meunier[\\n]+\", \" \") # entete droite\n## on selectionne tout bloc de texte qui commence par un nombre indéterminée de blanc qui s'achève par n'importe quel caractère répétés mais terimé par la séquence Meunier suivie de sauts de ligne.\nt_reg<-str_replace(t_reg,\"[\\\\s+].*gouverner[\\n]+\", \" \") # entete gauche\nt_reg<-str_replace_all(t_reg,\"[\\\\s+].*2021[\\n]\", \" \") # bas de page  gauche\nt_reg<-str_replace_all(t_reg,\"ESPRIT.*[\\n]\", \" \") # bas de page droit\n\n#on marque les paragraphes avec la chaine XXX pour les splitter dans un second temps\n\n\nt_reg<-str_replace_all(t_reg,\"\\n\\n\\n\", \"XXX\") \n\n# On supprime les saut de ligne en les remplaçant par un espace\n\nt_reg<-str_replace_all(t_reg,\"[\\n]\", \" \")\n\n#on enlève les notes de bas de page\nt_reg<-str_replace_all(t_reg,\"\\\\d\\\\s[\\\\-].*XXX\", \"XXX\")\n\n#on regroupe les pages\n\nt<-paste(unlist(t(t_reg)), collapse=\" \")\n\n\n\n#on enlève les notes dans le texte\n\nt<-str_replace_all(t,\"[A-Z|a-z]+\\\\d\\\\s[\\\\-]\", \" \")\n\nt<-str_replace_all(t,\"\\\\d\\\\d\\\\s[\\\\-]\", \" \")\n\n#hyphenation\n\nt<-str_replace_all(t,\"[A-Z|a-z]+[\\\\-]\\\\s\", \"\")\n\n#pour enlever les espaces excedentaires\n\nt<-str_squish(t)\nt\n\n#On découpe en paragraphes\nt<- str_split(t, \"XXX\",simplify = TRUE)\nt2<-as.data.frame(t(t))\n\n```\n\nPlus les textes sont standardisés et plus simple est le processus d'importation des pdf. Si l'on souhaite aller plus loin on recommande par exemple <https://ropensci.org/blog/2018/12/14/pdftools-20/> pour extraire un tableau. ( à développer en 4 ou 5 lignes avec des références)\n\n### La numérisation et l'OCR\n\nD'immenses archives sont numérisées, ce qui signifie qu'elles sont stockées sous forme d'image. L'information est contenu dans les pixels, et l'enjeu est d'y reconnaitre des formes caractéristiques : alphabet, ponctuation à travers de multiples variations. Les plus fortes sont celles manuscrites, mais l'écriture typographique est aussi très variables dans ses formes. C'est un enjeu industriel ancien. La reconnaissance optique des caractères a cependant fait d'immense progrès et atteint des niveaux de performance élevés.\n\nLe traitement des adresses a sans doute été le problème principal qui a stimulé les technologies de l'OCR. La qualité du matériau est essentiel, et s'assurer que les expéditeurs choisissent un modèle conventiel et normé de rédaction de l'adresse est une condition de leur succcès. La situation idéale ressemble à ceci.\n\n![Modèle de rédaction correcte d'une adresse postale](./images/adresse-postale.png)\n\nLa réalité ressemble souvent à celà\n\n![à çà](./images/Posted_registered_letter_cover_Ukraine1998.jpg)\n\nDans un environnement en science sociale la situation est moins complexe, les documents analysés ne seront le plus souvent pas des documents mansuscrits ( sauf pour les médiévistes), mais un scan de document plus structuré. Par exemple les jpg\n\nUne solution pour R est [tesseract](https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html). C'est un package qui permet d'accéder au programme du même nom, développé à l'origine chez Hewlett-Packard Laboratories entre 1985 et 1994, avec quelques modifications supplémentaires apportées en 1996 pour le portage sur Windows, et sur C en 1998. Tesseract a été mis en open Source par HP, en 2005, puis de 2006 à novembre 2018, a continué d'être développé par Google. Il s'appuie sur des réseaux neuronaux de type LSTM. C'est une petite, mais puissante intelligence artificielle qui supporte plus d'une centaine de langues.\n\nTestons-le sans attendre avec le texte suivant. Extrait du premier article du premier numéro de la revue \" Etalages\" Publiée en France de 1909 à 1938. L'image est un extrait du document numérisé fournit par la BNF.\n\n![Lettre de motivation](./images/N1_avril1909b.jpeg)\n\n<https://gabriben.github.io/NLP.html#introduction>\n\n```{r 303, fig.cap='', out.width='80%', fig.asp=1, fig.align='center', fig.width=9}\nlibrary(tesseract)\ntesseract_download(\"fra\") #pour télécharger le modèle de langage\n\nt1<-Sys.time()\ntext <- tesseract::ocr(\"./data/N1_avril1909b.jpeg\", engine = \"fra\")\nt2<-Sys.time()\nt<- t2-t1 #pour compter le temps de calcul\ncat(text) #pour afficher le texte avec sa mise en page\n\n#tesseract_info() #voir les langues disponibles\nt\n```\n\nPour améliorer la performance qui peut se mesurer au niveau des lettres, mais doit surtout l'être au niveau des mots, deux stratégies sont possibles. La première de pre-processing, la seconde de post-processing avec un mécanisme de détection et de correction d'erreurs. Le pre-processing consiste à traiter l'image en renforçant les contrastes , en éliminant le bruit, on en rend les pixels mieux digestes pour tesseract. C'est ce à quoi s'attache le package magick qui offre un bouquet de fonctions à cette fin. Nous laissons le lecteur le tester seul.\n\nLe post-processing sert à introduire des mécanismes de correction d'erreurs au niveau des mots. Pour une idée de ce type de développement voir [Gabriel, Yadir, Xiaojie, Mingyu](https://gabriben.github.io/NLP.html)\n\nNaturellement, un paramètre important est la vitesse de traitement des images. Dans un projet complet on peut être amener à traiter des centaines images en boucle. Dans notre exemple la durée est de `t` secondes, autrement dit 6 images à la minute ou 360 à l'heure...\n\n### Du speech au texte\n\nLa tradition méthodologique de la sociologie est celle de l'entretien, avec toute sorte d'acteurs. Elle aboutit à la production de transcriptions, plus ou moins détaillées et précises. Mais des textes\n\nOn peut désormais enregistrer la paroles des interfaces vocales. Le speech to text est de plus en plus efficace, voir l'API de google. Il existe déja des packages sur R qui permettent d'accéder aux solutions de google langage qui nécessite une clé d'API.\n\n<https://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html>\n\nune autre solution\n\nOn ne fait qu'entre-ouvrir le sujet, mais il est certainement un des futurs du NLP.\n\n## L'exploitation de base de données textuelles\n\nLes bases de données textuelles sont désormais nombreuses , variées et normalisées. Elles permettent de constituer rapidement des corpus étoffés\n\n### le cas Europresse\n\nOn commence par un exemple simple en utilisant la base [europresse](http://www.europresse.com/fr/). L'objectif est de constituer un fichier de références bibliographiques, exploitable via R.\n\nDans Europresse , nous avons fait une recherche sur les articles comprenant le terme \" vaccination\" dans la presse nationale françaises, constituées de 14 titres. On retient les 400 derniers articles du mois de Juillet 2021, le 12 le Président faisait une allocution.\n\nOn utilise [revtools](https://revtools.net/data.html#importing-to-r) pour sa fonction d'importation des fichiers au format .ris et de transformation en data frame structuré\n\n```{r}\nlibrary(revtools)\ndf <- read_bibliography(iconv(\"./data/vaccination.ris\"))%>%\n  mutate(jour=str_sub(DA, 9,10))\n\nflextable(head(df,3))\n\ng22<-ggplot(df, aes(x=jour))+\n  geom_bar()+\n  labs(x= \"Jour du mois de Juillet 2021\",y=NULL)+\n  geom_vline(xintercept=6, linetype=\"dashed\", color = \"red\")+\n  facet_grid(vars(journal))\ng22\n\n\n```\n\n### Jouer avec les bases bibliographiques\n\n`Fulltext` doit être développé.\n\n<https://books.ropensci.org/fulltext/data-sources.html>\n\nOn propose un cas à partir de scopus ( demander à Olivier)\n\n## Lire le web : Scrapping\n\nLe scrapping correspond à un internet sauvage où la collecte d'informations se traduit par une technique de chasseurs-cueilleurs, le glanage. C'est l'activité qui consiste à moissonner les informations disponibles sur le net en simulant et en automatisant la lecture par un navigateur (on préfère l'expression des québécois : des butineurs).\n\nElle consiste à construire un robot capable de lire et d'enregistrer les informations disponibles sous forme html puis à les distribuer (parsing) dans des tableaux structurés, selon une stratégie d'exploration du web préalablement définie. En réalité le scrapping pose deux problèmes :\n\n-   Celui de la structure de recherche. C'est le problème que relève les spiders, des robots qui recherchent dans les pages des liens, et vont de proche en proche, de lien en lien, pour explorer un domaine. Ils peuvent être plus systématiques et prendre davantage de l'organisation et structure d'un site web pour énumérer les pages.\n\n-   Celui de la collecte de l'information sur chacune des pages. Il s'appuie sur le principe que le langage html est un langage à balise où le contenu et le contenant sont clairement séparés. Par exemple, dans le corps de texte d'une page on définira un titre par la balise\n\n    <h1>dont l'instruction s'achève par la balise</h1>\n\n    . On sépare ainsi clairement le contenu de la forme.\n\n\\`\n\n<body>\n\n<h1>Un titre de niveau 1 (un gros titre)</h1>\n\n```         \n<p>Un paragraphe.</p>\n\n<h2>Un titre de niveau 2 (un sous titre)</h2>\n  <p>Un paragraphe.</p>\n\n  <h3>Un titre de niveau 3 (un sous-sous titre)</h3>\n    <p>Etc.</p>\n```\n\n</body>\n\n\\`\n\nUltérieurement on pourra définir les propriétés graphiques d'une balise par des CSS. Par exemple avec ceci, les paragraphes seront publiés en caractère bleu.\n\n`p{     color: blue; }`\n\nCe qui nous intéresse n'est pas la décoration, mais le fait que les développeurs définissent des balises spécifiques pour chacun des éléments de leurs pages web, et que si nous savons les repérer , nous avons le moyen de mieux lire le texte. Les balises sont la cible du scrapping. Ces dernières peuvent néanmoins être protégées par les développeurs et encapsulées par d'autres langages informatique rendant leur butinage impossible. L'information n'est alors plus contenue dans la balise et le code source d'une page web.\n\nDe nombreuses ressources sont disponibles, mais pour en rester à R , le package rvest permet de réaliser des extractions simples mais suffisantes pour de nombreux usages.\n\nUne application rvest :\n\n<https://www.r-bloggers.com/2018/10/first-release-and-update-dates-of-r-packages-statistics/>\n\nLe package rvest est générique :\n\n<https://community.rstudio.com/t/scraping-messages-in-forum-using-rvest/27846/2>\n\nVoici un petit exemple sur du contenu d'un forum de VTC, un thread relatif à la question de la vaccination obligatoire.\n\n```{r 307}\n\nlibrary(rvest)\n\n# Scrape thread titles, thread links, authors and number of views\n\nstart <- \"https://uberzone.fr/threads/si-la-vaccination-devient-obligatoire-vous-feriez-vous-vacciner-ou-changeriez-vous-de-corps-de-metier.17425\"\n\nx<-c(\"/page-2\", \"/page-3\", \"/page-4\")\n\nfor (val in x){\n  url<-paste0(start,val)\n  h <- read_html(url)\n\npost <- h %>%\n  html_nodes(\".bbWrapper\") %>%\n  html_text()%>%\n      str_replace_all(pattern = \"\\t|\\r|\\n\", replacement = \"\")\npost\n#authors <- h %>%\n#  html_nodes(\".username--style2 \") %>%\n#  html_text() %>%\n#  str_replace_all(pattern = \"\\t|\\r|\\n\", replacement = \"\")\n\n# Create master dataset (and scrape messages in each thread in process)\n\nmaster_data <- \n  tibble(post)\nrds_name<-paste0(\"./data/df_\",substr(val,2,6),\".rds\")\nsaveRDS(master_data,rds_name)\n}\n\nhead(master_data)\n\n```\n\n### Des problèmes pratiques, juridiques et éthiques\n\nLa pratique du scrapping se heurte d'abord à une question technique. Ce n'est pas un excercice facile, et il doit être confier à des spécialistes. Il se heurte aussi à différents problèmes d'ordre éthique et juridique. Si la pratique n'est pas interdite en tant que telle, elle se confronte à différents droits et principes éthiques\n\nEn termes pratiques, le scrapping crée des risques pour les sites :\n\n-   Le risque de deny of service, c'est à dire de saturer ou de parasiter un système et de s'exposer à ses contre-mesures, comme par exemple, des protections.\n-   Il contribue à la complexification du web, et implique une consommation excessive de ressources énergétiques.\n\nEt des risques pour la qualité du recueil de données\n\n-   Le risque d'information parcellaires, tronquées, inexactes qui résultent de ces contre-mesures. Les producteurs développent des stratégies moins naives. L'exemple des pages numérotée par ordre de production auxquels on substitue un nombre au hasard pour annihilier l'information temporelle.\n-   Le risque matériel de mal lire les informations, pour des raisons d'encodage approximatifs.\n\nEn termes de droits même les conditions légales relèvent de différents droits :\n\n-   De la propriété intellectuelle,\n-   Du respect de la vie privée,\n-   Du droit de la concurrence qui sans l'interdire, condamne la copie laissant espérer qu'une transformation des données fasse qu'il y échappe.\n\nCependant des facilités et tolérances sont souvent accordées quand c'est dans un objectif de recherche et que des précautions minimales d'anonymisation ou de pseudonymisation sont prises, que les règles de conservation et de destruction des données sont précisées.\n\nEn termes éthiques\n\n-   Un principe éthique essentiel dans la recherche, et ailleurs, et de ne pas nuire à la société dans son ensemble, hors cette technique participe à la \"robotisation\" du web (plus de 50% du trafic résulterait de la circulation des spiders , scrapers, sniffers et autres bots, comme dans la forêt une éthique écologique revient à préveler le minimal nécessaire pour l'étude entreprise\n\n## L'importance croissante des API\n\nLes API doivent être considérées comme la voie normale d'accès à l'information, du moins en droit. Elles relèvent du contrat. Le recours aux APIs est civilisé, ne serait-ce parce qu'on introduit une sorte d'étiquette, des règles de courtoisie, un système de reconnaissance réciproque et d'attribution de droits.\n\nSur le plan méthodologique elles présentent l'avantage de donner aux requêtes un caractère reproductible , mêmes si les bases visées peuvent varier. Elles asurent une grande fiabilité des données.\n\nL'utilisation d'API lève l'ambiguïté légale qui accompagne le scraping et peut ainsi paraître comme plus \"civilisée\". Elle nécessite naturellement que le gestionnaire de la base de données fournisse les moyens de s'identifier et de requêter, elle peut avoir l'inconvénient d'être coûteuse quand l'accès est payant, ce qui sera de plus en plus le cas.\n\n### Un tour d'horizon\n\nLa plus part des grandes plateformes offrent des API plus ou moins ouvertes, examinons-en quelques une pour comprendre plus clairement leur intérêt méthodologique. On va se concentrer sur trois exemples : le firehose de tweeter, l'API de google maps, la Crunchbase.\n\nTwitter n'est pas qu'un réseau social, c'est une gigantesque base de données qui enregistre les engagements et les humeurs de 500 millions d'humains à travers la planète et les centres d'intérêts. Elle permet potentiellement de saisir les opinions à différentes échelles géographiques et temporelles, y compris les plus locales et les plus courtes. Elle a le défaut de souffrir fortement de biais de sélection, le premier étant le biais d'engagement. Les passionnés d'un sujet parlent plus que les autres, d'une parole mieux contrôlée.\n\nLe cas de Google maps est passionnant à plus d'un égard. le premier d'entre eux est que dans l'effort d'indicer chaque objet de la planête, la base de données devient un référentiel universel, plus qu'une représentation intéressée du monde. Quand l'utilisateur commun cherche un chemin optimal, l'analyste de données y trouve un socle pour ordonner le monde.\n\nLa Crunchbase construite par le média Techcrunch repertorie les créations de start-up et les levées de fonds qu'elles ont obtenues. Elle recense les dirigeants, les acquisitions, décrit les business model.\n\nintégrité des bases de données, universalité des élément, interopérabilité, disponibilité\n\nLes problèmes posés :\n\n-   Justesse , précision et représentativité. Leur constitution n'est pas aléatoire, leurs couvertures restent partielle.\n-   Accessibilité, la privatisation du commun. Si pour le chercheur les APIs sont sur un plan de principe une merveille, elles instaurent sur un plan plus social des inégalités d'accès énormes aux données qui permettent de valoriser la connaissance. Ce mécanisme opère via deux canaux. Le premier est celui de la tarification; qui ségrège les chercheurs en fonction des ressources dont ils disposent. Le second passe par la couverture du champ, les données les plus précises et les plus denses se trouvent dans les régions les plus riches.\n-   Des catégorisations peu délibérées\n\n### un point de vue plus technique\n\n<https://www.dataquest.io/blog/r-api-tutorial/>\n\n### Un exemple avec Rtweet\n\nLes changement de règles de twitter rende l'exemple obsolète, on le garde pour mémoire.\n\n<https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html>\n\nPlusieurs packages de R permettent d'interroger le firehose ( la bouche d'incendie!) de twitter.\n\n<https://www.rdocumentation.org/packages/rtweet/versions/0.7.0>\n\nL'authentification ne nécessite pas de clé d'API, il suffit d'avoir son compte Twitter ouvert. Cependant la fonction lookup_coords requiert d'avoir une clé d'API ou google cloud map. Elle permet de sélectionner et conditionner l'extraction sur un critère géographique.\n\n<https://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api>\n\n```{r 308}\n#une boucle pour multiplier les hashtag \n\n#x<-c(\"#getaround\",\"#Uber\", \"#heetch\")\n\n#for (val in x) {\n#  tweets <- search_tweets(val,n=20000,retryonratelimit = TRUE)%>% #geocode = lookup_coords(\"france\")\n#      mutate(search=val)\n#  write_rds(tweets,paste0(\"tweets_\",substring(val,2),\".rds\"))\n#}\n\n#df_blablacar<-readRDS(\"./data/tweets_blablacar.rds\")\n#df_uber<-readRDS(\"./data/tweets_uber.rds\")\n#df_heetch<-readRDS(\"./data/tweets_heetch.rds\")\n\n#df<-rbind(df_blablacar,df_uber )\n\n#ls(df_blablacar)\n\n#foo<-df %>% select(account_lang, geo_coords,country_code, country, account_lang,place_name)\n\n```\n\nOn laisse le lecteur explorer les différentes fonctionnalités du package. On aime cependant celle-ci qui échantillonne le flux courant au taux annoncé de 1%. Voici l'extraction de ce qui se dit en France pendant 10 mn (600s). La procédure peut s'apparenter à une sorte de benchmark auquel on peut comparer une recherche plus spécifique.\n\n```{r 309, fig.cap='', out.width='80%', fig.asp=1, fig.align='center', fig.width=9, eval=FALSE}\n#rt <- stream_tweets(lookup_coords(\"france\"), timeout = 600)\n\n```\n\n### Un autre exemple\n\ngoogle map serait bien mais leur API fermée, il en faut une ouverte. discogs .?\n\n## Conclusion\n\nDans ce chapitre nous aurons égratigné des sujets techniques de constitution de corpus en envisageant différents moyens d'accès\n\nOn soulignera la technicité croissante et spécifique de chacun ces moyens de collecte.\n\nOn observera l'étendue des domaines à exploiter.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"q03_corpus_suite.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","bibliography":["references.bib"],"editor":"visual","theme":"cosmo","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"q03_corpus_suite.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}