{
  "hash": "49a44b677f7c566a09a0e54347e85ebe",
  "result": {
    "engine": "knitr",
    "markdown": "# Analyse Quantitative du corpus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#les librairies du chapître\nlibrary(tidyverse)\n#accessoires de ggplot\nlibrary(ggridges)\nlibrary(ggrepel)\nlibrary(ggwordcloud)\n\ntheme_set(theme_minimal()) \n\n#NLP\nlibrary(tokenizers)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats )\n#tableau\nlibrary(flextable)\nset_flextable_defaults(\n  font.size = 10, theme_fun = theme_vanilla,\n  padding = 6,\n  background.color = \"#EFEFEF\")\n\n#palettes\n\nlibrary(wesanderson)\n#names(wes_palettes)\ncol_1<-c(\"#85D4E3\")\ncol_1b<-c(\"#F4B5BD\")\ncol_2<-c <-c(\"#85D4E3\", \"#F4B5BD\")\ncol_4<-c(\"#85D4E3\", \"#F4B5BD\", \"#9C964A\", \"#CDC08C\")\n```\n:::\n\n\n**Objectifs du chapitre :**\n\n<div>\n\n-   Avant de se plonger sans l'analyse du lexique on peut étudier le corpus de manière quantitative d'autant plus s'il se distribue dans le temps. La fréquence des textes, leur longueur, la longueur des mots, leur diversité.\n\n</div>\n\nUne première manière d'aborder un texte ou un corpus est volumétrique. Quel volume de texte? Quelle longueur ? Combien de mots ? Quelles variations?\n\nA cette fin on utilise le cas des tweets de Donald Trump. Des premiers aux derniers, jusqu'au moment de son bannissement en Janvier 2021, après sa défaite aux élections présidentielles. Chargeons le fichier de données.( on retrouvera la description du fichier en préambule)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_csv(\"./data/TrumpTwitterArchive01-08-2021.csv\")\nnrow<-nrow(df) #nombre de ligne\nncol<-ncol(df) #nombre de colonne\ndf$nb_mots<-str_count(df$text, \" \")+1 # l'astuce : compter les espaces et ajouter 1, pour compter les mots\nsum_mots<-sum(df$nb_mots)             #ON COMPTE LE NOMBRE DE MOTS\n```\n:::\n\n\n## Comptons les mots\n\nIl y 56571 tweets et 9 variables et 1.117018\\times 10^{6} mots cumulés.\n\nOn peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de `stringr`, un package précieux que nous allons étudier de plus dans le chapitres suivant : `str_count`.\n\nOn note immédiatement la bi-modalité de la distribution qui correspond au changement de règle par Twitter en matière de non de caractères utilisés, étendu de 180 à 280.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df, aes(x=nb_mots))+\n  geom_histogram(fill=col_1, binwidth = 10)+\n  labs(title=paste0(\"Nombre total de mots du corpus : \",sum_mots), \n       x=\"Nombre de mots par post\", \n       y=\"Fréquence\")\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/602-1.png){width=672}\n:::\n:::\n\n\nLa bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de l'album de Joy Division : un graphique en crêtes (ridges plot) avec [`ggridges`](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).\n\nLe résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à l'occasion d'autre contenu en 50 mots environ.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$Year<-format(df$date, format = \"%Y\") #on extrait l'année de la date\nfoo<- df %>% \n  filter(Year!=\"2021\") #parce qu'il n'y a quelques quelques jours en janvier, le compte a été interrompu vers le 5 janavier\n\nggplot(foo,aes(x = nb_mots, y = Year, group = Year)) +\n  geom_density_ridges(scale = 4, color=\"grey90\",fill=col_1, alpha=.7)+\n  theme_ridges() +\n  scale_x_continuous(limits = c(1, 60)) +\n  labs(x=\"Nombre de mots par post\",\n       y=NULL)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/603-1.png){width=672}\n:::\n:::\n\n\n## La production dans le temps\n\nExaminons le nombre de tweets produit au cours du temps.\n\nOn se rappellera qu'après une carrière immobilière menée dans les casinos, le golf et les hôtels, l'appétit médiatique de Trump s'est réalisé dans \"the apprentice\", de 2004 à 2015. C'est un pro de la TV, il a une formation de Popstar. Il sera élu en Décembre 2016 pour prendre le pouvoir en Janvier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$date2<-format(df$date, \"%Y-%m-%d\")\ndf$date3<-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet<-as.character(df$isRetweet)\n\nfoo<-df %>% filter(Year!=2021)%>%\n  group_by(date3, isRetweet)%>%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/604-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#raf : labeliser avec les dates clés\n```\n:::\n\n\n## Popularité des tweets\n\nDans ce set de donnée si l'auteur est unique, la réception est multiple, rappelons que près de 90 millions de personnes suivaient Trump. On possède deux indicateurs : le nombre de retweet et de RT.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<-df %>% select(Year, favorites, retweets)%>%\n  filter(!is.na(favorites) & Year!=2021)%>%\n  group_by(Year)%>%\n  summarize(sum_rt=sum(retweets),\n            sum_fav=sum(favorites),\n            mean_rt=mean(retweets),\n            mean_fav=mean(favorites)\n            ) %>%\n  pivot_longer(-Year,names_to = \"variable\", values_to = \"values\")\n\nggplot(foo, aes(x=Year, y=values,group=variable))+\n  geom_line(aes(color=variable), size=1,alpha=.8)+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_4)+ \n  facet_wrap(vars(variable), ncol=2, scale=\"free\")+\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/605-1.png){width=672}\n:::\n:::\n\n\nOn s'aperçoit d'un changement de régime, quand l'audience est limitée RT et fac sont fortement corrélés, l'entrée en politique de Trump se caractérise par un changement de régime. La décorrélation peut s'expliquer par l'usage d'agent d'influences, qui retweetent plus qu'ils n'approuvent. Ils contribuent à la décorrélation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<-df %>% select(Year, favorites, retweets)%>%\n  filter(!is.na(favorites) & Year!=2021)%>%\n  group_by(Year)%>%\n  summarize(cor(favorites, retweets)) %>%\n  rename(correlation =2)\n            \nggplot(foo, aes(x=Year, y=correlation, group=1))+\n  geom_line(size=1,alpha=.8, color=col_1b)+\n  geom_smooth(color=col_1b, size=1.5, fill=\"grey90\")+\n  labs(y=\"Corrélation entre fav et rt\", x=NULL)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/606-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<-df %>% select(Year, favorites, retweets)%>%\n  filter(!is.na(favorites) & Year==2020)\n\n\nm <- ggplot(foo, aes(x = favorites, y = retweets)) +\n geom_point() +\nscale_x_log10()+\nscale_y_log10()\n\n# contour lines\nm + geom_density_2d()\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/607-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$date2<-format(df$date, \"%Y-%m-%d\")\ndf$date3<-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet<-as.character(df$isRetweet)\n\nfoo<-df %>% filter(Year!=2021)%>%\n  group_by(date3, isRetweet)%>%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/608-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#raf : labeliser avec les dates clés\n```\n:::\n\n\n## Lisibilité et complexité lexicale\n\nPour aller un peu plus loin - nous savons désormais que Trump aime une forme courte en 21 mots, et que son expérience de twitter est longue, on peut s'intéresser à des paramètres clés relatifs aux conditions de la réception: les textes sont-ils aisés à lire ? sont-ils sophistiqués ?\n\nIntroduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. Ce sont des classiques, les critères initiaux de l'analyse quantitative du texte. Ils sont toujours utiles.\n\n### Les indices de lisibilité\n\nLa lisibilité est une notion ancienne tout autant que sa mesure (par exemple @coleman_computer_1975). Elle répond à la question du degré de maîtrise requis pour lire un texte en s'appuyant sur les caractéristiques objectives du texte plutôt que sur sa perception. Il s'agissait donc d'évaluer la complexité d'un texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes, et la complexité des phrases mesurée par le nombre de mots.\n\nA partir de ces deux paramètres, une multitudes d'indicateurs ont été proposés. Dans l'exemple qui va suivre, on se contente d'un grand classique, le plus ancien, l'indice de Flesch [@flesch_new_1948] et de ses constituants : le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase.\n\nOn les retrouve, avec une autre grande variété, dans le package compagnon de quanteda , [`quanteda.textstats`](https://quanteda.io/reference/textstat_readability.html) , qui en fournit des dizaines de variantes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#on sélectionne les tweets originaux\n\nfoo<-df %>% filter(isRetweet==FALSE) # on ne prend pas en compte les RT\n\n#la fonction de calcul de lisibilité\nreadability<-textstat_readability(foo$text, \n                                  measure = c(\"Flesch\",\"meanSentenceLength\", \"meanWordSyllables\"),\n                                  min_sentence_length = 3,\n                                  max_sentence_length = 1000) \n# on joint les données\nfoo<-cbind(foo,readability[,2:4])\n\n#on agrège par année\nfoo1<-foo %>% filter(Year!=2021) %>%\n  group_by(Year) %>%\n  summarise(all_tweet=n(),\n            Flesch=mean(Flesch, na.rm=TRUE), \n            SentenceLength= mean(meanSentenceLength, na.rm=TRUE),\n            WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %>%\n  gather(variable, value, -Year)\n\n#visualisation\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la lisibilité des tweets de Trump\", x=NULL, y=NULL)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/609-1.png){width=672}\n:::\n:::\n\n\nPour aider le lecteur à donner un sens, voici l'abaque proposée par [Flesch](http://www.appstate.edu/~steelekm/classes/psy2664/Flesch.htm) lui-même.\n\n![Flesch](./image/ReadabilityFlesch.JPG)\n\nOn peut aussi prendre pour références les éléments suivants:\n\n\"All Plain English examples in this book score at least 60. Here are the scores of some reading materials I've tested. These are average scores of random samples.\" ( source ?)\n\n-   Comics 92\n\n-   Consumer ads in magazines 82\n\n-   Reader's Digest 65\n\n-   Time 52\n\n-   Wall Street Journal 43\n\n-   Harvard Business Review 43\n\n-   Harvard Law Review 32\n\n-   Auto insurance policy 10\n\nTrump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Le Reader's Digest est beaucoup plus simple, il se situe au dessus de la Harvard Business Review !\n\n### Les indices de complexité lexicale\n\nLa complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, l'indicateur marque plus cette variété que les variations de complexité lexicale.[@tweedie_how_1998]\n\nDans notre univers trumpesque, ce n'est pas trop sensible, d'autant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweet, une autre approche pourrait être de concaténer l'ensemble des tweets d'une période (un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvre l'ensemble des sujets d'intérêt de Trump, que les tweets fractionnent nécessairement. Ce qui en en question dans la mise en pratique n'est pas seulement la question du choix de l'indice mais aussi la définition de l'unité de calcul. La diversité lexicale concerne sans doute plus le discours que la phrase.\n\nOn choisit de ne travailler sur deux des multiples indicateurs disponibles :\n\n-   le CTTR de Caroll qui rapporte le nombre de mots distincts ( V) sur le nombre de mots exprimés. Avec ce critère la diversité maximale est obtenue quand le nombre de mots différents est égal au nombre de mots exprimés.\n\n$$\nCTTR = \\frac{V}{\\sqrt{2N}}\n$$\n\n-   le Mass supposé être moins sensible à la longueur des textes. (voir Torruella et Capsada 2013 ou )\n\n$$\nM = \\frac{log(n) - log(t)}{log² (n)}\n$$\n\nOn notera que le problème de la longueur variables des textes dans un corpus produit un biais : les textes seront mécaniquement plus divers que les textes court, ce qui conduit à des approches segmentées, où la mesure de diversité est une moyenne des moyenne pour chacun des segments de texte . On emploie ici le MATTR, dont le MA signifie moyenne mobile (moving average), et le TTR le token/type ratio.\n\n(attention un pb de log dans le calcul)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#on retient les tweets de plus de 5 mots\nfoo<-df %>%\n  filter(nb_mots>10 & isRetweet==\"FALSE\" & Year!=2021)\n\n#la fonction de calcul de diversité\nt1=Sys.time()\nlexdiv<-tokens(foo$text)%>%\n  textstat_lexdiv(foo$text, measure = c(\"CTTR\", \"Maas\"),  log.base = 10,\n                  remove_numbers = TRUE,  \n                  remove_punct = TRUE,  \n                  remove_symbols = TRUE,\n                  remove_hyphens = TRUE) \nt2=Sys.time()\nt<- t2-t1\nt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 4.253881 secs\n```\n\n\n:::\n\n```{.r .cell-code}\n#On combine les données et on aggrège sur l'année\nfoo<-cbind(foo,lexdiv[,2:3])\nfoo1<-foo %>% \n  group_by(Year) %>%\n  summarise(CTTR=mean(CTTR, na.rm=TRUE), \n            Maas=mean(Maas, na.rm=TRUE)) %>%\n  gather(variable, value, -Year)\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la diversité lexicale des tweets de Trump\", x=NULL, y=NULL)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/610-1.png){width=672}\n:::\n:::\n\n\n## La mesure de la concentration des termes\n\nLe langage d'un point de vue quantitatif a été caractérisé depuis bien longtemps et il est caractérisé par la loi de Zipf distribution 1949 . Estout est un des premiers à avoir proposé la loi, mais un sténographe fameux en aurait mis en évidence empiriquement avant 1916.\n\n### Un débat théorique\n\nCelle-ci s'exprime de la manière suivante : si on classe les termes par ordre de fréquence r, le produit de leur rang par la fréquence est égale à une constante. Autrement dit dans une métrique log, la relation entre rang et fréquence est linéaire et sa pente est négative.\n\n$$f(r).{r}={K} $$ Mandelbrot la généralise\n\nles études empiriques\n\nOn peut encore débattre de sa signification. Une économie cognitive? Une conséquence de la théorie de l'information ? Un signature pour identifier un discours \\## Les lois de distribution du langage\n\nLes lois puissances qui déterminent cet univers et marque dans le registre de l'analyse quantitative une évolution sérieuse. Alors que le modèle dominant de la statistique est la distribution gaussienne, le traitement du langage doit se familiariser avec des distributions puissance.\n\nLa régularité statistique nécessite une interprétation, il n'y en a pas qu'une\n\n-   la loi du moindre de effort qui va aboutir à un débat entre Mandelbrot et Simon.\n\n-   Economie de l'information, un mot puis deux, puis trois, puis des mots spécifique à un certain registre d'action. Le sens est une affaire d'échelle, du concret au général. Le plus concret est celui des mots scientifiques telles qu'on les retrouve en biologie pour décrire une espèce, en droit pour décrire une propriété, ou en médecine pour désigner une pathologie ou un élément d'anatomie. ( espace de connaissance - épistémologique)\n\n-   la distribution des locuteurs et de leurs élocution. Dans le langage vernaculaire celui de nos sociabilités, peu de mots sont employés, à mesure qu'on les enregistre ils prennent une place plus importance d'un point de de vue statistique dans les corpus. Les mots rares sont employés par peu de personne. (espace des population - démographique)\n\n### Application\n\nExaminons plus concrètement\n\nOn reprend la procédure de comptage des mots par groupe, mais sans filtrer sur la fréquence de ces mots. On en obtient `n_w` mots distincts. En examinant avec plus de détail il y a 5 élément d'information : a ) le trait (feature), c'est à dire ici le mot étudié, b) le nombre de fois où il apparaît dans le corpus, c)le rank qui lui correspond, d)le nombre de documents dans lesquels il apparaît.\n\nLe tableau nous donne les 10 premiers. Il y a 28000 éléments, dont une simple inspection montrera qu'il sont des liens ou d'autres mentions. Il est nécessaire de retraiter le corpus pour éliminer ces éléments. Nous étudierons comment faire dans les chapitres suivants. A ce stade on se contente d'enlever les *apax*[^q06_analyse_quantitative-1] dont on ne peut véritablement calculer un rang ( ils ont tous le dernier). Il nous reste 10854 termes.\n\n[^q06_analyse_quantitative-1]: les apax sont les termes qui n'apparaissent qu'une seule fois dans un corpus.\n\n\n::: {.cell tbl-cap='Mots les plus fréquents'}\n\n```{.r .cell-code}\nfoo<-df %>% \n  filter(isRetweet==FALSE) %>%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks<- tokens(foo$text) %>% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\nfreq_g <- textstat_frequency(toks)%>%\n  as.data.frame() %>%\n  filter(feature!=\"amp\") %>%\n  filter(frequency>1)\n\nflextable(head(freq_g, 15))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-16484a8e{}.cl-1637ad14{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1637ad32{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-163e5b00{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:6pt;padding-top:6pt;padding-left:6pt;padding-right:6pt;line-height: 1;background-color:transparent;}.cl-163e5b14{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:6pt;padding-top:6pt;padding-left:6pt;padding-right:6pt;line-height: 1;background-color:transparent;}.cl-163e8cc4{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8cd8{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8cd9{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8ce2{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8cec{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8ced{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8cf6{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-163e8cf7{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-16484a8e'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-163e8cc4\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad14\">feature</span></p></th><th class=\"cl-163e8cd8\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad14\">frequency</span></p></th><th class=\"cl-163e8cd8\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad14\">rank</span></p></th><th class=\"cl-163e8cd8\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad14\">docfreq</span></p></th><th class=\"cl-163e8cc4\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad14\">group</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cd9\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">great</span></p></td><td class=\"cl-163e8ce2\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">4,077</span></p></td><td class=\"cl-163e8ce2\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1</span></p></td><td class=\"cl-163e8ce2\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">3,755</span></p></td><td class=\"cl-163e8cd9\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">thank</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">2,041</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">3</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">2,029</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">people</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">2,040</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">4</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,895</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">just</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,633</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">5</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,572</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">now</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,544</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">6</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,505</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">president</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,466</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">7</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,322</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">trump</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,424</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">8</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,307</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">big</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,360</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">9</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,263</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">news</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,347</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">10</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,254</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">country</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,274</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">11</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,233</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">get</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,180</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">12</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,083</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">fake</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,177</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">13</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,100</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">new</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,167</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">14</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,089</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">democrats</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,163</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">15</span></p></td><td class=\"cl-163e8ced\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,119</span></p></td><td class=\"cl-163e8cec\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-163e8cf6\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">many</span></p></td><td class=\"cl-163e8cf7\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,108</span></p></td><td class=\"cl-163e8cf7\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">16</span></p></td><td class=\"cl-163e8cf7\"><p class=\"cl-163e5b14\"><span class=\"cl-1637ad32\">1,061</span></p></td><td class=\"cl-163e8cf6\"><p class=\"cl-163e5b00\"><span class=\"cl-1637ad32\">all</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nOn peut aussi représenter cela graphiquement. La loi de zipf s'observe jusqu'au 500 premiers mots, ensuite elle suit une autre pente. C'est assez caractéristique, la loi de zipf est incomplète, d'un point de vue empirique il semble qu'il y ait deux lois qui se superposes, la loi des mots communs, et celle des mots singuliers. C'est du moins hypothèse que nous proposons.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(freq_g, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"gam\", color=col_1)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/612-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfoo<-freq_g %>%filter(rank <500)\nggplot(foo, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"lm\", color=col_1)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/612-2.png){width=672}\n:::\n:::\n\n\nPour être plus qualitatif Une première manière de représenter la diversité du corpus est de représenter les mots les plus fréquents, selon cette fréquente et le ratio fréquence par document. Plus ce dernier est élevé plus il fait du mot un mot spécifique.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<- freq_g %>%\n  filter(frequency>150 &frequency<1500 )\n\nggplot(foo, aes(x=docfreq, y= frequency/docfreq))+\n  geom_text_repel(aes(label=feature), size=2, max.overlaps = 30)+\n  scale_x_log10()+\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/613-1.png){width=672}\n:::\n:::\n\n\n## Comptons les mots\n\nIl est temps de compter les mots, chacun d'entre eux, de se faire une idée une idée de leurs fréquences, de leur distribution.\n\nSouvent on éliminera ceux qui apparaissent de manière occasionnelle, mais aussi ceux qui apparaissent systématiquement dans tous les textes. Une fois ces deux filtrages effectués, le lexique est généralement de l'ordre de 500 à 10000 mots.\n\nDeux outils sont disponibles: les nuages de mots et les lollyplots. Les premiers donnent une idée immédiates, les seconds se prêtent mieux à une analyse systématique\n\n### Les nuages de mots\n\nIls sont devenus extrêmement populaires même si l'effet esthétique est plus important que leur utilité analytique.\n\n[`ggwordcloud`](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html)\n\nPour l'application on prépare les données avec quanteda : on tokenise et on construit le dfm (pour le détail voir chapitre tokenization), ce qui nous permets notamment d'éliminer la ponctuation et les mots courants (articles, déterminant etc) qui apportent peu de signification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<-df %>% filter(isRetweet==FALSE) %>%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks<- tokens(foo$text) %>% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\n\ndocvars(toks,\"Year\")<-foo$Year\n\n#on se concentre du les termes utilisés 300 fois.  \n\nfoo<-toks %>% \n    dfm_trim(min_termfreq = 250, verbose = FALSE)\n\nfreq <- textstat_frequency(foo)\n\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal() +  scale_size_area(max_size = 10) + \n  scale_color_gradient(low = col_1, high = col_1b)\n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/614-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(\"./image/g0.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")\n```\n:::\n\n\nEt pour faire des comparaisons, entre l'année 2016 qui le conduit à être élu, 2018 une année de midterm et 2020 année de sa défaite, on utilise la même procédure mais on distingue un comptage de fréquence de mot par période. Le ggplot est identique aux précédents mais comprend en plus une géométrie \"facet_wrap\" qui éclate le nuages de mot selon les 3 périodes étudiées.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo<-toks %>% \n  dfm_group(groups = Year) %>%\n    dfm_trim(min_termfreq = 1, verbose = FALSE)\n\nurl_regex <- \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n  \n#pour compter la fréquence des mots par année\nfreq <- textstat_frequency(foo, group =Year) %>%\n  mutate(feature=str_remove_all(feature, url_regex))|>\n  filter(!is.na(feature) & frequency>200 )%>%\n  filter(feature!=\"amp\")\n           \n\nset.seed(42)\nlibrary(ggwordcloud)\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal()+\n  facet_wrap(vars(group)) +  \n  scale_size_area(max_size = 8) \n```\n\n::: {.cell-output-display}\n![](q06_Analyse_quantitative_files/figure-html/615-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(\"./image/g1.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")\n```\n:::\n\n\n## Conclusion\n\nNous aurons appris à\n\n-   Compter le nombre de documents et leurs longueurs\n-   Mesurer la complexité du langage\n-   Mesurer la diversité de son vocabulaire.\n-   A évaluer la concentration des sources\n-   A se donner une première idée de la lexicographie\n\nCes mesures n'ont ne sens que si elles peuvent être l'objet de comparaison :\n\n-   De manière interne la comparaison se fait dans dans le temps et à travers des segments. On s'intéresse moins au niveau, qu'aux différences entre les niveaux.\n-   De manière externe elle requiert un étalonnage. Comparer par rapport au français courant, à un niveau de langue soutenu, ou relâché. L'étalonnage revient à caractériser des types de corpus : presses, écriture savante, réseaux sociaux, publications officielles etc. On ne peut que souhaiter que des comparaisons systématiques soient engagées et compilées pour donner des points de repère précis quand on étudie un corpus particulier.\n\nElles participent à un premier niveau d'analyse du texte, en surface, visant à apprécier la dynamique de sa production, à établir les échelles d'analyse, à repérer les éléments structurels.\n\nLe texte est une matière qui a un poids (le nombre de mot), une variété (le nombre d'expressions), une complexité (les règles qui l'organisent). nous venons de nous doter des premiers outils d'analyse, il est temps de passer à la suite.\n",
    "supporting": [
      "q06_Analyse_quantitative_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}