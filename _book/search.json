[
  {
    "objectID": "q07_tokenisation.html",
    "href": "q07_tokenisation.html",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "7.1 Quelques exercices de tokenization",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#quelques-exercices-de-tokenization",
    "href": "q07_tokenisation.html#quelques-exercices-de-tokenization",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "7.1.1 Les lettres\nCommençons par un exemple simple, à l’aide d’une courte citation de Max Weber. On choisit les lettres pour unité de découpe, et l’on utilise le package ‘tokenizer’. Automatiquement, ‘tokenizer’ met le texte en minuscule et élimine la ponctuation\n\n\nShow the code\n#Les données\nMaxWeber &lt;- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.\")\n\n#On tokenise, plus on transforme en dataframe le résultat.\ntoc_maxweber&lt;-tokenize_characters(MaxWeber)%&gt;%\n        as.data.frame()%&gt;%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo&lt;-toc_maxweber %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=n())%&gt;%\n        filter(n&gt;0)\n\n#On représente par un diagramme en barre cette distribution des occuences d'apparition, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=10, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+\n        labs(title = \"Fréquence des tokens, unité = lettres\", \n             x=\"tokens\", \n             y=\"nombre d'occurences\", \n             caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n\n\n\n\n\n\n\n\n\n\n\n7.1.2 les mots\n\n\nShow the code\n#Les données\n\nMaxWeber &lt;- paste0(\"Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains. La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés. Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent. Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de l’intérêt général.\")\n\n#On tokenise, plus on transforme en dataframe le résultat. le strp_punc écarte la ponctuation du processus.\ntoc_maxweber&lt;-tokenize_words(MaxWeber,strip_punct=TRUE)%&gt;%\n        as.data.frame()%&gt;%\n        rename(tokens=1)\n\n#On compte pour chaque token sa fréquence d'apparition\nfoo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=sum(n))\n\n#On représente par un diagramme en barre cette distribution des occurrences, en classant les tokens par fréquence\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n        annotate(\"text\", x=10,y=4, label=paste(\"nombre de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"Fréquence des tokens, unité = mots\", x=\"tokens\", y=\"nombre d'occurences\", caption =\" 'Bureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.' \")\n\n\n\n\n\n\n\n\n\nOn peut également constater que certains mots sont proches, par exemple les deux derniers sur le graphiques précédents qui sont des déclinaisons du verbe appliquer. Il peut alors être pertinent de regrouper ces différentes formes verbales (comme un mot au singulier et au pluriel, au féminin et au masculin, ou conjugué sous différentes formes), pour faciliter l’analyse. C’est ce qu’on fait avec les opérations de stemming ou de lemmatisation, présentées au chapitre 8.\n\n\n7.1.3 Les phrases\nOn reproduit les mêmes opérations, mais cette fois sur les phrases de l’exemple précédent.\n\n\nShow the code\ntokenize_sentences(MaxWeber)%&gt;%\n  as.data.frame()%&gt;%\n  rename(tokens=1)%&gt;%\n  flextable(cwidth = 6)\n\n\ntokensBureaucratie: le moyen le plus rationnel que l’on connaisse pour exercer un contrôle impératif sur des êtres humains.La bureaucratie est une forme d'organisation générale caractérisée par la prépondérance des règles et de procédures qui sont appliquées de façon impersonnelle par des agents spécialisés.Ces agents appliquent les règles sans discuter des objectifs ou des raisons qui les fondent.Ils doivent faire preuve de neutralité et oublier leurs propres intérêts personnels au profit de l’intérêt général.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#n-grammes",
    "href": "q07_tokenisation.html#n-grammes",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "7.2 N-grammes",
    "text": "7.2 N-grammes\nLes n-grammes sont des séquences de n tokens, généralement consécutifs. Sur la d’un base d’un corpus important on peut calculer la probabilité d’apparition d’un n-gramme. C’est l’exercice auquel une équipe de google s’est attelé avec le Books Ngram Viewer. et dont nous encourageons l’étude détaillée.\nL’article original doit aussi être lu. Sur la base de 5 millions de livres numérisés en 2011 et représentant 4% du nombre total de livres à jamais publiés à ce moment, être lu aussi, il montre que, pour l’anglais, chaque année 8500 mots entrent dans le lexique, sans que les dictionnaires, pour des raisons évidentes de concision, n’en font l’inventaire. Le webster comprend 300 000 mots, alors que pour l’anglais l’estimation du nombre de termes différents, est passé de l’ordre de 500 000 en 1900, à plus d’1.5 million en 2000. Il donne aussi une dimension au lexique : celle de sa densité. Dans un corpus, à un moment donné quelle est la probabilité d’observer un token donné (au plus simple un mot). Sur un corpus de 500 milliards de termes (pour toutes les langues traitées), un mot qui apparaît une fois sur 1000 ( 10^-3), apparaît 5 millions de fois, s’il apparaît une fois sur 1 million, il apparaît 5000 fois. L’échelle de mesure est ainsi la fréquence : 10-3, 10-5, 10-6 ….\nLa notion de ngram au-delà de son apparence de tautologie possède aussi un intérêt théorique majeure. Les ngram sont des chaines de markov. Intérêt théorique des n gram : ce sont des chaines de markov.\nProcessus de markov\nPour comprendre l’importance de ce concept on peut considérer au moins 3 applications remarquables\nApplication à l’auto-complétion . Son efficacité dépend naturellement de la taille des données récoltées. On comprend que pour google il est aisé d’être précis dans l’estimation de ces probabilités!\nMais mieux encore, ces propriété markovienne ( probabilistique) permettent aussi la correction d’erreur avec l’algorithme de Viterbi https://fr.wikipedia.org/wiki/Algorithme_de_Viterbi\n\nLe principe de ‘textcat’ est fondée sur ces n-grammes de lettre. Chaque langue se caractérise par une distribution particulière des n-grammes. Pour décider de l’appartenance d’un texte à une langue, si on dispose des profils de distribution, on compare la distribution des n-grammes du texte à ces références. On peut ainsi calculer une distance et attribuer le texte à la langue dont il est le plus proche.\n\n\n7.2.1 Mise en oeuvre\nSi la notion est simple, sa mise en oeuvre l’est presque autant. Nous l’illustrons avec une séries d’exemples et le package “tokenizer” qui a ici un avantage pédagogique. On étudiera plus loin les ressources de “quanteda”.\nOn commence de suite par un exemple sur les lettres. c’est de pure forme. On sélectionne les ngrammes (k&gt;1 et k&gt;4) dont la fréquence est supérieure à trois.\n\n\nShow the code\n#tokenization des lettres\ntoc_maxweber&lt;-tokenize_character_shingles(MaxWeber,n=3, n_min=2) %&gt;%\n        as.data.frame()%&gt;%\n  rename(tokens=1)\n\nflextable(head(toc_maxweber, n=20))\n\n\ntokensbuburururerereaeaeauauaucucucrcrcrararatatatititie\n\n\nShow the code\nfoo&lt;-toc_maxweber %&gt;%mutate(n=1) %&gt;% \n        group_by(tokens)%&gt;% \n        summarise(n=sum(n))%&gt;%\n  filter(n&gt;3)\n\nggplot(foo, aes(x=reorder(tokens,n), y=n))+\n               geom_bar(stat=\"identity\", fill=\"royalblue\")+\n  annotate(\"text\", x=5,y=11, label=paste(\"nombre total de tokens =\", nrow(toc_maxweber)))+\n               coord_flip()+labs(title = \"digrammes et trigrammes des lettres\", x=\"n-gramme\", y=\"nombre d'occurences\")\n\n\n\n\n\n\n\n\n\nOn peut faire la même chose sur les mots, et en plus en éliminant les stopwords. l’intérêt de la procédure est de se concentrer sur les idées, des paires de mots consistants du point de vue sémantique. Un ordre ce dégage ! la bureaucratie est un moyen, puis une rationalité qui dépend d’un exercice.\nOn note qu’il n’y a pas besoin de beaucoup de mots pour produire du sens par l’analyse statistique de leurs distributions.\n\n\nShow the code\ntoc_maxweber&lt;-tokenize_ngrams(MaxWeber,\n                              n=3, \n                              n_min=2, \n                              stopwords = stopwords('fr')) %&gt;%\n  as.data.frame()%&gt;%\n  rename(tokens=1)\n\nqflextable(head(toc_maxweber, n=19))\n\n\ntokensbureaucratie moyenbureaucratie moyen plusmoyen plusmoyen plus rationnelplus rationnelplus rationnel l’onrationnel l’onrationnel l’on connaissel’on connaissel’on connaisse exercerconnaisse exercerconnaisse exercer contrôleexercer contrôleexercer contrôle impératifcontrôle impératifcontrôle impératif êtresimpératif êtresimpératif êtres humainsêtres humains\n\n\nOn peut également s’intéresser aux n-grammes non directement consécutifs mais séparés par k tokens. C’est sans doute un moyen de saisir des corrélations de mots à plus grande distance. La possibilité formelle est là , nous avouons ne pas savoir à quel usage elle correspond. Nous serions sur ce point ravi d’avoir des réponses.\n\n\nShow the code\ntoc_maxweber&lt;-tokenize_skip_ngrams(MaxWeber,\n                                   n=3, \n                                   n_min=2, \n                                   k=2, \n                                   stopwords = stopwords('fr')) %&gt;%\n        as.data.frame()%&gt;%rename(tokens=1)\nqflextable(head(toc_maxweber, n=19))\n\n\ntokensbureaucratie moyenbureaucratie plusbureaucratie rationnelbureaucratie moyen plusbureaucratie moyen rationnelbureaucratie moyen l’onbureaucratie plus rationnelbureaucratie plus l’onbureaucratie plus connaissebureaucratie rationnel l’onbureaucratie rationnel connaissebureaucratie rationnel exercermoyen plusmoyen rationnelmoyen l’onmoyen plus rationnelmoyen plus l’onmoyen plus connaissemoyen rationnel l’on\n\n\nDans cet exemple, aucun n-gramme n’est répété, mais c’est rarement le cas avec des corpus plus importants. Dans ce cas, une forte répétition de n-grammes est un indice d’une unité sémantique composée de plusieurs tokens que l’on peut alors regrouper en un seul et même token. C’est ce que l’on verra dans la section suivante, avec la méthodes des collocation,",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#choisir-des-n-grammes-pertinents",
    "href": "q07_tokenisation.html#choisir-des-n-grammes-pertinents",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "7.3 Choisir des n-grammes pertinents",
    "text": "7.3 Choisir des n-grammes pertinents\nDans ce e-book l’unité principale d’analyse restera le mot. Mais nous savons, au moins intuitivement que certaines combinaisons de mots représentent des expressions qui ont la valeur d’un mot, une valeur sémantique, par exemple, l’expression “Assemblée Nationale”. Ces deux mots réunis constituent un syntagme, une unité de sens. La question qui se pose est alors de savoir comment les identifier dans le flot des n-grammes ?\nLa technique est simple : si deux mots se retrouvent dans un ordre donné plus fréquemment que ce que le produit de leurs probabilités d’apparition laisse espérer, c’est qu’ils constituent une expression. On peut imaginer faire un test du chi² pour décider si un couple de mots constitue une unité sémantique ou non.\nLe package quanteda propose une bonne solution à ce problème avec la fonction collocation.\n\n7.3.1 Créer les tokens avec ‘quanteda’\nÀ partir du corpus des commentaires de TripAdvisor concernant les hôtels de Polynésie Française,\n\n\nShow the code\n#les données\nAvisTripadvisor&lt;-read_rds(\"./data/AvisTripadvisor.rds\")\nAvisTripadvisor$Taille_hotel&lt;-as.character(AvisTripadvisor$Taille_hotel)\nAvisTripadvisor$Taille_hotel[is.na(AvisTripadvisor$Taille_hotel)]&lt;-\"Autre\"\n\n\n#création du corpus\ncorpus&lt;-corpus(AvisTripadvisor,docid_field = \"ID\",text_field = \"Commetaire\", docvars =AvisTripadvisor)\nft&lt;- head(corpus,3) %&gt;% as.data.frame()%&gt;%\n  rename(tokens=1)%&gt;%\n  flextable(cwidth = 6)\nft\n\n\ncorpustokensTout est magnifique au Vahine island. Séjour de rêve avec une équipe très chaleureuse . Le cadre est splendide. Notre meilleur hébergement en Polynésie.Merci à toute l' équipe pour votre gentillesse et vos nombreuses attentions.Tout était parfait, notre meilleure expérience et plus belle découverte en Polynésie.Un grand merci à Amélie et toute son équipe pour un service de très haut niveau.De la chambre, en passant par la restauration et les activités, tout était parfait. Encore merci à tous, en espérant revenir très vite.Un séjour magnifique, 3 jours époustouflants, accueillis chaleureusement par toute l’équipe du Vahine Island. Tout est au rendez vous, la cuisine délicieuse, le lieu au delà de notre imagination, un calme parfait. À faire, re faire, sans jamais s’en lasser.\n\n\nOn crée un objet de format token, avec la fonction tokens de quanteda, et on choisit d’enlèver la ponctuation, les symboles et les nombres avec les arguments correspondants.\nNotre tokenizer ici a quelques problèmes . Si le “.” n’est pas suivi d’un espace, il ne peut distinguer les mots, il ne saisit pas non plus l’ellipse de “l’équipe”, qui devrait distinguer le determinant “la” de “équipe. Souvent il faudra au préalable remanier les chaines de caractère pour une meilleure qualité de tokenisation.\n\n\nShow the code\n#transformation en objet token\ntok&lt;-tokens(corpus,remove_punct = TRUE, remove_symbols=TRUE, remove_numbers=TRUE)\n\nhead(tok,3) \n\n\nTokens consisting of 3 documents and 21 docvars.\n1 :\n [1] \"Tout\"       \"est\"        \"magnifique\" \"au\"         \"Vahine\"    \n [6] \"island\"     \"Séjour\"     \"de\"         \"rêve\"       \"avec\"      \n[11] \"une\"        \"équipe\"    \n[ ... and 22 more ]\n\n2 :\n [1] \"Tout\"         \"était\"        \"parfait\"      \"notre\"        \"meilleure\"   \n [6] \"expérience\"   \"et\"           \"plus\"         \"belle\"        \"découverte\"  \n[11] \"en\"           \"Polynésie.Un\"\n[ ... and 37 more ]\n\n3 :\n [1] \"Un\"              \"séjour\"          \"magnifique\"      \"jours\"          \n [5] \"époustouflants\"  \"accueillis\"      \"chaleureusement\" \"par\"            \n [9] \"toute\"           \"l’équipe\"        \"du\"              \"Vahine\"         \n[ ... and 27 more ]\n\n\nAu passage, on peut aussi enlever les stopwords. C’est a dire les mots d’usages courants mais sans signification intrinsèque : les conjonctions, les déterminants, etc, qui se présentent sous la formes de dictionnaires.\nPour que les n-grammes très fréquents restent des syntagmes signifiants, on laisse apparentes les positions des stopwords, avec l’option padding= TRUE.\n\n\nShow the code\n#enlever les stopwords\ntok&lt;-tokens_remove(tok,stopwords('fr'),padding=FALSE)\nhead(tok, 3)\n\n\nTokens consisting of 3 documents and 21 docvars.\n1 :\n [1] \"Tout\"        \"magnifique\"  \"Vahine\"      \"island\"      \"Séjour\"     \n [6] \"rêve\"        \"équipe\"      \"très\"        \"chaleureuse\" \"cadre\"      \n[11] \"splendide\"   \"meilleur\"   \n[ ... and 7 more ]\n\n2 :\n [1] \"Tout\"         \"parfait\"      \"meilleure\"    \"expérience\"   \"plus\"        \n [6] \"belle\"        \"découverte\"   \"Polynésie.Un\" \"grand\"        \"merci\"       \n[11] \"Amélie\"       \"toute\"       \n[ ... and 18 more ]\n\n3 :\n [1] \"séjour\"          \"magnifique\"      \"jours\"           \"époustouflants\" \n [5] \"accueillis\"      \"chaleureusement\" \"toute\"           \"l’équipe\"       \n [9] \"Vahine\"          \"Island\"          \"Tout\"            \"rendez\"         \n[ ... and 13 more ]\n\n\n\n\n7.3.2 Application à la détection des entités nommées\nOn cherche ici à identifier les noms propres présents dans le corpus.\n\n\nShow the code\nlibrary(quanteda.textstats)\n#on sélectionne les mots commençant par une majuscule\ntoks_cap &lt;- tokens_select(tok, \n                               pattern = \"^[A-Z]\",\n                               valuetype = \"regex\",\n                               case_insensitive = FALSE, \n                               padding = TRUE)\n\n#on cherche les collocations\ntstat_col_cap &lt;- textstat_collocations(toks_cap, min_count = 3, tolower = FALSE)\n\nflextable(head(as.data.frame(tstat_col_cap)))\n\n\ncollocationcountcount_nestedlengthlambdazBora Bora155026.60401653.27337Sylvie Yves21028.18378325.93161Muriel Franck19027.14159224.46065Corinne Frédéric17028.60970123.23737Pearl Beach13028.03793521.75092Yves Sylvie13027.28575421.32976\n\n\n\n\n7.3.3 Composer des tokens à partir d’expressions multi-mots : collocation\nDans ce corpus, les noms propres correspondent aux noms des îles et des hôtels, et aux prénoms composés. La valeur du lambda montre la force de l’association entre les mots, on retiendra d’une manière générale un lambda au moins supérieur à 3 pour remplacer les tokens d’origine par leurs n-grammes.\n\n\nShow the code\ntoks_comp &lt;- tokens_compound(tok, pattern = tstat_col_cap[tstat_col_cap$z &gt; 3,], \n                             case_insensitive = FALSE)\n\nhead(toks_comp)\n\n\nTokens consisting of 6 documents and 21 docvars.\n1 :\n [1] \"Tout\"        \"magnifique\"  \"Vahine\"      \"island\"      \"Séjour\"     \n [6] \"rêve\"        \"équipe\"      \"très\"        \"chaleureuse\" \"cadre\"      \n[11] \"splendide\"   \"meilleur\"   \n[ ... and 7 more ]\n\n2 :\n [1] \"Tout\"         \"parfait\"      \"meilleure\"    \"expérience\"   \"plus\"        \n [6] \"belle\"        \"découverte\"   \"Polynésie.Un\" \"grand\"        \"merci\"       \n[11] \"Amélie\"       \"toute\"       \n[ ... and 18 more ]\n\n3 :\n [1] \"séjour\"          \"magnifique\"      \"jours\"           \"époustouflants\" \n [5] \"accueillis\"      \"chaleureusement\" \"toute\"           \"l’équipe\"       \n [9] \"Vahine_Island\"   \"Tout\"            \"rendez\"          \"cuisine\"        \n[ ... and 12 more ]\n\n4 :\n [1] \"Vraiment\"    \"beau\"        \"cadre\"       \"idyllique\"   \"personnel\"  \n [6] \"très\"        \"attentionné\" \"adoré\"       \"séjour\"      \"attention\"  \n[11] \"spéciale\"    \"voyage\"     \n[ ... and 15 more ]\n\n5 :\n [1] \"Vahiné_Island\" \"entre\"         \"monde\"         \"parallèle\"    \n [5] \"où\"            \"sait\"          \"plus\"          \"très\"         \n [9] \"bien\"          \"tient\"         \"réel\"          \"tient\"        \n[ ... and 60 more ]\n\n6 :\n [1] \"adoré\"         \"séjour\"        \"Vahiné_Island\" \"enfants\"      \n [5] \"bungalow\"      \"pilotis\"       \"bungalow\"      \"plage\"        \n [9] \"c'était\"       \"paradis\"       \"motu\"          \"idyllique\"    \n[ ... and 40 more ]\n\n\n\n\n7.3.4 Identifier les autres concepts\nDans ce corpus, on peut aussi s’attendre à voir apparaître d’autres expressions multi-mots qui représentent des concepts, telles que “petit déjeuner”.\n\n\nShow the code\ncol&lt;-textstat_collocations(toks_comp, min_count = 10)\n\nflextable(head(as.data.frame(col)))\n\n\ncollocationcountcount_nestedlengthlambdazpetit déjeuner769026.95571479.38781très bien714023.02262164.09272très bon422023.50611653.16466salle bain274028.72414252.20121très agréable374023.63673750.52490grand merci167025.11229550.38965\n\n\nAu vue de la diversité des collocations, on choisit un lambda supérieur à 7 pour retenir les concepts les plus pertinents.\n\n\nShow the code\ntoks_comp &lt;- tokens_compound(tok, pattern = col[col$z &gt; 7,])\n\nhead(toks_comp)\n\n\nTokens consisting of 6 documents and 21 docvars.\n1 :\n [1] \"Tout\"            \"magnifique\"      \"Vahine\"          \"island\"         \n [5] \"Séjour_rêve\"     \"équipe\"          \"très\"            \"chaleureuse\"    \n [9] \"cadre_splendide\" \"meilleur\"        \"hébergement\"     \"Polynésie.Merci\"\n[ ... and 4 more ]\n\n2 :\n [1] \"Tout_parfait\" \"meilleure\"    \"expérience\"   \"plus_belle\"   \"découverte\"  \n [6] \"Polynésie.Un\" \"grand_merci\"  \"Amélie\"       \"toute_équipe\" \"service\"     \n[11] \"très\"         \"haut\"        \n[ ... and 10 more ]\n\n3 :\n [1] \"séjour\"             \"magnifique\"         \"jours\"             \n [4] \"époustouflants\"     \"accueillis\"         \"chaleureusement\"   \n [7] \"toute_l’équipe\"     \"Vahine\"             \"Island\"            \n[10] \"Tout\"               \"rendez\"             \"cuisine_délicieuse\"\n[ ... and 11 more ]\n\n4 :\n [1] \"Vraiment\"                   \"beau\"                      \n [3] \"cadre_idyllique\"            \"personnel_très_attentionné\"\n [5] \"adoré_séjour\"               \"attention\"                 \n [7] \"spéciale\"                   \"voyage_noce\"               \n [9] \"soirée\"                     \"d’anniversaire\"            \n[11] \"paysage\"                    \"magnifique\"                \n[ ... and 9 more ]\n\n5 :\n [1] \"Vahiné\"    \"Island\"    \"entre\"     \"monde\"     \"parallèle\" \"où\"       \n [7] \"sait\"      \"plus\"      \"très_bien\" \"tient\"     \"réel\"      \"tient\"    \n[ ... and 45 more ]\n\n6 :\n [1] \"adoré_séjour\"     \"Vahiné\"           \"Island\"           \"enfants\"         \n [5] \"bungalow_pilotis\" \"bungalow_plage\"   \"c'était\"          \"paradis\"         \n [9] \"motu\"             \"idyllique\"        \"situé\"            \"lagon\"           \n[ ... and 33 more ]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#des-tokens-au-dtm",
    "href": "q07_tokenisation.html#des-tokens-au-dtm",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "7.4 Des tokens au dtm",
    "text": "7.4 Des tokens au dtm\nD’un point de vue pratique le nombre de colonne est de l’ordre de plusieurs milliers. Dans le cas de texte courts, quelques dizaines, ou même centaine de mots, moins de quelques pour-cent des lignes auront une valeur. C’est un tableau creux (sparse matrix) qui peut nécessiter des représentations particulières et plus économes.\n\n\nShow the code\n#on transforme en document-features matrix pour des représentations graphiques \n\ndfm&lt;-dfm(toks_comp,remove_padding=TRUE) \n\ndfm &lt;- dfm_remove(dfm, stopwords(\"fr\"))\n\ndfm_top &lt;- topfeatures(dfm, n = 80, decreasing = TRUE,  scheme =\"count\")\n\ndfm_top\n\n\n             a           très           plus           tout           bien \n          2345           1355           1194           1141            930 \n       pension        chambre          hôtel       bungalow             si \n           896            883            804            775            748 \n         c'est     restaurant            peu          plage           fait \n           740            716            702            670            663 \n           car          repas         séjour      personnel          faire \n           623            606            600            577            551 \n     bungalows        service          merci       vraiment     magnifique \n           527            480            479            475            472 \n         comme        piscine          lagon          aussi        cuisine \n           469            469            467            463            453 \n      chambres          c’est           donc        accueil petit_déjeuner \n           450            448            436            432            427 \n       l'hôtel          petit          super           tous            vue \n           426            419            395            392            386 \n       l’hôtel           soir           prix        surtout          juste \n           386            377            371            359            351 \n            où       toujours            bon            top        qualité \n           348            348            346            343            315 \n         quand        endroit       agréable           deux      polynésie \n           315            313            313            306            305 \n       famille          cadre      l'accueil       terrasse           nuit \n           303            302            299            294            288 \n    recommande          d'une           rien          calme           motu \n           284            284            281            279            276 \n     également        superbe             ça          n'est    gentillesse \n           275            267            266            265            257 \n          lieu       poissons       beaucoup          après      très_bien \n           257            254            254            253            248 \n          trop           être   restauration        journée          temps \n           247            247            246            246            244 \n\n\nShow the code\n#un nuage de mots rapide library(quanteda.textplots) \n\ntextplot_wordcloud(dfm, max_word=80, color = rev(RColorBrewer::brewer.pal(6, \"RdBu\")))\n\n\n\n\n\nMots les plus fréquents du corpus\n\n\n\n\nShow the code\ndfm2&lt;-dfm %&gt;% dfm_subset(!is.na(Taille_hotel))%&gt;%\n  dfm_group(groups = Taille_hotel)\n\n\ntextplot_wordcloud(dfm2, comparison = TRUE, max_words = 200,\n                   color = c(\"blue\", \"red\"))\n\n\n\n\n\nMots les plus fréquents du corpus",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#pondérer-la-fréquences-des-termes",
    "href": "q07_tokenisation.html#pondérer-la-fréquences-des-termes",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "7.5 Pondérer la fréquences des termes",
    "text": "7.5 Pondérer la fréquences des termes\nDans un dtm, chaque cellule du tableau indique la fréquence d’un mot dans un document. Cette mesure cependant peut ne pas être pertinentes. Les mots les plus fréquents peuvent se distribuer également entre les documents, et leur fréquence indique au mieux un lieu commun.\nL’idée clé va être de pondérer la fréquence d’un terme dans un document par la présence de ce terme à travers les documents. S’il se retrouve partout on en minore le poids, s’ils est présent dans une petit groupe de document on va lui donner de l’importance.\n\n7.5.1 le concept de tfidf\nCette idée est formalisée par le critère du TfIdf. La fréquence d’un terme dans un document (Tf) par être pondéré par l’inverse de la fréquence des documents dans lesquels il est présent (Idf). S’il est présent partout (c’est le cas des verbes auxiliaires) sont poids sera minimal, s’il est concentré dans un petit nombre de documents on lui donnera un poids plus important.\nFormalisation\nla fréquence brute des termes est le rapport du nombre de termes i dans le document d sur le nombre de termes du document i\n\\(tf_{id}= n_{id}\\)\nCette fréquence peut être modulée par le nombre de mots dans le texte, cette fréquence relative permet d’avoir une idée de la densité d’un terme dans un document.\n\\(tf_{id}= n_{id}/\\sum_{i}(n_{id}\\)\nla pondération\n\\(idf_{id}= log10(D_{}/D_{i})\\)\nCe concept souligne une chose capitale : un terme, ou token, dans un corpus, est caractérisé par une double distribution : d’une part sa part dans l’ensemble des termes de références, et d’autre part, sa présence à travers les documents.\n\n\n7.5.2 D’autres variantes\nCelles de l’idf https://programminghistorian.org/fr/lecons/analyse-de-documents-avec-tfidf\nle bm25 est une généralisation à une requête Q de plusieurs termes et vise à qualifier la pertinence\non développe avec une étude empirique.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#conclusion",
    "href": "q07_tokenisation.html#conclusion",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "7.6 Conclusion",
    "text": "7.6 Conclusion\nDans ce chapitre, nous avons vu comment découper un corpus en unités, les tokens. Nous avons abordé le sujet des n-grammes, et vu comment composer des tokens à partir de concepts multi-mots, identifiés par des n-grammes adjacents.\nOn conclue sur l’idée que la distribution des termes d’un texte doit se comprendre dans deux dimensions : la fréquence des termes parmi tous les termes, leur fréquence à travers les documents.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q07_tokenisation.html#footnotes",
    "href": "q07_tokenisation.html#footnotes",
    "title": "7  Découper les mots en jetons (Tokenize)",
    "section": "",
    "text": "En français digramme est la meilleure traduction de bigram, une bigramme en français est un mot dont les lettres peuvent former deux autres mot.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Découper les mots en jetons (Tokenize)</span>"
    ]
  },
  {
    "objectID": "q04_explorer.html#kwic",
    "href": "q04_explorer.html#kwic",
    "title": "4  Explorer le corpus",
    "section": "4.1 Kwic",
    "text": "4.1 Kwic\nLe premier réflexe dans la lecture d’un corpus est de chercher dans quels contextes sont utilisé des mots cibles. C’est l’objet d’une vieille technique les : Key word in context.\nexemple\n\n\nShow the code\ndf &lt;- read_csv(\"data/PMPLast3.csv\", locale = locale(encoding = \"WINDOWS-1252\"))%&gt;%\n   rename(Text=11, Year=3)\n \ncorp&lt;-corpus(df$Text)\n \nfoo&lt;- kwic(corp, \"évaluation\",\n  window = 6) %&gt;%\n  as.data.frame()\n\nset_flextable_defaults(\n  font.size = 10, theme_fun = theme_vanilla,\n  padding = 6,\n  background.color = \"#EFEFEF\")\n\nft&lt;-flextable(foo)\nft &lt;- set_caption(ft, caption = \"Keywords in context\") \nft\n\n\ndocnamefromtoprekeywordpostpatterntext312121des politiques municipales ? L 'évaluationde la qualification de ces consultationsévaluationtext631313- plus particulièrement les recherches enévaluationet les analyses avantage - coûtévaluationtext63110110, lorsqu'ils sont en présence d'uneévaluationqui conclut à l'échec ou àévaluationtext63169169l'autre , les leçons issues d'uneévaluationd'un programme . Cet article aévaluationtext796262décision facilement suspecté . L 'évaluationest donc nécessaire à la survieévaluationtext113137137point d'outils qui rendent possible uneévaluationdes politiques publiques .évaluationtext123261261pas sans un contrôle , uneévaluationdes politiques déjà mises en œuvreévaluationtext2255151à un recensement et à uneévaluationdes moyens à la disposition duévaluationtext257129129passer d'une notation traditionnelle à uneévaluationmoderne révèlent les contradictions entre cesévaluationtext2661919soit d'une fétichisation ( pour qu'uneévaluationsoit bonne et que ses conclusionsévaluationtext2769595présente une synthèse et une premièreévaluationde travaux effectués dans ce domaineévaluationtext2846565engendré des déceptions , accompagnées d'uneévaluationsans doute trop sommaire et rapideévaluationtext3138686actuelle , il propose ensuite uneévaluationcritique de la nouvelle organisation enévaluationtext3315656les Universités ont toutes réclamé leurévaluationqui leur permet de mieux seévaluationtext3417676sont associées , ainsi qu'une brèveévaluationde l'intérêt qu'elles présentent pour différentsévaluationtext356115115de coqueluche , nous effectuons uneévaluationcomparée des coûts de la maladieévaluationtext3673434l'évolution de ces projets affecte leurévaluationet par conséquent le choix deévaluationtext3939797encore peu utilisés , et l'auto-évaluationformalisée restait embryonnaire . Il fautévaluationtext439148148fourni le matériel empirique pour uneévaluationcritique des approches théoriques et desévaluationtext448173173on peut tenter de mener uneévaluationa priori de la mise enévaluationtext45355Quand on leur parleévaluation, beaucoup de décideurs pensent aussitôtévaluationtext454169169cet article est donc celle d'uneévaluationde processus , dont le principalévaluationtext4753636dresser le cahier des charges d'uneévaluationqui sera effectuée dans un secondévaluationtext47899un jugement de valeur , uneévaluationest un processus composé d'étapes dontévaluationtext48144La relation entreévaluationet budget peut être examinée àévaluationtext4855959les enjeux des politiques soumises àévaluation. Or , les acteurs ,évaluationtext492245245sur une autonomie accrue et uneévaluationdes résultats . Mais dans leévaluationtext4958888un modèle micro-économique reposant sur uneévaluationex-ante des performances des entreprises ,évaluationtext496166166mais qu'il est couplé à uneévaluationde la performance par objectifs procheévaluationtext50155Le texte propose uneévaluationde la performance financière des communesévaluationtext521347347enjeux de gestion . Une premièreévaluationde la démarche , courant 1997évaluationtext528233233, l'équilibre souhaité entre mesure etévaluation. La dernière partie s'attachera àévaluationtext528269269dichotomie pour systématiquement allier mesure etévaluation.évaluationtext5507171confrontés à la relation ambivalente entreévaluationet décision publique . Leurs modesévaluationtext607242242l’Évaluation , l'auteur plaide pour uneévaluationpragmatique et propose un changement deévaluationtext6117171bien souvent , la fiabilité d'uneévaluationne repose pas uniquement sur laévaluationtext6243131mal défriché . Pourtant , cetteévaluationest possible en suivant une procédureévaluationtext653146146regard de ses effets , cetteévaluationdite pluraliste s'avère être singulièrement incomplèteévaluationtext673129129communication étudie le lien complexe entreévaluationet décision . Diverses analyses montrentévaluationtext6955656savoir faire entrer , dans uneévaluationdite rationalisante des projets d'investissements ,évaluationtext706106106négociation des prochains contrats , uneévaluationpluriannuelle de la réalisation des premiersévaluationtext724235235conduites dans 1021 services et uneévaluationqualitative du dispositif apportent des éclairagesévaluationtext785151151. L’espace limité réservé à cetteévaluationoblige à forcer le trait .évaluationtext7875656qu'il est souvent difficile de concilierévaluationdes politiques publiques , d'une partévaluationtext7879696à se substituer à une réelleévaluationdes résultats et des effets finauxévaluationtext7946565temps , les modalités de cetteévaluation. La mise en évidence d’uneévaluationtext794123123de soins , complété par uneévaluationde la qualité , qui empruntentévaluationtext844110110mettons ensuite en évidence que cetteévaluationparticipe à la constitution progressive etévaluationtext8555555a évolué l’intérêt porté à cetteévaluationsur la période 1980-2007 dans cesévaluationtext855112112mettant en avant la logique d’uneévaluationactuelle en vue de la modernisationévaluationtext8776969publique , cette démarche propose uneévaluationintégrée à son contexte politique prenantévaluationtext8783535lancement de l’expérimentation et de sonévaluation, sa mise en œuvre etévaluationtext9022121communication financière sur Internet . Uneévaluationdes pratiques actuelles des collectivités émettricesévaluationtext916149149une double figure de l’innovation enévaluationdans les collectivités territoriales : l’innovationévaluationtext9386060des lois d’une part , entreévaluationex ante et ex post deévaluationtext9412626la portée dans le cadre d’uneévaluationde la politique publique éponyme .évaluationtext992101101( validation dans la pratique etévaluation) , grâce à la créationévaluationtext9937979sphères administrative et politique , sonévaluationsystématique et une réduction de laévaluation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Explorer le corpus</span>"
    ]
  },
  {
    "objectID": "q04_explorer.html#explorer-le-corpus",
    "href": "q04_explorer.html#explorer-le-corpus",
    "title": "4  Explorer le corpus",
    "section": "4.2 Explorer le corpus",
    "text": "4.2 Explorer le corpus\nAvant de procéder aux analyses du corpus, il est souvent utile de le représenter. On va utiliser le package Corpora explore à cette fin. Il permet de préparer un corpus et de le visualiser de manière interactive avec la génération d’une app shiny. Malheuresuement nous ne savons pas rendre compte de la dynamique de l’outil. On peut naviguer aisement dans l’ensemble de texte.\nOn va utiliser une collection de donnée préparée avec Manel Benzarafa de l’Université paris Nanterre, et qui comprend l’intégralité des résumés, auteurs etc.. relatifs aux articles publiés dans PMP. Une base bibliographique intégrale. 1025 articles la compose.\n\n\nShow the code\nlibrary(readr) \n#install.packages(\"corporaexplorer\") \nlibrary(corporaexplorer)   \nlibrary(readr) \n\nfoo&lt;-df %&gt;% select(Key, Author, Title, Year, Text) %&gt;%\n  rename(year=Year)%&gt;%filter(Text!=\"Null\" & !is.na(year))    \n\ncorpus &lt;- prepare_data(foo, date_based_corpus =FALSE, grouping_variable = \"year\")\n\nexplore(corpus)\n\n\nShiny applications not supported in static R Markdown documents\n\n\nDans la photo d’écran suivante, on teste les termes ” politique” et “management”. Chaque tuile ( uile) représente un des 1025 abstracts qui composent le corpus. Les couleurs correspondent à la fréquence des deux termes.\n\n\n\nExploration des abstracts de PMP\n\n\n\n4.2.1 reprendre le topic de revtools dans topic model\nhttps://revtools.net/screening.html#screening-with-topic-models",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Explorer le corpus</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#stringr",
    "href": "q05_manipuler_caractere.html#stringr",
    "title": "5  Manipuler les chaines de caractères",
    "section": "5.1 Stringr",
    "text": "5.1 Stringr\nC’est l’outil général que nous favorisons , il est intégré dans la suite tidyverse et propose de nombreuses fonctions.\nhttps://stringr.tidyverse.org/index.html\n\n5.1.1 Compter :\nDans le chunck suivant on utilise le set de donnée PMP40ans. Le but est de compter le nombre de fois où un terme apparaît dans chacun des textes avec la fonction str_count. On en donne l’évolution au cours du temps\n\n\nShow the code\nfoo &lt;- df %&gt;% \n  dplyr::select(Year, Text)%&gt;% \n  filter(!is.na(Text))%&gt;% #on filtre les textes manquants\n  mutate(n=str_count(Text, \"évaluation\")) %&gt;% #\n  group_by(Year)%&gt;% #on agrège sur l'année \n  summarise(n=sum(n))\n\nggplot(foo, aes(Year,n))+geom_line() +\n  geom_smooth()+\n  ylim(0,40)\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Extraire\nDans certaines situations on peut souhaiter extraire une chaîne de caractère pour l’utiliser à un autre usage.L’exemple ordinaire est celui d’extraire l’année d’une date. ou un auteur\nstr_extract\n\n\n5.1.3 Remplacer\nstr_replace",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#regex",
    "href": "q05_manipuler_caractere.html#regex",
    "title": "5  Manipuler les chaines de caractères",
    "section": "5.2 Regex",
    "text": "5.2 Regex\nDans la section précédente on a appris à rechercher une chaîne de caractères, ou plusieurs dans une chaîne de caractères, à les compter, les extraire, les enlever, les remplacer. Dans nos exemples, on a employé des pattern simple. Dans l’exemple “évaluation”, on a tenu compte uniquement de cette forme, elle aurait pu être au pluriel, porté une majuscule, exacte ou inexacte. L’enjeu est de pouvoir saisir toute les variantes en une seule expression.\nDans notre exemple la solution est la suivante : [E|e|é]valuation.*, en voulant saisir plus de variation on peut employer [E|e|é]valu.* qui permet se saisir les former verbales.\nUne expression régulière, ou regex, ou expression rationnelle ou expression normale ou motif est une chaîne de caractères qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Son invention est attribuée au logicien Stephen Cole Kleene)\nsa pratique se fonde sur une forte théorie.\n\n5.2.1 les éléments principaux\nhttps://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432\nLes regex reposent sur plusieurs concepts :\n\ndes jeux de caractères qui sont défini par des crochets []. Dans l’exemple suivant on remplace toutes les voyelles, par rien, puis toute les majuscules par X. Il y a différentes variantes par exemple [A-Z] pour les majuscules ou [0-9] pour les chiffres.\n\n\n\nShow the code\n# Mathias Malzieu\nx&lt;-c(\"Si Cendrillon avait eu une horloge dans le coeur, elle aurait bloqué les aiguilles à minuit moins une et se serait éclaté au bal toute sa vie.\")\n\nstr_replace_all(x,\"[aeiou]\", \"\")\n\n\n[1] \"S Cndrlln vt  n hrlg dns l cr, ll rt blqé ls glls à mnt mns n t s srt éclté  bl tt s v.\"\n\n\nShow the code\nstr_replace_all(x,\"[A-Z]\", \"X\")\n\n\n[1] \"Xi Xendrillon avait eu une horloge dans le coeur, elle aurait bloqué les aiguilles à minuit moins une et se serait éclaté au bal toute sa vie.\"\n\n\n\nmeta caractères Les méta caractères représentent un type de caractère. Ils commencent généralement par une barre oblique inverse (backslash). La barre oblique inverse étant un caractère spécial dans R, elle doit être échappée chaque fois qu’elle est utilisée avec une autre barre oblique inverse. En d’autres termes, R exige deux barres obliques inverses lors de l’utilisation de métacaractères. Chaque méta-caractère correspondra à un seul caractère.\n\n\\s : Ce méta caractère représente les espaces. Il correspondra à chaque espace, tabulation et nouvelle ligne.\n\nLes quantificateurs permette de contrôler le nombre de caractère que l’on attend. Par exemple , le quantificateur + indique que l’on souhaite que l’élément recherché apparaisse une ou plus fois. Si l’action est d’identifier a, a+, renvoie a, aa, aaa etc . Ceci ne semble pas très utile, sauf si le caractère peut être n’importe quelle lettre de l’alphabet, ce qui est représenté par le . Une variante du + est *, c’est la même idée mais cela inclue l’option de 0 caractère.\nGroupes de capture\n\n\n\n5.2.2 Des formules usuelles\nLa formulation de regex est un art, dans le quotidien on se contentera de reprendre des formules communes\nen voici quelles-unes\n\ndate “[0-9]{2}-[0-9]{2}-[0-9]{2}”\nmention\nadresse web\nnuméro de téléphone",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q05_manipuler_caractere.html#conclusion",
    "href": "q05_manipuler_caractere.html#conclusion",
    "title": "5  Manipuler les chaines de caractères",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nSur le plan pratique nous avons fortement avancé, nous pouvons jouer avec les chaînes de caractère, et en saisir les variations. nous avons les moyens de pré-traiter le texte\n\npour réduire les morphologies\npour extraire des entités nommées\n…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Manipuler les chaines de caractères</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html",
    "href": "q06_Analyse_quantitative.html",
    "title": "6  Analyse Quantitative du corpus",
    "section": "",
    "text": "6.1 Comptons les mots\nIl y 56571 tweets et 9 variables et 1.117018^{6} mots cumulés.\nOn peut vouloir compter le nombre de mots. A cette fin on emploie une fonction de stringr, un package précieux que nous allons étudier de plus dans le chapitres suivant : str_count.\nOn note immédiatement la bi-modalité de la distribution qui correspond au changement de règle par Twitter en matière de non de caractères utilisés, étendu de 180 à 280.\nShow the code\nggplot(df, aes(x=nb_mots))+\n  geom_histogram(fill=col_1, binwidth = 10)+\n  labs(title=paste0(\"Nombre total de mots du corpus : \",sum_mots), \n       x=\"Nombre de mots par post\", \n       y=\"Fréquence\")\nLa bimodalité provient surement du changement de taille maximum effectué en septembre 2017, le passage de 180 caractères max à 280. On peut le vérifier en examinant cette même distribution - par les courbes de densité - pour chacune des années, avec cette technique rendue fameuse par la pochette de l’album de Joy Division : un graphique en crêtes (ridges plot) avec ggridges.\nLe résultat remarquable est que si Trump dans un premier temps exploite cette nouvel fonctionnalité, il en revient avec un phrasé de 20 mots en moyenne, gardant cependant à l’occasion d’autre contenu en 50 mots environ.\nShow the code\ndf$Year&lt;-format(df$date, format = \"%Y\") #on extrait l'année de la date\nfoo&lt;- df %&gt;% \n  filter(Year!=\"2021\") #parce qu'il n'y a quelques quelques jours en janvier, le compte a été interrompu vers le 5 janavier\n\nggplot(foo,aes(x = nb_mots, y = Year, group = Year)) +\n  geom_density_ridges(scale = 4, color=\"grey90\",fill=col_1, alpha=.7)+\n  theme_ridges() +\n  scale_x_continuous(limits = c(1, 60)) +\n  labs(x=\"Nombre de mots par post\",\n       y=NULL)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#la-production-dans-le-temps",
    "href": "q06_Analyse_quantitative.html#la-production-dans-le-temps",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.2 La production dans le temps",
    "text": "6.2 La production dans le temps\nExaminons le nombre de tweets produit au cours du temps.\nOn se rappellera qu’après une carrière immobilière menée dans les casinos, le golf et les hôtels, l’appétit médiatique de Trump s’est réalisé dans “the apprentice”, de 2004 à 2015. C’est un pro de la TV, il a une formation de Popstar. Il sera élu en Décembre 2016 pour prendre le pouvoir en Janvier.\n\n\nShow the code\ndf$date2&lt;-format(df$date, \"%Y-%m-%d\")\ndf$date3&lt;-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet&lt;-as.character(df$isRetweet)\n\nfoo&lt;-df %&gt;% filter(Year!=2021)%&gt;%\n  group_by(date3, isRetweet)%&gt;%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n\n\n\n\n\n\n\n\n\nShow the code\n#raf : labeliser avec les dates clés",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#popularité-des-tweets",
    "href": "q06_Analyse_quantitative.html#popularité-des-tweets",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.3 Popularité des tweets",
    "text": "6.3 Popularité des tweets\nDans ce set de donnée si l’auteur est unique, la réception est multiple, rappelons que près de 90 millions de personnes suivaient Trump. On possède deux indicateurs : le nombre de retweet et de RT.\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year!=2021)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(sum_rt=sum(retweets),\n            sum_fav=sum(favorites),\n            mean_rt=mean(retweets),\n            mean_fav=mean(favorites)\n            ) %&gt;%\n  pivot_longer(-Year,names_to = \"variable\", values_to = \"values\")\n\nggplot(foo, aes(x=Year, y=values,group=variable))+\n  geom_line(aes(color=variable), size=1,alpha=.8)+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_4)+ \n  facet_wrap(vars(variable), ncol=2, scale=\"free\")+\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nOn s’aperçoit d’un changement de régime, quand l’audience est limitée RT et fac sont fortement corrélés, l’entrée en politique de Trump se caractérise par un changement de régime. La décorrélation peut s’expliquer par l’usage d’agent d’influences, qui retweetent plus qu’ils n’approuvent. Ils contribuent à la décorrélation.\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year!=2021)%&gt;%\n  group_by(Year)%&gt;%\n  summarize(cor(favorites, retweets)) %&gt;%\n  rename(correlation =2)\n            \nggplot(foo, aes(x=Year, y=correlation, group=1))+\n  geom_line(size=1,alpha=.8, color=col_1b)+\n  geom_smooth(color=col_1b, size=1.5, fill=\"grey90\")+\n  labs(y=\"Corrélation entre fav et rt\", x=NULL)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfoo&lt;-df %&gt;% select(Year, favorites, retweets)%&gt;%\n  filter(!is.na(favorites) & Year==2020)\n\n\nm &lt;- ggplot(foo, aes(x = favorites, y = retweets)) +\n geom_point() +\nscale_x_log10()+\nscale_y_log10()\n\n# contour lines\nm + geom_density_2d()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndf$date2&lt;-format(df$date, \"%Y-%m-%d\")\ndf$date3&lt;-as.POSIXct(df$date2, format = \"%Y-%m-%d\")\ndf$isRetweet&lt;-as.character(df$isRetweet)\n\nfoo&lt;-df %&gt;% filter(Year!=2021)%&gt;%\n  group_by(date3, isRetweet)%&gt;%\n  summarise(n=n())\n\n## plot time series of tweets\n\nggplot(foo, aes(x=date3, y=n,group=isRetweet))+\n  geom_line(aes(color=isRetweet), alpha=.2)+\n  geom_smooth(aes(color=isRetweet),span=0.5)+\n    scale_x_datetime(date_breaks = \"1 year\", labels = scales::label_date_short())+\n  labs(y=\"nombre de tweets par jour\", x=NULL)+\n  scale_color_manual(values = col_2)\n\n\n\n\n\n\n\n\n\nShow the code\n#raf : labeliser avec les dates clés",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#lisibilité-et-complexité-lexicale",
    "href": "q06_Analyse_quantitative.html#lisibilité-et-complexité-lexicale",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.4 Lisibilité et complexité lexicale",
    "text": "6.4 Lisibilité et complexité lexicale\nPour aller un peu plus loin - nous savons désormais que Trump aime une forme courte en 21 mots, et que son expérience de twitter est longue, on peut s’intéresser à des paramètres clés relatifs aux conditions de la réception: les textes sont-ils aisés à lire ? sont-ils sophistiqués ?\nIntroduisons deux quantifications utiles du texte : la lisibilité et la complexité lexicale. Ce sont des classiques, les critères initiaux de l’analyse quantitative du texte. Ils sont toujours utiles.\n\n6.4.1 Les indices de lisibilité\nLa lisibilité est une notion ancienne tout autant que sa mesure (par exemple Coleman and Liau (1975)). Elle répond à la question du degré de maîtrise requis pour lire un texte en s’appuyant sur les caractéristiques objectives du texte plutôt que sur sa perception. Il s’agissait donc d’évaluer la complexité d’un texte à partir de deux critères : la complexité des mots capturée par le nombre moyen de syllabes, et la complexité des phrases mesurée par le nombre de mots.\nA partir de ces deux paramètres, une multitudes d’indicateurs ont été proposés. Dans l’exemple qui va suivre, on se contente d’un grand classique, le plus ancien, l’indice de Flesch (Flesch 1948) et de ses constituants : le nombre moyen de syllabes par mot et le nombre moyen de mots par phrase.\nOn les retrouve, avec une autre grande variété, dans le package compagnon de quanteda , quanteda.textstats , qui en fournit des dizaines de variantes.\n\n\nShow the code\n#on sélectionne les tweets originaux\n\nfoo&lt;-df %&gt;% filter(isRetweet==FALSE) # on ne prend pas en compte les RT\n\n#la fonction de calcul de lisibilité\nreadability&lt;-textstat_readability(foo$text, \n                                  measure = c(\"Flesch\",\"meanSentenceLength\", \"meanWordSyllables\"),\n                                  min_sentence_length = 3,\n                                  max_sentence_length = 1000) \n# on joint les données\nfoo&lt;-cbind(foo,readability[,2:4])\n\n#on agrège par année\nfoo1&lt;-foo %&gt;% filter(Year!=2021) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(all_tweet=n(),\n            Flesch=mean(Flesch, na.rm=TRUE), \n            SentenceLength= mean(meanSentenceLength, na.rm=TRUE),\n            WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %&gt;%\n  gather(variable, value, -Year)\n\n#visualisation\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la lisibilité des tweets de Trump\", x=NULL, y=NULL)\n\n\n\n\n\n\n\n\n\nPour aider le lecteur à donner un sens, voici l’abaque proposée par Flesch lui-même.\n\n\n\nFlesch\n\n\nOn peut aussi prendre pour références les éléments suivants:\n“All Plain English examples in this book score at least 60. Here are the scores of some reading materials I’ve tested. These are average scores of random samples.” ( source ?)\n\nComics 92\nConsumer ads in magazines 82\nReader’s Digest 65\nTime 52\nWall Street Journal 43\nHarvard Business Review 43\nHarvard Law Review 32\nAuto insurance policy 10\n\nTrump ne parait pas être sa caricature, non niveau de lisibilité correspond à la Licence. Le Reader’s Digest est beaucoup plus simple, il se situe au dessus de la Harvard Business Review !\n\n\n6.4.2 Les indices de complexité lexicale\nLa complexité lexicale rend compte de la diversité du vocabulaire, elle consiste à rapporter le nombre de mot uniques sur le nombre total de mots. La difficulté est que la taille des corpus joue fortement sur cette mesure et que lorsque cette taille est hétérogène, l’indicateur marque plus cette variété que les variations de complexité lexicale.(Tweedie and Baayen 1998)\nDans notre univers trumpesque, ce n’est pas trop sensible, d’autant plus que nous allons moyenner les tweets par période.Notons au passage que si nous moyennons la diversité lexicale de chaque tweet, une autre approche pourrait être de concaténer l’ensemble des tweets d’une période (un jour, une semaine) pour approcher cette variable à une autre échelle, qui couvre l’ensemble des sujets d’intérêt de Trump, que les tweets fractionnent nécessairement. Ce qui en en question dans la mise en pratique n’est pas seulement la question du choix de l’indice mais aussi la définition de l’unité de calcul. La diversité lexicale concerne sans doute plus le discours que la phrase.\nOn choisit de ne travailler sur deux des multiples indicateurs disponibles :\n\nle CTTR de Caroll qui rapporte le nombre de mots distincts ( V) sur le nombre de mots exprimés. Avec ce critère la diversité maximale est obtenue quand le nombre de mots différents est égal au nombre de mots exprimés.\n\n\\[\nCTTR = \\frac{V}{\\sqrt{2N}}\n\\]\n\nle Mass supposé être moins sensible à la longueur des textes. (voir Torruella et Capsada 2013 ou )\n\n\\[\nM = \\frac{log(n) - log(t)}{log² (n)}\n\\]\nOn notera que le problème de la longueur variables des textes dans un corpus produit un biais : les textes seront mécaniquement plus divers que les textes court, ce qui conduit à des approches segmentées, où la mesure de diversité est une moyenne des moyenne pour chacun des segments de texte . On emploie ici le MATTR, dont le MA signifie moyenne mobile (moving average), et le TTR le token/type ratio.\n(attention un pb de log dans le calcul)\n\n\nShow the code\n#on retient les tweets de plus de 5 mots\nfoo&lt;-df %&gt;%\n  filter(nb_mots&gt;10 & isRetweet==\"FALSE\" & Year!=2021)\n\n#la fonction de calcul de diversité\nt1=Sys.time()\nlexdiv&lt;-tokens(foo$text)%&gt;%\n  textstat_lexdiv(foo$text, measure = c(\"CTTR\", \"Maas\"),  log.base = 10,\n                  remove_numbers = TRUE,  \n                  remove_punct = TRUE,  \n                  remove_symbols = TRUE,\n                  remove_hyphens = TRUE) \nt2=Sys.time()\nt&lt;- t2-t1\nt\n\n\nTime difference of 4.253881 secs\n\n\nShow the code\n#On combine les données et on aggrège sur l'année\nfoo&lt;-cbind(foo,lexdiv[,2:3])\nfoo1&lt;-foo %&gt;% \n  group_by(Year) %&gt;%\n  summarise(CTTR=mean(CTTR, na.rm=TRUE), \n            Maas=mean(Maas, na.rm=TRUE)) %&gt;%\n  gather(variable, value, -Year)\n\nggplot(foo1,aes(x=Year, y=value, group=variable))+\n  geom_line(size=1.2, aes(color=variable), stat=\"identity\")+\n  facet_wrap(vars(variable), scale=\"free\", ncol=1)+\n  labs(title = \"Evolution de la diversité lexicale des tweets de Trump\", x=NULL, y=NULL)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#la-mesure-de-la-concentration-des-termes",
    "href": "q06_Analyse_quantitative.html#la-mesure-de-la-concentration-des-termes",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.5 La mesure de la concentration des termes",
    "text": "6.5 La mesure de la concentration des termes\nLe langage d’un point de vue quantitatif a été caractérisé depuis bien longtemps et il est caractérisé par la loi de Zipf distribution 1949 . Estout est un des premiers à avoir proposé la loi, mais un sténographe fameux en aurait mis en évidence empiriquement avant 1916.\n\n6.5.1 Un débat théorique\nCelle-ci s’exprime de la manière suivante : si on classe les termes par ordre de fréquence r, le produit de leur rang par la fréquence est égale à une constante. Autrement dit dans une métrique log, la relation entre rang et fréquence est linéaire et sa pente est négative.\n\\[f(r).{r}={K} \\] Mandelbrot la généralise\nles études empiriques\nOn peut encore débattre de sa signification. Une économie cognitive? Une conséquence de la théorie de l’information ? Un signature pour identifier un discours ## Les lois de distribution du langage\nLes lois puissances qui déterminent cet univers et marque dans le registre de l’analyse quantitative une évolution sérieuse. Alors que le modèle dominant de la statistique est la distribution gaussienne, le traitement du langage doit se familiariser avec des distributions puissance.\nLa régularité statistique nécessite une interprétation, il n’y en a pas qu’une\n\nla loi du moindre de effort qui va aboutir à un débat entre Mandelbrot et Simon.\nEconomie de l’information, un mot puis deux, puis trois, puis des mots spécifique à un certain registre d’action. Le sens est une affaire d’échelle, du concret au général. Le plus concret est celui des mots scientifiques telles qu’on les retrouve en biologie pour décrire une espèce, en droit pour décrire une propriété, ou en médecine pour désigner une pathologie ou un élément d’anatomie. ( espace de connaissance - épistémologique)\nla distribution des locuteurs et de leurs élocution. Dans le langage vernaculaire celui de nos sociabilités, peu de mots sont employés, à mesure qu’on les enregistre ils prennent une place plus importance d’un point de de vue statistique dans les corpus. Les mots rares sont employés par peu de personne. (espace des population - démographique)\n\n\n\n6.5.2 Application\nExaminons plus concrètement\nOn reprend la procédure de comptage des mots par groupe, mais sans filtrer sur la fréquence de ces mots. On en obtient n_w mots distincts. En examinant avec plus de détail il y a 5 élément d’information : a ) le trait (feature), c’est à dire ici le mot étudié, b) le nombre de fois où il apparaît dans le corpus, c)le rank qui lui correspond, d)le nombre de documents dans lesquels il apparaît.\nLe tableau nous donne les 10 premiers. Il y a 28000 éléments, dont une simple inspection montrera qu’il sont des liens ou d’autres mentions. Il est nécessaire de retraiter le corpus pour éliminer ces éléments. Nous étudierons comment faire dans les chapitres suivants. A ce stade on se contente d’enlever les apax1 dont on ne peut véritablement calculer un rang ( ils ont tous le dernier). Il nous reste 10854 termes.\n\n\nShow the code\nfoo&lt;-df %&gt;% \n  filter(isRetweet==FALSE) %&gt;%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks&lt;- tokens(foo$text) %&gt;% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\nfreq_g &lt;- textstat_frequency(toks)%&gt;%\n  as.data.frame() %&gt;%\n  filter(feature!=\"amp\") %&gt;%\n  filter(frequency&gt;1)\n\nflextable(head(freq_g, 15))\n\n\nMots les plus fréquentsfeaturefrequencyrankdocfreqgroupgreat4,07713,755allthank2,04132,029allpeople2,04041,895alljust1,63351,572allnow1,54461,505allpresident1,46671,322alltrump1,42481,307allbig1,36091,263allnews1,347101,254allcountry1,274111,233allget1,180121,083allfake1,177131,100allnew1,167141,089alldemocrats1,163151,119allmany1,108161,061all\n\n\nOn peut aussi représenter cela graphiquement. La loi de zipf s’observe jusqu’au 500 premiers mots, ensuite elle suit une autre pente. C’est assez caractéristique, la loi de zipf est incomplète, d’un point de vue empirique il semble qu’il y ait deux lois qui se superposes, la loi des mots communs, et celle des mots singuliers. C’est du moins hypothèse que nous proposons.\n\n\nShow the code\nggplot(freq_g, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"gam\", color=col_1)\n\n\n\n\n\n\n\n\n\nShow the code\nfoo&lt;-freq_g %&gt;%filter(rank &lt;500)\nggplot(foo, aes(x=rank, y=frequency))+\n  geom_point(size=.1)+\n  scale_x_log10()+\n  scale_y_log10() + \n  geom_smooth(method=\"lm\", color=col_1)\n\n\n\n\n\n\n\n\n\nPour être plus qualitatif Une première manière de représenter la diversité du corpus est de représenter les mots les plus fréquents, selon cette fréquente et le ratio fréquence par document. Plus ce dernier est élevé plus il fait du mot un mot spécifique.\n\n\nShow the code\nfoo&lt;- freq_g %&gt;%\n  filter(frequency&gt;150 &frequency&lt;1500 )\n\nggplot(foo, aes(x=docfreq, y= frequency/docfreq))+\n  geom_text_repel(aes(label=feature), size=2, max.overlaps = 30)+\n  scale_x_log10()+\n  scale_y_log10()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#comptons-les-mots-1",
    "href": "q06_Analyse_quantitative.html#comptons-les-mots-1",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.6 Comptons les mots",
    "text": "6.6 Comptons les mots\nIl est temps de compter les mots, chacun d’entre eux, de se faire une idée une idée de leurs fréquences, de leur distribution.\nSouvent on éliminera ceux qui apparaissent de manière occasionnelle, mais aussi ceux qui apparaissent systématiquement dans tous les textes. Une fois ces deux filtrages effectués, le lexique est généralement de l’ordre de 500 à 10000 mots.\nDeux outils sont disponibles: les nuages de mots et les lollyplots. Les premiers donnent une idée immédiates, les seconds se prêtent mieux à une analyse systématique\n\n6.6.1 Les nuages de mots\nIls sont devenus extrêmement populaires même si l’effet esthétique est plus important que leur utilité analytique.\nggwordcloud\nPour l’application on prépare les données avec quanteda : on tokenise et on construit le dfm (pour le détail voir chapitre tokenization), ce qui nous permets notamment d’éliminer la ponctuation et les mots courants (articles, déterminant etc) qui apportent peu de signification.\n\n\nShow the code\nfoo&lt;-df %&gt;% filter(isRetweet==FALSE) %&gt;%\n  filter( Year %in% c(\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))# on ne prend pas en compte les RT\n\ntoks&lt;- tokens(foo$text) %&gt;% \n  dfm(remove_punct = TRUE,  remove = stopwords(\"english\"))\n\ndocvars(toks,\"Year\")&lt;-foo$Year\n\n#on se concentre du les termes utilisés 300 fois.  \n\nfoo&lt;-toks %&gt;% \n    dfm_trim(min_termfreq = 250, verbose = FALSE)\n\nfreq &lt;- textstat_frequency(foo)\n\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal() +  scale_size_area(max_size = 10) + \n  scale_color_gradient(low = col_1, high = col_1b)\n\n\n\n\n\n\n\n\n\nShow the code\nggsave(\"./image/g0.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")\n\n\nEt pour faire des comparaisons, entre l’année 2016 qui le conduit à être élu, 2018 une année de midterm et 2020 année de sa défaite, on utilise la même procédure mais on distingue un comptage de fréquence de mot par période. Le ggplot est identique aux précédents mais comprend en plus une géométrie “facet_wrap” qui éclate le nuages de mot selon les 3 périodes étudiées.\n\n\nShow the code\nfoo&lt;-toks %&gt;% \n  dfm_group(groups = Year) %&gt;%\n    dfm_trim(min_termfreq = 1, verbose = FALSE)\n\nurl_regex &lt;- \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n  \n#pour compter la fréquence des mots par année\nfreq &lt;- textstat_frequency(foo, group =Year) %&gt;%\n  mutate(feature=str_remove_all(feature, url_regex))|&gt;\n  filter(!is.na(feature) & frequency&gt;200 )%&gt;%\n  filter(feature!=\"amp\")\n           \n\nset.seed(42)\nlibrary(ggwordcloud)\n\nggplot(freq, aes(label = feature)) +\n  geom_text_wordcloud(aes(size=frequency, color=rank)) +\n  theme_minimal()+\n  facet_wrap(vars(group)) +  \n  scale_size_area(max_size = 8) \n\n\n\n\n\n\n\n\n\nShow the code\nggsave(\"./image/g1.jpg\", plot=last_plot(), width = 27, height = 19, units = \"cm\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#conclusion",
    "href": "q06_Analyse_quantitative.html#conclusion",
    "title": "6  Analyse Quantitative du corpus",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nNous aurons appris à\n\nCompter le nombre de documents et leurs longueurs\nMesurer la complexité du langage\nMesurer la diversité de son vocabulaire.\nA évaluer la concentration des sources\nA se donner une première idée de la lexicographie\n\nCes mesures n’ont ne sens que si elles peuvent être l’objet de comparaison :\n\nDe manière interne la comparaison se fait dans dans le temps et à travers des segments. On s’intéresse moins au niveau, qu’aux différences entre les niveaux.\nDe manière externe elle requiert un étalonnage. Comparer par rapport au français courant, à un niveau de langue soutenu, ou relâché. L’étalonnage revient à caractériser des types de corpus : presses, écriture savante, réseaux sociaux, publications officielles etc. On ne peut que souhaiter que des comparaisons systématiques soient engagées et compilées pour donner des points de repère précis quand on étudie un corpus particulier.\n\nElles participent à un premier niveau d’analyse du texte, en surface, visant à apprécier la dynamique de sa production, à établir les échelles d’analyse, à repérer les éléments structurels.\nLe texte est une matière qui a un poids (le nombre de mot), une variété (le nombre d’expressions), une complexité (les règles qui l’organisent). nous venons de nous doter des premiers outils d’analyse, il est temps de passer à la suite.\n\n\n\n\nColeman, Meri, and T. L. Liau. 1975. “A Computer Readability Formula Designed for Machine Scoring.” Journal of Applied Psychology 60 (2): 283–84. https://doi.org/10.1037/h0076540.\n\n\nFlesch, Rudolph. 1948. “A New Readability Yardstick.” Journal of Applied Psychology 32 (3): 221–33. https://doi.org/10.1037/h0057532.\n\n\nTweedie, Fiona J., and R. Harald Baayen. 1998. “How Variable May a Constant Be? Measures of Lexical Richness in Perspective.” Computers and the Humanities 32: 323–52.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  },
  {
    "objectID": "q06_Analyse_quantitative.html#footnotes",
    "href": "q06_Analyse_quantitative.html#footnotes",
    "title": "6  Analyse Quantitative du corpus",
    "section": "",
    "text": "les apax sont les termes qui n’apparaissent qu’une seule fois dans un corpus.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analyse Quantitative du corpus</span>"
    ]
  }
]